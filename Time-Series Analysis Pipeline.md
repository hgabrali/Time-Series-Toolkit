# ğŸ› ï¸ Time-Series Analysis Pipeline: EDA, Preprocessing & Feature Engineering

Zaman serisi modellemesinde verinin kalitesi ve hazÄ±rlÄ±k aÅŸamasÄ±, model seÃ§iminden daha kritiktir. AÅŸaÄŸÄ±daki tablo, ham veriden model girdisine kadar olan sÃ¼reÃ§te incelenmesi gereken **SorunlarÄ±**, **BileÅŸenleri**, **Ã‡Ã¶zÃ¼m YÃ¶ntemlerini** ve **KullanÄ±lan AraÃ§larÄ±** detaylandÄ±rÄ±r. 

| Analiz AlanÄ± (Analysis Area) | Sorunlar & BileÅŸenler (Problems & Components) | Teknik Detay & Ã–nem (Technical Detail & Importance) | Ã‡Ã¶zÃ¼m YÃ¶ntemleri (Solution Methods) | KullanÄ±lan AraÃ§lar & Testler (Tools & Tests) |
| :--- | :--- | :--- | :--- | :--- |
| **1. YapÄ±sal Analiz**<br>*(Structure & Quality)* | **DuraÄŸanlÄ±k (Stationarity)** | Verinin ortalama (*mean*) ve varyansÄ±nÄ±n (*variance*) zamanla deÄŸiÅŸmesi sorunudur. Modeller (ARIMA vb.) geÃ§miÅŸteki istatistiksel daÄŸÄ±lÄ±mÄ±n gelecekte de aynÄ± kalacaÄŸÄ±nÄ± varsayar. | â€¢ **Differencing:** Fark alma ($y_t - y_{t-1}$).<br>â€¢ **Log Transform:** VaryansÄ± sabitlemek (Heteroskedastisiteyi gidermek).<br>â€¢ **Decomposition:** Trendi ayÄ±klamak. | â€¢ **ADF Testi:** $p < 0.05$ ise DuraÄŸan.<br>â€¢ **KPSS Testi:** Trend duraÄŸanlÄ±ÄŸÄ± iÃ§in.<br>â€¢ **Rolling Statistics:** Hareketli ortalama/varyans grafiÄŸi. |
| **2. Veri BÃ¼tÃ¼nlÃ¼ÄŸÃ¼**<br>*(Data Integrity)* | **SÃ¼reklilik & Eksik Veri (Frequency & Missing Values)** | Zaman serisi "EÅŸit AralÄ±k" (*Equidistant*) gerektirir. Eksik tarihler (*Missing Dates*), lag hesaplamalarÄ±nÄ± bozar. Perakendede eksik veri genellikle "0 SatÄ±ÅŸ" anlamÄ±na gelir (*MNAR*). | â€¢ **Reindexing:** Eksik tarihleri araya doldurma.<br>â€¢ **Imputation:** `0` ile doldurma (satÄ±ÅŸ yoksa) veya `Linear Interpolation` (sensÃ¶r verisiyse).<br>â€¢ **Forward Fill (ffill):** Bir Ã¶nceki deÄŸeri taÅŸÄ±ma. | â€¢ `df.asfreq('D')`<br>â€¢ `df.reindex()`<br>â€¢ `df.fillna()`<br>â€¢ `df.interpolate()` |
| **3. AyrÄ±ÅŸtÄ±rma**<br>*(Decomposition)* | **Trend, Mevsimsellik & GÃ¼rÃ¼ltÃ¼** | Sinyali parÃ§alarÄ±na ayÄ±rma iÅŸlemidir.<br>â€¢ **Trend:** Uzun vadeli yÃ¶n.<br>â€¢ **Seasonality:** Sabit periyotlu tekrarlar.<br>â€¢ **Residuals:** AÃ§Ä±klanamayan gÃ¼rÃ¼ltÃ¼. | â€¢ **Additive Model:** $Y = T + S + R$ (Genlik sabitse).<br>â€¢ **Multiplicative Model:** $Y = T \times S \times R$ (Genlik trendle artÄ±yorsa).<br>â€¢ **STL Decomposition:** LOESS tabanlÄ± esnek ayrÄ±ÅŸtÄ±rma. | â€¢ `statsmodels.tsa.seasonal_decompose`<br>â€¢ `STL` (Robust decomposition)<br>â€¢ **ACF / PACF Plot:** Mevsimsel periyodu (Ã¶rn: Lag 7) tespit etmek iÃ§in. |
| **4. Anomali Tespiti**<br>*(Outlier Detection)* | **AykÄ±rÄ± DeÄŸerler (Outliers)** | Beklenen daÄŸÄ±lÄ±mÄ±n dÄ±ÅŸÄ±ndaki uÃ§ deÄŸerler. Modelin Ã¶ÄŸrenme sÃ¼recini (Ã¶zellikle MSE kaybÄ±nÄ±) bozar.<br>â€¢ **Point Anomaly:** Tekil sapma.<br>â€¢ **Contextual:** Zamana gÃ¶re sapma (KÄ±ÅŸÄ±n dondurma satÄ±ÅŸÄ± patlamasÄ±). | â€¢ **Trimming:** Veriyi silme (Veri kaybÄ± riski).<br>â€¢ **Winsorization:** %1 ve %99 persentillerine sabitleme.<br>â€¢ **Imputation:** AykÄ±rÄ± deÄŸeri ortalama ile deÄŸiÅŸtirme. | â€¢ **Z-Score:** $|Z| > 3$<br>â€¢ **IQR Method:** Boxplot analizi.<br>â€¢ **Isolation Forest:** Makine Ã¶ÄŸrenmesi tabanlÄ± tespit.<br>â€¢ **Prophet:** Anomalilere karÅŸÄ± direnÃ§lidir. |
| **5. Feature Engineering**<br>*(Time Features)* | **Takvim Etkisi (Calendar Effects)** | Model ham tarihi ("2023-01-01") anlayamaz. Tarihsel bilginin sinyale dÃ¶nÃ¼ÅŸmesi gerekir.<br>â€¢ **Cyclicality:** AralÄ±k ve Ocak ayÄ±nÄ±n yakÄ±nlÄ±ÄŸÄ±. | â€¢ **Extraction:** YÄ±l, Ay, GÃ¼n, Hafta Sonu Mu?<br>â€¢ **Cyclical Encoding:** Ay ve GÃ¼n bilgisini `SinÃ¼s` ve `KosinÃ¼s` dÃ¶nÃ¼ÅŸÃ¼mÃ¼ ile (x, y) dÃ¼zlemine oturtmak.<br>â€¢ **Event Flagging:** Tatil gÃ¼nlerini `0/1` olarak iÅŸaretlemek. | â€¢ `df.dt.month`, `df.dt.dayofweek`<br>â€¢ `np.sin(2 * np.pi * df.month/12)`<br>â€¢ `holidays` kÃ¼tÃ¼phanesi. |
| **6. Feature Engineering**<br>*(Lag & Window)* | **Gecikme ve Momentum (Lags & Rolling Windows)** | â€¢ **Lag Features:** GeÃ§miÅŸ deÄŸerler ($t-1, t-7$). Otokorelasyonu yakalar.<br>â€¢ **Rolling Window:** Hareketli ortalamalar. GÃ¼rÃ¼ltÃ¼yÃ¼ azaltÄ±r, trendi belirginleÅŸtirir. | â€¢ **Shift:** Veriyi kaydÄ±rma.<br>â€¢ **Rolling Mean:** Son 7 gÃ¼nÃ¼n ortalamasÄ± (Momentum).<br>â€¢ **Rolling Std:** Volatilite (deÄŸiÅŸkenlik) Ã¶lÃ§Ã¼mÃ¼. | â€¢ `df.shift(1)`<br>â€¢ `df.rolling(window=7).mean()`<br>â€¢ `df.rolling(window=30).std()` |

# ğŸ¯ Time-Series Analysis Pipeline: The DARTS Integration
*(Zaman Serisi Analiz HattÄ±: DARTS Entegrasyonu)*

EDA ve Ã–n Ä°ÅŸleme aÅŸamalarÄ±ndan sonra, modelleme sÃ¼recini standardize etmek iÃ§in **DARTS** kÃ¼tÃ¼phanesini kullanÄ±yoruz. AÅŸaÄŸÄ±daki tablo, geleneksel yÃ¶ntemlerin zorluklarÄ±nÄ± ve DARTS'Ä±n bu alanlardaki teknik Ã§Ã¶zÃ¼mlerini analiz etmektedir.



### ğŸ“Š Comparative Analysis Matrix: Manual vs. DARTS
*(KarÅŸÄ±laÅŸtÄ±rmalÄ± Analiz Matrisi: Manuel ve DARTS)*

| Analysis Area (Analiz AlanÄ±) | Problems & Components (Sorunlar ve BileÅŸenler) | Technical Detail & Importance (Teknik Detay ve Ã–nem) | Solution Methods (Ã‡Ã¶zÃ¼m YÃ¶ntemleri) | Tools & Tests (AraÃ§lar ve Testler) |
| :--- | :--- | :--- | :--- | :--- |
| **1. Data Structure & Handling**<br>*(Veri YapÄ±sÄ± ve YÃ¶netimi)* | **Problem:** Pandas DataFrame Ã¼zerinde tarih indeksleme hatalarÄ± ve veri bÃ¼tÃ¼nlÃ¼ÄŸÃ¼ riskleri.<br>**Component:** `TimeSeries` Object (DARTS'Ä±n temel veri nesnesi). | **Detail:** Veriyi ve zaman indeksini tek bir nesne iÃ§inde kapsÃ¼ller (*Encapsulation*).<br>**Importance:** Veri manipÃ¼lasyonu sÄ±rasÄ±nda zaman ekseninin kaymasÄ±nÄ± (*Misalignment*) Ã¶nler. | **Method:** `TimeSeries.from_dataframe()`<br>Veriyi doÄŸrudan zaman serisi formatÄ±na Ã§evirir ve eksik tarihleri (*Gaps*) otomatik yÃ¶netir. | â€¢ **Imputation Tools:** Eksik verileri doldurma.<br>â€¢ **Mapping:** Veri tiplerini (float32) optimize etme. |
| **2. Feature Management**<br>*(Ã–zellik YÃ¶netimi)* | **Problem:** Gelecekten gelen veriyi geÃ§miÅŸe sÄ±zdÄ±rma (*Data Leakage*) riski ve dÄ±ÅŸsal deÄŸiÅŸkenleri (*Covariates*) hizalama zorluÄŸu.<br>**Component:** `PastCovariates` & `FutureCovariates`. | **Detail:** GeÃ§miÅŸ (Ã¶r. Laglar) ve Gelecek (Ã¶r. Tatiller) deÄŸiÅŸkenlerini ayrÄ± ayrÄ± iÅŸler.<br>**Importance:** Modelin sadece "bilmesi gereken" veriyi gÃ¶rmesini saÄŸlar, sÄ±zÄ±ntÄ±yÄ± engeller. | **Method:** `covariates=` parametresi.<br>Modelleri eÄŸitirken dÄ±ÅŸsal faktÃ¶rleri (*External Regressors*) tek parametre ile dahil etme. | â€¢ **Encoders:** Tarihsel Ã¶zellikleri (Ay, GÃ¼n) otomatik Ã¼retme.<br>â€¢ **Scalers:** `Scaler` (MinMax/Standard) ile veriyi Ã¶lÃ§eklendirme. |
| **3. Model Training API**<br>*(Model EÄŸitimi ArayÃ¼zÃ¼)* | **Problem:** FarklÄ± kÃ¼tÃ¼phanelerin (Statsmodels, PyTorch, Sklearn) tamamen farklÄ± API yapÄ±larÄ±na sahip olmasÄ±.<br>**Component:** Unified API (BirleÅŸik ArayÃ¼z). | **Detail:** TÃ¼m modeller iÃ§in standart `fit()` ve `predict()` yapÄ±sÄ±nÄ± kullanÄ±r.<br>**Importance:** Modeller arasÄ±nda (Ã¶r. ARIMA'dan LSTM'e) geÃ§iÅŸ yapmayÄ± tek satÄ±r kod deÄŸiÅŸikliÄŸine indirger (*Interchangeability*). | **Method:** `model.fit(series)`<br>Klasik istatistiksel modellerden derin Ã¶ÄŸrenmeye kadar aynÄ± sÃ¶zdizimi (*Syntax*). | â€¢ **Local Models:** ARIMA, ETS, FFT.<br>â€¢ **Global Models:** RNN, N-BEATS, XGBoost, TCN. |
| **4. Validation & Evaluation**<br>*(DoÄŸrulama ve DeÄŸerlendirme)* | **Problem:** Zaman serilerinde rastgele bÃ¶lme (*Random Split*) yapÄ±lamamasÄ± ve geriye dÃ¶nÃ¼k testlerin (*Backtesting*) karmaÅŸÄ±k kod gerektirmesi.<br>**Component:** `historical_forecasts` & `backtest`. | **Detail:** Kayan pencereler (*Sliding Windows*) veya geniÅŸleyen pencereler (*Expanding Windows*) ile simÃ¼lasyon.<br>**Importance:** Modelin gerÃ§ek dÃ¼nya performansÄ±nÄ± en doÄŸru ÅŸekilde taklit eder. | **Method:** `model.backtest()`<br>Zaman ekseni Ã¼zerinde adÄ±m adÄ±m ilerleyerek tahmin Ã¼retir ve hata hesaplar. | â€¢ **Metrics:** MAPE, RMSE, MAE, R2.<br>â€¢ **Plotting:** Tahminleri ve gerÃ§ek veriyi gÃ¶rselleÅŸtirme. |
| **5. Probabilistic Forecasting**<br>*(OlasÄ±lÄ±ksal Tahmin)* | **Problem:** Ã‡oÄŸu modelin sadece tek bir nokta tahmini (*Point Forecast*) Ã¼retmesi ve belirsizliÄŸi Ã¶lÃ§ememesi.<br>**Component:** `Likelihood` & `QuantileRegression`. | **Detail:** Tahminlerin gÃ¼ven aralÄ±klarÄ±nÄ± (*Confidence Intervals*) ve daÄŸÄ±lÄ±mlarÄ±nÄ± hesaplar.<br>**Importance:** Ä°ÅŸ kararlarÄ±nda risk analizi yapabilmek iÃ§in "en kÃ¶tÃ¼/en iyi senaryo"yu gÃ¶rmeyi saÄŸlar. | **Method:** `num_samples=` parametresi.<br>Tahmin sÄ±rasÄ±nda Monte Carlo simÃ¼lasyonu ile birden fazla olasÄ±lÄ±k Ã¼retme. | â€¢ **Interval Plots:** GÃ¼ven aralÄ±klarÄ±nÄ± grafikte gÃ¶lgeli alan olarak gÃ¶sterme.<br>â€¢ **Quantile Loss:** DoÄŸruluk metriÄŸi olarak dilim kaybÄ±. |

---

### ğŸ’¡ Summary of the DARTS Advantage
*(DARTS AvantajÄ±nÄ±n Ã–zeti)*

Geleneksel yÃ¶ntemlerde her adÄ±m (Veri temizleme, Ã¶zellik Ã¼retme, modelleme, test etme) ayrÄ± araÃ§lar ve karmaÅŸÄ±k kod bloklarÄ± gerektirirken; **DARTS**, bu sÃ¼reci **Analysis Area** baÅŸlÄ±ÄŸÄ±ndan **Tools & Tests** aÅŸamasÄ±na kadar tek bir Ã§atÄ± altÄ±nda (*Unified Framework*) birleÅŸtirir.



# ğŸ•°ï¸ Comprehensive Guide to ARIMA Modelling & Parameters
*(ARIMA Modelleme ve Parametreler KapsamlÄ± Rehberi)*

Bu bÃ¶lÃ¼m, Klasik Zaman Serisi YÃ¶ntemlerinin (*Classical Time-Series Methods*) temel taÅŸÄ± olan ARIMA modelini, parametre seÃ§imlerini ve doÄŸrulama sÃ¼reÃ§lerini teknik detaylarla ele alÄ±r.



### ğŸ“Š Comparative Analysis Matrix: ARIMA Components & Workflow
*(KarÅŸÄ±laÅŸtÄ±rmalÄ± Analiz Matrisi: ARIMA BileÅŸenleri ve Ä°ÅŸ AkÄ±ÅŸÄ±)*

| Analysis Area (Analiz AlanÄ±) | Problems & Components (Sorunlar ve BileÅŸenler) | Technical Detail & Importance (Teknik Detay ve Ã–nem) | Solution Methods (Ã‡Ã¶zÃ¼m YÃ¶ntemleri) | Tools & Tests (AraÃ§lar ve Testler) |
| :--- | :--- | :--- | :--- | :--- |
| **1. ARIMA Fundamentals**<br>*(ARIMA Temelleri)* | **Problem:** GÃ¼Ã§lÃ¼ mevsimsel kalÄ±plarÄ± olmayan verileri tahmin etme.<br>**Components:**<br>â€¢ **AR:** AutoRegression<br>â€¢ **I:** Integrated<br>â€¢ **MA:** Moving Average | **Detail:** ARIMA, geÃ§miÅŸ deÄŸerleri (*AR*), trendleri (*I*) ve geÃ§miÅŸ tahmin hatalarÄ±nÄ± (*MA*) birleÅŸtirir.<br>**Importance:** Verideki trendleri dÃ¼zelterek (*Adjusting for trends*) doÄŸru tahminler Ã¼retmeyi hedefler. | **Combination:**<br>$$ARIMA(p, d, q)$$<br>ÃœÃ§ bileÅŸenin matematiksel kombinasyonu. | â€¢ **Forecast Models:** GÃ¼nlÃ¼k satÄ±ÅŸ vb. tahminleri.<br>â€¢ **Components Check:** Trend ve Otokorelasyon analizi. |
| **2. Stationarity & Parameter `d`**<br>*(Durgunluk ve d Parametresi)* | **Problem:** Verinin "Durgun" (*Stationary*) olmamasÄ± (zamanla artan/azalan trend).<br>**Component:** **Integrated (I)** kÄ±smÄ±. | **Detail:** `d` parametresi, seriyi durgunlaÅŸtÄ±rmak (sabit ortalama ve varyans) iÃ§in kaÃ§ kez fark alÄ±nacaÄŸÄ±nÄ± belirtir.<br>**Importance:** Durgun olmayan veriler modellerin hatalÄ± Ã§alÄ±ÅŸmasÄ±na neden olur. | **Differencing:**<br>Ã–nceki deÄŸeri mevcut deÄŸerden Ã§Ä±karma (*Subtracting previous from current*).<br>$$y'_t = y_t - y_{t-1}$$ | â€¢ **Visual Check:** Ham seri ve Hareketli Ortalama Ã§izimi.<br>â€¢ **ADF Test:** Ä°statistiksel durgunluk testi.<br>â€¢ **Rolling Mean:** GÃ¼rÃ¼ltÃ¼yÃ¼ azaltarak trendi gÃ¶rme. |
| **3. Choosing `d` (Workflow)**<br>*(d SeÃ§imi Ä°ÅŸ AkÄ±ÅŸÄ±)* | **Problem:** Trendin varlÄ±ÄŸÄ±na karar verme (Deterministik mi Stokastik mi?).<br>**Decision:** `d=0` vs `d=1`. | **Detail:**<br>â€¢ **H0 (Null):** Seri durgun deÄŸil (Unit Root var).<br>â€¢ **H1 (Alt):** Seri durgun.<br>**Importance:** AÅŸÄ±rÄ± fark alma (*Over-differencing*) gÃ¼rÃ¼ltÃ¼ yaratÄ±r. | **Step-by-Step:**<br>1. **d=0:** GÃ¶rsel ve ADF (p<0.05) kontrolÃ¼.<br>2. **d=1:** Trend varsa fark al ve tekrar test et.<br>3. **d=2:** Nadiren gerekir. | â€¢ **ADF p-value:**<br>If $p < 0.05 \rightarrow$ Stationary ($d=0$ veya iÅŸlemi durdur).<br>If $p \ge 0.05 \rightarrow$ Non-Stationary (Fark al). |
| **4. Choosing `p` (AR Order)**<br>*(p Derecesi SeÃ§imi)* | **Problem:** GeÃ§miÅŸ gÃ¼nlerin bugÃ¼ne etkisini Ã¶lÃ§me.<br>**Component:** **AutoRegression (AR)**. | **Detail:** Bir gecikmenin (*Lag*) ÅŸimdiki zaman Ã¼zerindeki **saf ve doÄŸrudan etkisi** (*Pure/Direct Effect*).<br>**Importance:** Gereksiz gecikmeler model karmaÅŸÄ±klÄ±ÄŸÄ±nÄ± artÄ±rÄ±r (*Overfitting risk*). | **PACF Analysis:**<br>GÃ¼ven aralÄ±ÄŸÄ±nÄ±n (*Confidence Interval*) dÄ±ÅŸÄ±na taÅŸan son anlamlÄ± Ã§ubuÄŸu (*Significant Spike*) bulma.<br>â€¢ **Cut-off Point:** YararlÄ± hafÄ±zanÄ±n bittiÄŸi yer. | **PACF Plot:**<br>â€¢ **Tall bar outside band:** AnlamlÄ± etki.<br>â€¢ **Sharp drop:** Kesilme noktasÄ± ($p$ adayÄ±). |
| **5. Choosing `q` (MA Order)**<br>*(q Derecesi SeÃ§imi)* | **Problem:** GeÃ§miÅŸ hatalarÄ±n bugÃ¼ne etkisini Ã¶lÃ§me.<br>**Component:** **Moving Average (MA)**. | **Detail:** GeÃ§miÅŸ tahmin hatalarÄ±nÄ±n (*Forecast Errors*) ÅŸok etkisi.<br>**Importance:** "GeÃ§miÅŸteki kaÃ§ hata bugÃ¼nÃ¼ hala etkiliyor?" sorusuna yanÄ±t verir. | **ACF Analysis:**<br>ACF grafiÄŸindeki **kesilme noktasÄ±na** (*Cut-off*) bakÄ±lÄ±r.<br>â€¢ **Rule:** PACF yerine ACF kullanÄ±lÄ±r. | **ACF Plot:**<br>â€¢ **Sharp Cut-off after Lag q:** MA(q) adayÄ±.<br>â€¢ **Gradual Decay:** Genellikle AR sÃ¼recini iÅŸaret eder. |
| **6. Diagnostics & Evaluation**<br>*(TanÄ±lama ve DeÄŸerlendirme)* | **Problem:** Modelin gÃ¼venilirliÄŸi ve performansÄ±.<br>**Components:**<br>â€¢ **Residuals** (Hatalar)<br>â€¢ **Metrics** (Metrikler) | **Detail:** Hatalar **Beyaz GÃ¼rÃ¼ltÃ¼** (*White Noise*) olmalÄ±dÄ±r (Ortalama=0, Varyans=Sabit, Korelasyon=Yok).<br>**Importance:** Hatalarda desen varsa model bilgiyi tam Ã¶ÄŸrenememiÅŸtir. | **Tests:**<br>â€¢ **Ljung-Box:** HatalarÄ±n rastgeleliÄŸini test eder.<br>â€¢ **AIC:** Model seÃ§imi (DÃ¼ÅŸÃ¼k iyidir).<br>â€¢ **MAE/RMSE:** DoÄŸruluk Ã¶lÃ§Ã¼mÃ¼. | â€¢ **Residual Plots:** GÃ¶rsel kontrol.<br>â€¢ **AIC Score:** KarmaÅŸÄ±klÄ±k vs Uyum dengesi.<br>â€¢ **Test Set:** GerÃ§ek vs Tahmin karÅŸÄ±laÅŸtÄ±rmasÄ±. |

---

### ğŸ“ Summary of Critical Actions
*(Kritik AksiyonlarÄ±n Ã–zeti)*

1.  **Stationarity:** Start with `d=0`. If trend exists (Visual or ADF p $\ge$ 0.05), apply differencing (`d=1`).
    *(Durgunluk: d=0 ile baÅŸla. Trend varsa fark al.)*
2.  **AR Order (`p`):** Look at the **PACF** plot. Select the lag where the sharp drop occurs.
    *(AR Derecesi: PACF grafiÄŸine bak. Keskin dÃ¼ÅŸÃ¼ÅŸÃ¼n olduÄŸu gecikmeyi seÃ§.)*
3.  **MA Order (`q`):** Look at the **ACF** plot. Select the lag where the cut-off occurs.
    *(MA Derecesi: ACF grafiÄŸine bak. Kesilme noktasÄ±nÄ± seÃ§.)*
4.  **Validation:** Ensure residuals resemble **White Noise** and minimize **AIC**.
    *(DoÄŸrulama: ArtÄ±klarÄ±n Beyaz GÃ¼rÃ¼ltÃ¼ye benzediÄŸinden emin ol ve AIC'yi minimize et.)*


# ğŸ—“ï¸ Classical Time-Series Methods: SARIMA
*(Klasik Zaman Serisi YÃ¶ntemleri: SARIMA)*

**SARIMA** (*Seasonal AutoRegressive Integrated Moving Average*), klasik ARIMA modelinin, verilerdeki **mevsimsel dÃ¶ngÃ¼leri** (*seasonal cycles*) modelleyebilecek ÅŸekilde geniÅŸletilmiÅŸ halidir. Standart ARIMA modelleri serinin **kÄ±sa vadeli hafÄ±zasÄ±nÄ±** (*short-term memory*) yakalarken, SARIMA haftalÄ±k veya aylÄ±k tekrarlayan desenleri modeller.



### ğŸ“Š Comparative Analysis Matrix: SARIMA Architecture & Workflow
*(KarÅŸÄ±laÅŸtÄ±rmalÄ± Analiz Matrisi: SARIMA Mimarisi ve Ä°ÅŸ AkÄ±ÅŸÄ±)*

| Analysis Area (Analiz AlanÄ±) | Problems & Components (Sorunlar ve BileÅŸenler) | Technical Detail & Importance (Teknik Detay ve Ã–nem) | Solution Methods (Ã‡Ã¶zÃ¼m YÃ¶ntemleri) | Tools & Tests (AraÃ§lar ve Testler) |
| :--- | :--- | :--- | :--- | :--- |
| **1. Seasonality Handling**<br>*(Mevsimsellik YÃ¶netimi)* | **Problem:** ARIMA'nÄ±n mevsimsel ÅŸoklarÄ± (Ã¶rn. Noel satÄ±ÅŸlarÄ±) gÃ¶rememesi.<br>**Component:** **Seasonal Period ($s$)**. | **Detail:** $s$, dÃ¶ngÃ¼nÃ¼n uzunluÄŸudur (HaftalÄ±k veri iÃ§in $s=7$, AylÄ±k veri iÃ§in $s=12$).<br>**Importance:** Modelin hangi aralÄ±klarla geÃ§miÅŸe bakacaÄŸÄ±nÄ± belirler. | **Notation:**<br>$$SARIMA(p, d, q) \times (P, D, Q)_s$$<br>Mevsimsel olmayan ve mevsimsel parametrelerin Ã§arpÄ±mÄ±. | â€¢ **Seasonal Decomposition:** Trend ve mevsimselliÄŸi gÃ¶rsel ayÄ±rma.<br>â€¢ **ACF Plot:** $s, 2s, 3s$ gecikmelerindeki (*lags*) sÄ±Ã§ramalarÄ± kontrol etme. |
| **2. Seasonal AutoRegression ($P$)**<br>*(Mevsimsel Oto-Regresyon)* | **Problem:** Bu ayÄ±n satÄ±ÅŸlarÄ±nÄ±n, geÃ§en yÄ±lÄ±n aynÄ± ayÄ±ndaki satÄ±ÅŸlarla iliÅŸkisi.<br>**Component:** **Seasonal AR ($P$)**. | **Detail:** "KaÃ§ tane mevsimsel dÃ¼n ($t-s, t-2s$) bugÃ¼nÃ¼ etkiliyor?" sorusuna yanÄ±t verir.<br>**Importance:** GeÃ§miÅŸ sezonlarÄ±n momentumunu bugÃ¼ne taÅŸÄ±r. | **Interaction:**<br>Standart $p$ (dÃ¼n) ile Mevsimsel $P$ (geÃ§en yÄ±l bugÃ¼n) birlikte Ã§alÄ±ÅŸÄ±r.<br>**Initial Guess:** Mevsimsel PACF grafiÄŸinde $s$ gecikmesinde bÃ¼yÃ¼k sÄ±Ã§rama varsa $P=1$. | â€¢ **PACF Plot:** $s$ katlarÄ±nda (12, 24...) keskin dÃ¼ÅŸÃ¼ÅŸler aranÄ±r.<br>â€¢ **Grid Search:** En iyi $P$ deÄŸerini deneme yanÄ±lma ile bulma. |
| **3. Seasonal Differencing ($D$)**<br>*(Mevsimsel Fark Alma)* | **Problem:** Mevsimsel trendler (Ã¶rn. yÄ±ldan yÄ±la artan yaz trafiÄŸi).<br>**Component:** **Seasonal Integrated ($D$)**. | **Detail:** Mevsimsel seviye kaymalarÄ±nÄ± (*Level Shifts*) kaldÄ±rmak iÃ§in fark alÄ±r ($y_t - y_{t-s}$).<br>**Importance:** Veriyi mevsimsel olarak durgunlaÅŸtÄ±rÄ±r (*Seasonally Stationary*). | **Method:**<br>Genellikle $D=1$ yeterlidir (Seriden geÃ§en yÄ±lÄ±n aynÄ± ayÄ±nÄ± Ã§Ä±karma).<br>**Goal:** ACF artÄ±k yavaÅŸ bir azalma gÃ¶stermemelidir. | â€¢ **Canova-Hansen Test:** Mevsimsel kararlÄ±lÄ±k testi.<br>â€¢ **Visual Check:** $s$ periyodunda tekrar eden dalgalarÄ±n dÃ¼zleÅŸmesi. |
| **4. Seasonal Moving Average ($Q$)**<br>*(Mevsimsel Hareketli Ortalama)* | **Problem:** GeÃ§miÅŸ sezonlardaki tahmin hatalarÄ±nÄ±n bugÃ¼ne etkisi.<br>**Component:** **Seasonal MA ($Q$)**. | **Detail:** "KaÃ§ tane mevsimsel hata ÅŸoku (*Error Shocks*) kalÄ±cÄ± oluyor?" (Ã–rn: GeÃ§en AralÄ±k hatasÄ± bu AralÄ±k'Ä± dÃ¼zeltir).<br>**Importance:** Tahminlerin mevsimsel sapmalara karÅŸÄ± direnÃ§li olmasÄ±nÄ± saÄŸlar. | **Calculation:**<br>Model, $t-s$ zamanÄ±ndaki hatayÄ± ($e_{t-s}$) kullanarak revize eder.<br>**Initial Guess:** Mevsimsel ACF grafiÄŸinde $s$ gecikmesinde bÃ¼yÃ¼k sÄ±Ã§rama varsa $Q=1$. | â€¢ **ACF Plot:** $s$ gecikmesindeki (*lag s*) negatif korelasyon veya kesilme noktasÄ±. |
| **5. Model Tuning & Selection**<br>*(Model Ayarlama ve SeÃ§im)* | **Problem:** Toplam 7 parametrenin ($p,d,q,P,D,Q,s$) optimizasyonu.<br>**Component:** **Hyperparameter Tuning**. | **Detail:** Ã‡ok sayÄ±da kombinasyon model karmaÅŸÄ±klÄ±ÄŸÄ±nÄ± artÄ±rÄ±r.<br>**Importance:** YanlÄ±ÅŸ $s$ veya $D$ seÃ§imi, tamamen hatalÄ± tahminlere yol aÃ§ar. | **Grid-Search Strategy:**<br>â€¢ $d$ ve $D$'yi sabit tut.<br>â€¢ KÃ¼Ã§Ã¼k bir komÅŸulukta $(p,q)$ ve $(P,Q)$ kombinasyonlarÄ±nÄ± dene.<br>â€¢ **Auto-ARIMA:** Otomatik deneme. | â€¢ **AIC/BIC:** Model karÅŸÄ±laÅŸtÄ±rma (DÃ¼ÅŸÃ¼k iyidir).<br>â€¢ **Ljung-Box:** Mevsimsel hatalarÄ±n rastgeleliÄŸini test etme. |
| **6. Evaluation & Success Criteria**<br>*(DeÄŸerlendirme ve BaÅŸarÄ± Kriterleri)* | **Problem:** Modelin basit bir tahminden daha iyi olup olmadÄ±ÄŸÄ±nÄ± kanÄ±tlama.<br>**Component:** **Benchmarking**. | **Detail:** Model, "Saf Mevsimsel Referans" (*NaÃ¯ve Seasonal Baseline*) ile kÄ±yaslanmalÄ±dÄ±r.<br>**Importance:** KarmaÅŸÄ±k model basit mantÄ±ÄŸÄ± geÃ§emiyorsa gereksizdir (*over-engineered*). | **Metrics:**<br>â€¢ **AIC:** Model uyumu/karmaÅŸÄ±klÄ±ÄŸÄ± (DÃ¼ÅŸÃ¼k iyidir).<br>â€¢ **MAE/RMSE:** DoÄŸruluk (DÃ¼ÅŸÃ¼k iyidir).<br>**Formula:** $\hat{y}_t = y_{t-s}$ (Baseline). | â€¢ **Out-of-Sample Test:** Tutulan veri setinde test.<br>â€¢ **Residual Check:** Hatalar Beyaz GÃ¼rÃ¼ltÃ¼ (*White Noise*) olmalÄ±dÄ±r. |



---

### ğŸš€ Key Takeaway
*(Temel Ã‡Ä±karÄ±m)*

* **Logic:** **SARIMA = ARIMA + Seasonal Layer**.
* **Application:** You only need it when ACF/PACF show repeating **seasonal spikes** (*tekrarlayan mevsimsel sÄ±Ã§ramalar*).
* **The "S" Factor:** While standard ARIMA handles general trends, **SARIMA** is indispensable when "this December" depends heavily on "last December".
    *(Standart ARIMA genel trendleri yÃ¶netirken, "bu AralÄ±k" ayÄ±nÄ±n bÃ¼yÃ¼k Ã¶lÃ§Ã¼de "geÃ§en AralÄ±k" ayÄ±na baÄŸlÄ± olduÄŸu durumlarda SARIMA vazgeÃ§ilmezdir.)*


# Machine Learning for Time Series: Comparative Analysis
**(Zaman Serileri iÃ§in Makine Ã–ÄŸrenimi: KarÅŸÄ±laÅŸtÄ±rmalÄ± Analiz)**

AÅŸaÄŸÄ±daki tablo, Klasik YÃ¶ntemler (ARIMA, SARIMA, ETS) ile Modern Makine Ã–ÄŸrenimi (ML) yaklaÅŸÄ±mlarÄ± arasÄ±ndaki temel farklarÄ±, teknik gereksinimleri ve Ã§Ã¶zÃ¼m yÃ¶ntemlerini Ã¶zetlemektedir.

| Analiz AlanÄ± <br> (Analysis Area) | Sorunlar & BileÅŸenler <br> (Problems & Components) | Teknik Detay & Ã–nem <br> (Technical Detail & Importance) | Ã‡Ã¶zÃ¼m YÃ¶ntemleri <br> (Solution Methods) | AraÃ§lar & Testler <br> (Tools & Tests) |
| :--- | :--- | :--- | :--- | :--- |
| **Veri Ä°liÅŸkileri ve Desenler** <br> *(Data Relationships & Patterns)* | **Sorun:** Klasik modeller (ARIMA) doÄŸrusal (linear) varsayÄ±mlara dayanÄ±r ve tek deÄŸiÅŸkendir (univariate). <br><br> **Ä°htiyaÃ§:** Fiyat-satÄ±ÅŸ iliÅŸkisi gibi doÄŸrusal olmayan (non-linear) etkileÅŸimleri ve dÄ±ÅŸsal sinyalleri (exogenous signals) modellemek. | **Non-Linear Patterns:** ML modelleri karmaÅŸÄ±k etkileÅŸimleri otomatik Ã¶ÄŸrenir. <br><br> **Covariates:** Hava durumu, promosyonlar gibi yÃ¼zlerce Ã¶zellik (feature) modele dahil edilebilir. | **Denetimli Ã–ÄŸrenme:** Problemi, girdi ve Ã§Ä±ktÄ± matrislerine dÃ¶nÃ¼ÅŸtÃ¼rerek Ã§Ã¶zme. <br><br> **AÄŸaÃ§ TabanlÄ± Modeller:** Veriyi bÃ¶lerek (splitting) heterojen iliÅŸkileri yakalar. | **Algoritmalar:** XGBoost, LightGBM, CatBoost. <br><br> **Test:** Feature Importance (Ã–zellik Ã–nemi) analizi. |
| **Ã–ÄŸrenme Stratejisi** <br> *(Learning Strategy)* | **Sorun:** Klasik yÃ¶ntemlerde her Ã¼rÃ¼n iÃ§in ayrÄ± model (Local Model) gerekir (1000 Ã¼rÃ¼n = 1000 model). <br><br> **Ä°htiyaÃ§:** GeÃ§miÅŸi az olan yeni Ã¼rÃ¼nler iÃ§in tahmin yapabilmek (Cold-Start Problem). | **Global Models & Cross-Learning:** Tek bir model binlerce seriden ortak kalÄ±plarÄ± (shared parameters) Ã¶ÄŸrenir. <br><br> **Forecast Strategy:** Ã–zyinelemeli (recursive) hata birikimi yerine, geleceÄŸi doÄŸrudan tahmin (Direct Multi-step) imkanÄ±. | **Global Havuz:** TÃ¼m serileri tek bir veri setinde birleÅŸtirme. <br><br> **Ã‡apraz Ã–ÄŸrenme:** Benzer Ã¼rÃ¼nlerin davranÄ±ÅŸlarÄ±nÄ± yeni Ã¼rÃ¼nlere transfer etme. | **YaklaÅŸÄ±m:** Global ML Models (vs. Local ARIMA). <br><br> **Strateji:** Direct Multi-step Forecasting. |
| **Veri Ä°ÅŸleme ve MÃ¼hendislik** <br> *(Data Processing & Engineering)* | **Sorun:** ML algoritmalarÄ± zamanÄ±n sÄ±ralÄ± yapÄ±sÄ±nÄ± (sequential nature) doÄŸrudan anlamaz. <br><br> **BileÅŸen:** Veriyi modele "Ã¶ÄŸretmek" iÃ§in Ã¶zellik mÃ¼hendisliÄŸi (Feature Engineering) ÅŸarttÄ±r. | **Input Shape:** Veri 1-D diziden, Ã–zellik Matrisi (X) ve Hedef VektÃ¶re (y) dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼r (Tabular/Matrix). <br><br> **Feature Eng:** Lag (Gecikme), Rolling Mean (Yuvarlanan Ortalama), Calendar Features (Takvim Ã–zellikleri). | **Sliding Windows:** Kayan pencereler ile veriyi yapÄ±landÄ±rma. <br><br> **Manual Engineering:** Zaman bilgisini sÃ¼tunlara dÃ¶nÃ¼ÅŸtÃ¼rme. | **Ä°ÅŸlem:** Windowing, Lagging. <br><br> **UyarÄ±:** Feature Engineering is King (Ã–zellik MÃ¼hendisliÄŸi KraldÄ±r). |
| **DuraÄŸanlÄ±k ve Ekstrapolasyon** <br> *(Stationarity & Extrapolation)* | **Sorun:** AÄŸaÃ§ tabanlÄ± modeller, eÄŸitim verisindeki maksimum deÄŸerin Ã¶tesini tahmin edemez (Cannot Extrapolate). <br><br> **BileÅŸen:** Trend iÃ§eren verilerde sapma riski. | **Stationarity:** Klasik modellerde trend/varyans sabitlenmelidir. ML'de ise trendin arÄ±ndÄ±rÄ±lmasÄ± (Detrending) veya lineer modellerle desteklenmesi kritiktir. <br><br> **Extrapolation Warning:** Trend yukarÄ± giderken aÄŸaÃ§ modeli dÃ¼z bir Ã§izgi tahmin edebilir. | **Hybrid Models:** Ä°statistiÄŸin trend yakalama gÃ¼cÃ¼nÃ¼ ML ile birleÅŸtirme. <br><br> **Preprocessing:** Fark alma (Differencing) veya Log dÃ¶nÃ¼ÅŸÃ¼mÃ¼. | **Modeller:** Prophet (Trend+Regressors), Linear Regression + XGBoost. <br><br> **Metrik:** MSE (Optimization), MAE (Reporting). |
| **Ä°leri Seviye Mimariler** <br> *(Advanced Architectures)* | **Sorun:** SÄ±ralÄ± verilerde uzun vadeli baÄŸÄ±mlÄ±lÄ±klarÄ± (long-term dependencies) yakalamak. <br><br> **BileÅŸen:** HafÄ±za (Memory) ve Dikkat (Attention) mekanizmalarÄ±. | **RNNs:** "Hidden State" ile geÃ§miÅŸi hatÄ±rlar ancak yavaÅŸtÄ±r. <br><br> **Transformers:** "Attention" mekanizmasÄ± ile geÃ§miÅŸteki Ã¶nemli anlara odaklanÄ±r; uzun ufuklu tahminlerde (long-horizon) en iyisidir. | **Deep Learning:** SÄ±ralÄ± iÅŸlem (RNN/LSTM) veya GeniÅŸleyen EvriÅŸimler (TCN/CNN). <br><br> **Attention Mechanism:** Hangi geÃ§miÅŸ verinin Ã¶nemli olduÄŸunu aÄŸÄ±rlÄ±klandÄ±rma. | **Derin Modeller:** LSTM, GRU, TFT (Temporal Fusion Transformer), Informer, N-BEATS. <br><br> **DonanÄ±m:** GPU Acceleration. |


# XGBoost for Time-Series Forecasting: Technical Breakdown
**(Zaman Serisi TahminciliÄŸinde XGBoost: Teknik Ã‡Ã¶zÃ¼mleme)**

XGBoost (Extreme Gradient Boosting), yapÄ±landÄ±rÄ±lmÄ±ÅŸ/tablosal verilerde (structured/tabular data) gÃ¶sterdiÄŸi Ã¼stÃ¼n performans ve mÃ¼hendislik optimizasyonlarÄ± ile bilinen gÃ¼Ã§lÃ¼ bir algoritmadÄ±r. AÅŸaÄŸÄ±daki tablo, zaman serisi baÄŸlamÄ±nda XGBoost'un teknik yeteneklerini ve uygulama yÃ¶ntemlerini Ã¶zetler.

| Analiz AlanÄ± <br> (Analysis Area) | Sorunlar & BileÅŸenler <br> (Problems & Components) | Teknik Detay & Ã–nem <br> (Technical Detail & Importance) | Ã‡Ã¶zÃ¼m YÃ¶ntemleri <br> (Solution Methods) | AraÃ§lar & Testler <br> (Tools & Tests) |
| :--- | :--- | :--- | :--- | :--- |
| **Model Mimarisi ve MantÄ±ÄŸÄ±** <br> *(Model Architecture & Logic)* | **BileÅŸen:** Karar AÄŸaÃ§larÄ± (Decision Trees). <br><br> **Sorun:** Tek baÅŸÄ±na aÄŸaÃ§lar zayÄ±f Ã¶ÄŸrenicilerdir (Weak Learners). | **Ensemble Strategy:** ZayÄ±f aÄŸaÃ§larÄ± bir araya getirerek gÃ¼Ã§lendirir. <br><br> **Gradient Boosting:** SÄ±ralÄ± (sequential) Ã¶ÄŸrenme kullanÄ±r. Her yeni aÄŸaÃ§, Ã¶nceki aÄŸacÄ±n hatasÄ±nÄ± (residuals) dÃ¼zeltir (farklÄ± olarak Random Forest baÄŸÄ±msÄ±z Ã§alÄ±ÅŸÄ±r/bagging). | **Gradient Descent:** HatalarÄ± minimize etmek iÃ§in gradyan iniÅŸi kullanÄ±r. <br><br> **Tree Pruning:** AÄŸacÄ± geriye doÄŸru budayarak (max_depth) gereksiz dallarÄ± temizler. | **Algoritma:** XGBoost (Extreme Gradient Boosting). <br><br> **Kavram:** Boosting vs Bagging. |
| **Performans ve Optimizasyon** <br> *(Performance & Optimization)* | **Sorun:** BÃ¼yÃ¼k verilerde yavaÅŸlÄ±k ve aÅŸÄ±rÄ± Ã¶ÄŸrenme (overfitting) riski. <br><br> **BileÅŸen:** Hesaplama hÄ±zÄ± ve model genelleÅŸtirme yeteneÄŸi. | **Parallel Processing:** Ã–zellikleri paralel iÅŸleyerek hÄ±z kazandÄ±rÄ±r. <br><br> **Regularization:** L1 (Lasso) ve L2 (Ridge) ile model karmaÅŸÄ±klÄ±ÄŸÄ±nÄ± cezalandÄ±rÄ±r, varyans/yanlÄ±lÄ±k dengesini (bias-variance tradeoff) kurar. | **Sparsity-aware Split Finding:** Eksik veriler (missing data) iÃ§in "varsayÄ±lan yÃ¶n" (default direction) Ã¶ÄŸrenir; imputasyon gerektirmez. <br><br> **Objective Function:** Hata fonksiyonunu optimize eder. | **Parametreler:** `reg:squarederror`, `lambda`, `alpha`. <br><br> **Ã–zellik:** High Speed, Low Bias. |
| **Zaman Serisi UygulamasÄ±** <br> *(Time-Series Implementation)* | **Sorun:** XGBoost "zamanÄ±" gÃ¶remez; sÄ±ralÄ± yapÄ±yÄ± anlamaz. <br><br> **BileÅŸen:** Veri hazÄ±rlÄ±ÄŸÄ± ve bÃ¶lÃ¼nmesi. | **Feature Engineering is King:** Zamansal baÄŸÄ±mlÄ±lÄ±klarÄ± anlamak iÃ§in `lag` (gecikme) ve `rolling` (yuvarlanan) Ã¶zelliklere muhtaÃ§tÄ±r. <br><br> **Temporal Split:** Veri rastgele karÄ±ÅŸtÄ±rÄ±lmaz (No Shuffle). Gelecek, geÃ§miÅŸle tahmin edilmelidir (Data Leakage Ã¶nlemi). | **Supervised Learning:** Zaman serisini denetimli Ã¶ÄŸrenme problemine dÃ¶nÃ¼ÅŸtÃ¼rme. <br><br> **Dataset:** Training (Ã¶rn. 2020-22) vs Testing (Ã¶rn. 2023). | **Girdiler:** Lag Features, Rolling Mean. <br><br> **YÃ¶ntem:** Time-based Splitting. |
| **Tahmin KarakteristiÄŸi** <br> *(Prediction Characteristics)* | **Sorun:** Model uÃ§ deÄŸerleri/sÄ±Ã§ramalarÄ± (extreme spikes) yumuÅŸatma eÄŸilimindedir. <br><br> **KÄ±sÄ±t:** Extrapolation Problem (Ekstrapolasyon Sorunu). | **Reason:** AÄŸaÃ§lar, eÄŸitim setinde gÃ¶rdÃ¼ÄŸÃ¼ max/min deÄŸerlerin dÄ±ÅŸÄ±na Ã§Ä±kamaz. Yaprak dÃ¼ÄŸÃ¼mÃ¼n ortalamasÄ±nÄ± tahmin eder. <br><br> **Success:** HaftalÄ±k zirve ve dip zamanlamalarÄ±nÄ± (ritim/seasonality) Ã§ok iyi yakalar. | **Limitations:** Genliklerde (amplitudes) sapma beklenebilir. Trend Ã§ok gÃ¼Ã§lÃ¼yse, trendi ayrÄ± modelleyip (detrending) artÄ±klarÄ± XGBoost'a vermek gerekebilir. | **GÃ¶rselleÅŸtirme:** Actual vs Predicted Plot. <br><br> **Analiz:** Residual Analysis. |
| **Metrikler ve DeÄŸerlendirme** <br> *(Metrics & Evaluation)* | **BileÅŸen:** Modeli eÄŸitirken kullanÄ±lan matematik vs. Ä°nsanlara sunulan rapor. <br><br> **Sorun:** TÃ¼revi alÄ±nabilir fonksiyon ihtiyacÄ± vs. Yorumlanabilirlik. | **Optimization (Training):** `MSE/RMSE`. BÃ¼yÃ¼k hatalarÄ± karesini alarak aÄŸÄ±r cezalandÄ±rÄ±r. TÃ¼revi alÄ±nabilir (Differentiable). <br><br> **Reporting (Evaluation):** `MAE` (Mean Absolute Error). HatayÄ± yorumlanabilir birimlerle ifade eder (Ã¶rn. "Â±50 birim sapma"). | **Objective:** `reg:squarederror` (EÄŸitim iÃ§in). <br><br> **Communication:** PaydaÅŸlara MAE ile raporlama. | **Metrikler:** MSE (Mean Squared Error), MAE, RMSE. <br><br> **Feature Imp:** Gain, Weight, Cover. |

# ğŸ“‰ Deep Learning: A Time-Series Perspective
> **Ã–zet:** Derin Ã–ÄŸrenme (DL), Ã¶zellikle ham veriden otomatik Ã¶zellik Ã§Ä±karma (automated feature extraction) ve karmaÅŸÄ±k/doÄŸrusal olmayan (non-linear) desenleri yakalama konusundaki yeteneÄŸi ile zaman serisi tahminciliÄŸinde (forecasting) devrim yaratmÄ±ÅŸtÄ±r. Geleneksel istatistiksel yÃ¶ntemlerin (ARIMA vb.) aksine, DL modelleri (RNN, LSTM, CNN) bÃ¼yÃ¼k veri setleri ve Ã§ok boyutlu iliÅŸkileri yÃ¶netmede "GÃ¼Ã§ Merkezi" (Powerhouse) gÃ¶revi gÃ¶rÃ¼r.

Bu dokÃ¼man; mimari, problemler, teknik detaylar ve Ã§Ã¶zÃ¼m yÃ¶ntemleri ekseninde derinlemesine bir karÅŸÄ±laÅŸtÄ±rma sunar.

---

## ğŸ“Š Comparative Technical Analysis (KarÅŸÄ±laÅŸtÄ±rmalÄ± Teknik Analiz)

| **Analysis Area** <br> *(Analiz AlanÄ±)* | **Problems & Components** <br> *(Problemler & BileÅŸenler)* | **Technical Detail & Importance** <br> *(Teknik Detay & Ã–nem)* | **Solution Methods** <br> *(Ã‡Ã¶zÃ¼m YÃ¶ntemleri)* | **Tools & Tests** <br> *(AraÃ§lar & Testler)* |
| :--- | :--- | :--- | :--- | :--- |
| **1. DL Architecture & Time-Series Basics** | **Components:**<br>ğŸ”¹ **Input Layer:** Modelin dÃ¼nyaya aÃ§Ä±lan kapÄ±sÄ± (Gecikmeli deÄŸerler/Lags, Rolling stats, Takvim Ã¶zellikleri).<br>ğŸ”¹ **Hidden Layers:** AÄŸÄ±rlÄ±klandÄ±rma ve iÅŸlemenin yapÄ±ldÄ±ÄŸÄ± "gizli" katmanlar.<br>ğŸ”¹ **Output Layer:** Tahmin (Regresyon/SÄ±nÄ±flandÄ±rma) noktasÄ±.<br><br>**Problems:**<br>âŒ Geleneksel yÃ¶ntemlerin (ARIMA) karmaÅŸÄ±k desenlerde yetersiz kalmasÄ±.<br>âŒ Trend ve mevsimselliÄŸi elle ayrÄ±ÅŸtÄ±rma zorunluluÄŸu. | **Importance:**<br>ğŸš€ **Auto Feature Extraction:** Ham veriden desenleri otomatik Ã¶ÄŸrenme.<br>ğŸš€ **Non-Linearity:** `ReLU`, `Sigmoid`, `Tanh` gibi aktivasyonlar ile kaotik iliÅŸkileri modelleme.<br>ğŸš€ **Universal Approx. Theorem:** Teorik olarak her fonksiyonu Ã¶ÄŸrenebilme yeteneÄŸi. | **Methods:**<br>ğŸ›  **DNN (Deep Neural Networks):** Birden fazla gizli katman kullanÄ±mÄ±.<br>ğŸ›  **Global Models:** 1000+ seriyi tek modelde eÄŸitip Ã¼rÃ¼nler arasÄ± iliÅŸkileri (cross-learning) Ã¶ÄŸrenme. | **Frameworks:**<br>ğŸ“¦ `TensorFlow (Keras)`<br>ğŸ“¦ `PyTorch`<br><br>**Tests:**<br>â€¢ Regresyon: Linear Activation.<br>â€¢ SÄ±nÄ±flandÄ±rma: Softmax / Probability. |
| **2. Learning Process & Optimization** | **Components:**<br>ğŸ”¹ **Forward Propagation:** Veri akÄ±ÅŸÄ± (Matris Ã§arpÄ±mÄ± + Aktivasyon).<br>ğŸ”¹ **Loss Function:** Hata hesaplama (Ground Truth vs Prediction).<br>ğŸ”¹ **Back-propagation:** Ã–ÄŸrenmenin kalbi; hatanÄ±n zincir kuralÄ± ile geriye yayÄ±lmasÄ±.<br><br>**Problems:**<br>âŒ BaÅŸlangÄ±Ã§ta rastgele aÄŸÄ±rlÄ±klar = YanlÄ±ÅŸ tahminler.<br>âŒ Overfitting (AÅŸÄ±rÄ± Ã–ÄŸrenme). | **Importance:**<br>ğŸš€ **Gradient Descent:** KayÄ±p fonksiyonunun en dik iniÅŸ yÃ¶nÃ¼nÃ¼ (negatif gradyan) takip etme.<br>ğŸš€ **Iterative Optimization:** Parametrelerin (Weights & Biases) `epochs` boyunca optimize edilmesi. | **Methods:**<br>ğŸ›  **Optimizer:** `Adam` (Adaptive Moment Estimation) - EndÃ¼stri standardÄ±.<br>ğŸ›  **Regularization:** `Dropout` ve `Early Stopping` teknikleri.<br>ğŸ›  **Weight Update:** KÄ±smi tÃ¼revler ile gÃ¼ncelleme. | **Loss Metrics:**<br>ğŸ“‰ Time-Series: `MSE`, `MAE`, `Quantile Loss`.<br>ğŸ“‰ Classification: `Cross-Entropy`. |
| **3. RNNs (Recurrent Neural Networks)** | **Components:**<br>ğŸ”¹ **Hidden State ($h_t$):** AÄŸÄ±n "hafÄ±zasÄ±". Ã–nceki Ã§Ä±ktÄ± ($t-1$), ÅŸimdiki girdinin ($t$) bir parÃ§asÄ±dÄ±r.<br>ğŸ”¹ **Sequential Process:** ZamanÄ± bir boyut olarak iÅŸler.<br><br>**Problems:**<br>âŒ **Vanishing Gradient Problem:** AÄŸÄ±rlÄ±klar < 1 ise hata geriye doÄŸru kÃ¼Ã§Ã¼lÃ¼r, gradyan kaybolur.<br>âŒ **Short-Term Memory:** AÄŸ sadece yakÄ±n geÃ§miÅŸi hatÄ±rlar, uzak geÃ§miÅŸi "unutur". | **Technical Detail:**<br>ğŸš€ **Looping Structure:** Kendi Ã§Ä±ktÄ±sÄ±nÄ± kendine girdi olarak veren dÃ¶ngÃ¼sel yapÄ±.<br>ğŸš€ **Difference:** FFN (Ä°leri Beslemeli) aÄŸlar anlÄ±ktÄ±r; RNN'ler tarihsel baÄŸlamÄ± korur.<br>ğŸ“ **Formula:** $h_t = \tanh(W_h \cdot h_{t-1} + W_x \cdot x_t)$ | **Methods:**<br>ğŸ›  **BPTT:** Backpropagation Through Time (Zaman iÃ§inde geri yayÄ±lÄ±m).<br>ğŸ›  **Seq2Seq:** SÄ±ralÄ± modelleme iÃ§in temel yapÄ± taÅŸÄ±. | **Status:**<br>âš ï¸ Vanilla RNN'ler genellikle modern projelerde yerini LSTM/GRU mimarilerine bÄ±rakmÄ±ÅŸtÄ±r. |
| **4. LSTMs (Long Short-Term Memory)** | **Components (The Mini-Factory):**<br>ğŸ”¹ **Forget Gate:** Gereksiz bilgiyi siler (Sigmoid: 0=Unut, 1=Sakla).<br>ğŸ”¹ **Input Gate:** Hangi yeni bilginin saklanacaÄŸÄ±nÄ± seÃ§er.<br>ğŸ”¹ **Cell State:** Uzun vadeli hafÄ±za ÅŸeridi (Conveyor Belt).<br>ğŸ”¹ **Output Gate:** O anki tahmini aÃ§Ä±ÄŸa Ã§Ä±karÄ±r.<br><br>**Problems Solved:**<br>âœ… Vanishing Gradient Problemi.<br>âœ… Uzun vadeli baÄŸÄ±mlÄ±lÄ±k kaybÄ±. | **Importance:**<br>ğŸš€ **Gating Mechanism:** Bilgi akÄ±ÅŸÄ±nÄ± kapÄ±larla kontrol etme.<br>ğŸš€ **Dual Capability:** KÄ±sa vadeli gÃ¼rÃ¼ltÃ¼yÃ¼ (haftalÄ±k dalgalanma) ve uzun vadeli trendi (yÄ±llÄ±k mevsimsellik) aynÄ± anda yÃ¶netir.<br>ğŸš€ **Resilience:** Ani ÅŸoklarÄ± ve rejim deÄŸiÅŸikliklerini modelleyebilir. | **Methods:**<br>ğŸ›  **Selectivity:** Ã‡eliÅŸkili gÃ¶revleri yÃ¶netme (GÃ¼rÃ¼ltÃ¼yÃ¼ unut, Trendi hatÄ±rla).<br>ğŸ›  **Retail Context:** "GeÃ§en yÄ±lki Noel etkisini sakla, dÃ¼nkÃ¼ rastgele dÃ¼ÅŸÃ¼ÅŸÃ¼ unut." | **Libraries:**<br>ğŸ“¦ `PyTorch Forecasting`<br>ğŸ“¦ `Darts`<br><br>**Use Cases:**<br>ğŸ“ˆ SatÄ±ÅŸ Tahmini<br>ğŸ’¹ Finans/Borsa<br>ğŸŒ¦ Hava Durumu<br>âš¡ Enerji TÃ¼ketimi |
| **5. Challenges & Advanced Views** | **Problems:**<br>âŒ **Data Hunger:** "KÃ¼Ã§Ã¼k Veri" (Small Data) ile klasik yÃ¶ntemlerden kÃ¶tÃ¼ Ã§alÄ±ÅŸabilir.<br>âŒ **Computational Cost:** GPU/TPU gereksinimi.<br>âŒ **Black Box:** Yorumlanabilirlik (Interpretability) zorluÄŸu.<br><br>**Components:**<br>ğŸ”¹ High-Dimensional Data handling. | **Importance:**<br>ğŸš€ **Unstructured Data:** GÃ¶rÃ¼ntÃ¼, metin ve zaman serisini birleÅŸtirebilme.<br>ğŸš€ **Generalization:** GÃ¶rÃ¼lmemiÅŸ verilere genelleme yeteneÄŸi. | **Modern Architectures:**<br>ğŸ›  **Transformers:** Attention mekanizmasÄ± (Ã–rn: TFT - Temporal Fusion Transformer) ile yorumlanabilirliÄŸi artÄ±rma.<br>ğŸ›  **1D-CNNs:** Zaman iÃ§indeki yerel desenleri filtreleme. | **Hardware:**<br>ğŸ’» NVIDIA GPUs<br>ğŸ’» Google TPUs |

---

## ğŸ§  Deep Dive: RNN vs LSTM Logic

### ğŸ”„ Recurrent Neural Networks (RNN)
* **MantÄ±k:** Ä°nsan hafÄ±zasÄ±nÄ± taklit etmeye Ã§alÄ±ÅŸÄ±r ancak "dikkat sÃ¼resi" kÄ±sadÄ±r.
* **KÄ±sÄ±t:** Zincirleme Ã§arpÄ±m kuralÄ± yÃ¼zÃ¼nden, hata geriye doÄŸru yayÄ±lÄ±rken sÃ¶nÃ¼mlenir (**Vanishing Gradient**). Bu, aÄŸÄ±n serinin baÅŸÄ±ndaki olaylarÄ± Ã¶ÄŸrenmesini engeller.

### ğŸ”‹ Long Short-Term Memory (LSTM)
RNN'in "UnutkanlÄ±k" sorununa Ã§Ã¶zÃ¼mdÃ¼r. HÃ¼cre iÃ§inde **4 Temel BileÅŸen** barÄ±ndÄ±ran bir "Mini Fabrika" gibi Ã§alÄ±ÅŸÄ±r:

1.  **Forget Gate (Unutma KapÄ±sÄ±):** "GeÃ§miÅŸten gelen hangi bilgi artÄ±k Ã§Ã¶p?" (Ã–rn: GeÃ§en haftaki tek seferlik promosyon).
2.  **Input Gate (Girdi KapÄ±sÄ±):** "Åu anki olaylardan hangisi gelecekte iÅŸime yarar?" (Ã–rn: BugÃ¼n baÅŸlayan Bayram tatili).
3.  **Cell State (HÃ¼cre Durumu):** Bilgilerin bozulmadan aktÄ±ÄŸÄ± otoban/taÅŸÄ±ma bandÄ±.
4.  **Output Gate (Ã‡Ä±ktÄ± KapÄ±sÄ±):** "Åu an dÄ±ÅŸ dÃ¼nyaya ne sÃ¶ylemeliyim?"

> **Expert Insight:** Modern zaman serisi problemlerinde (Ã¶zellikle Retail ve Finans), verinin hem **dÃ¶ngÃ¼sel** (mevsimsellik) hem de **kaotik** (ÅŸoklar) doÄŸasÄ±nÄ± yakalamak iÃ§in LSTM veya Transformer tabanlÄ± mimariler fiili standarttÄ±r.
