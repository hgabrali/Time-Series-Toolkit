


# PART 1: ğŸš‚ Loading the Train Data: Step-by-Step Process
*(EÄŸitim Verisini YÃ¼kleme: AdÄ±m AdÄ±m SÃ¼reÃ§)*

Ã–nceki aÅŸamada Ã§oÄŸu dosya baÅŸarÄ±yla yÃ¼klenmiÅŸti, ancak maÄŸaza satÄ±ÅŸlarÄ±nÄ±n zaman serisi verilerini (*time-series data*) iÃ§eren `train.csv` dosyasÄ± hariÃ§. Bu dosya oldukÃ§a bÃ¼yÃ¼ktÃ¼r ve standart yÃ¶ntemlerle okunmasÄ± zordur.

Bu bÃ¶lÃ¼mde, bÃ¼yÃ¼k veri dosyalarÄ±nÄ± yÃ¶netmek iÃ§in **"Chunking"** (ParÃ§alara AyÄ±rma) ve **"Sampling"** (Ã–rnekleme) stratejilerini uyguladÄ±k. Ä°ÅŸte adÄ±m adÄ±m uygulanan iÅŸlemler:

---

### 1. Install the `gdown` Library
*(gdown KÃ¼tÃ¼phanesinin Kurulumu)*
* **Ä°ÅŸlem (Action):** `gdown` kÃ¼tÃ¼phanesini ortamÄ±mÄ±za kurduk.
* **Neden? (Why?):** BÃ¼yÃ¼k dosyamÄ±z Google Drive Ã¼zerinde barÄ±nmaktadÄ±r. Standart `requests` kÃ¼tÃ¼phanesi yerine, bÃ¼yÃ¼k dosyalarÄ± indirme (*large file download*) konusunda daha yetenekli ve stabil olan `gdown` aracÄ±nÄ± tercih ettik.

### 2. Download the `train.csv` File
*(train.csv DosyasÄ±nÄ±n Ä°ndirilmesi)*
* **Ä°ÅŸlem (Action):** DosyayÄ± indirdik ve Ã§alÄ±ÅŸma ortamÄ±mÄ±za `train.csv` adÄ±yla kaydettik.
* **Neden? (Why?):** Veriyi iÅŸleyebilmek iÃ§in yerel ortama (*local environment*) veya Colab diskine alÄ±nmasÄ± gerekiyordu.

### 3. Select Stores from the "Pichincha" Region
*(Pichincha BÃ¶lgesinden MaÄŸazalarÄ±n SeÃ§imi)*
* **Ä°ÅŸlem (Action):** Analiz kapsamÄ±mÄ±zÄ± daraltmak iÃ§in maÄŸaza listesini filtreledik ve sadece "Pichincha" bÃ¶lgesindeki maÄŸazalarÄ± seÃ§tik.
* **Neden? (Why?):** TÃ¼m veri yerine belirli bir bÃ¶lgeye odaklanarak analizi daha yÃ¶netilebilir hale getirdik (*region filtering*).

### 4. Read the Data in Chunks and Filter by Store
*(Veriyi ParÃ§alar Halinde Okuma ve MaÄŸazaya GÃ¶re Filtreleme)*
* **Ä°ÅŸlem (Action):** `train.csv` dosyasÄ± Ã§ok bÃ¼yÃ¼k olduÄŸu iÃ§in tek seferde okumak yerine, 1 milyon satÄ±rlÄ±k **parÃ§alar halinde** (*chunks*) okuduk. Her parÃ§a okunduÄŸunda, sadece yukarÄ±da seÃ§tiÄŸimiz "Pichincha" maÄŸazalarÄ±na ait satÄ±rlarÄ± tuttuk.
* **Neden? (Why?):** Bellek yÃ¶netimi (*memory management*) iÃ§in kritiktir. TÃ¼m dosyayÄ± RAM'e yÃ¼klemek yerine, parÃ§a parÃ§a iÅŸleyip gereksiz veriyi anÄ±nda elemek (*on-the-fly filtering*) sistemin Ã§Ã¶kmesini engeller.

### 5. Combine the Chunks
*(ParÃ§alarÄ±n BirleÅŸtirilmesi)*
* **Ä°ÅŸlem (Action):** FiltrelenmiÅŸ parÃ§alarÄ± (*filtered chunks*) tek bir DataFrame Ã§atÄ±sÄ± altÄ±nda birleÅŸtirdik (*concatenation*).
* **Neden? (Why?):** Analiz ve modelleme aÅŸamasÄ±nda bÃ¼tÃ¼ncÃ¼l bir veri seti Ã¼zerinde Ã§alÄ±ÅŸabilmek iÃ§in parÃ§alarÄ± tekrar bir araya getirmemiz gerekiyordu.

### 6. Sample 2 Million Rows
*(2 Milyon SatÄ±rÄ±n Ã–rneklenmesi)*
* **Ä°ÅŸlem (Action):** FiltrelenmiÅŸ veriden rastgele 2 milyon satÄ±r seÃ§tik (*random sampling*).
* **Neden? (Why?):** EÄŸitim amaÃ§lÄ± (*educational sake*) hesaplamalarÄ± hÄ±zlandÄ±rmak iÃ§in. GerÃ§ek dÃ¼nyada tÃ¼m veri kullanÄ±labilir ancak Ã¶ÄŸrenme sÃ¼recinde iÅŸlem sÃ¼resini kÄ±saltmak (*computation speed*) iÃ§in veri boyutu optimize edildi.

### 7. Clean Up
*(Temizlik)*
* **Ä°ÅŸlem (Action):** Bellekte yer kaplayan geÃ§ici parÃ§a listelerini sildik.
* **Neden? (Why?):** Python'Ä±n bellek yÃ¶netimini rahatlatmak (*garbage collection*) ve RAM'i verimli kullanmak iÃ§in gereksiz deÄŸiÅŸkenler temizlendi.

### 8. Verification
*(DoÄŸrulama)*
* **Ä°ÅŸlem (Action):** OluÅŸturulan DataFrame'lerin ilk satÄ±larÄ±nÄ± (`head`) yazdÄ±rarak kontrol ettik.
* **Neden? (Why?):** Verinin dÃ¼zgÃ¼n yÃ¼klenip yÃ¼klenmediÄŸini ve formatÄ±n beklediÄŸimiz gibi olup olmadÄ±ÄŸÄ±nÄ± teyit etmek (*sanity check*).

---

# PART 2: 
