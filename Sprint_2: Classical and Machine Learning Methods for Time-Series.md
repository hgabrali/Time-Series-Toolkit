# ğŸ“…  Classical and Machine Learning Methods for Time-Series
*(Hafta 2: Zaman Serileri iÃ§in Klasik ve Makine Ã–ÄŸrenimi YÃ¶ntemleri)*

> **Date Range:** Dec 8 - Dec 15

In this module, we will explore both **classical time-series methods** (*klasik zaman serisi yÃ¶ntemleri*) and **machine learning approaches** (*makine Ã¶ÄŸrenimi yaklaÅŸÄ±mlarÄ±*) for forecasting. We aim to bridge the gap between traditional statistical theory and modern predictive algorithms.
*(Bu modÃ¼lde, tahminleme iÃ§in hem klasik zaman serisi yÃ¶ntemlerini hem de makine Ã¶ÄŸrenimi yaklaÅŸÄ±mlarÄ±nÄ± inceleyeceÄŸiz. Geleneksel istatistiksel teori ile modern tahmin algoritmalarÄ± arasÄ±ndaki boÅŸluÄŸu doldurmayÄ± hedefliyoruz.)*

---

## ğŸ¯ Learning Objectives & Key Outcomes
*(Ã–ÄŸrenme Hedefleri ve Temel Ã‡Ä±karÄ±mlar)*

By the end of this week, you will be able to:
*(Bu haftanÄ±n sonunda ÅŸunlarÄ± yapabileceksiniz:)*

### 1. ğŸ“‰ Classical Time-Series Modelling
*(Klasik Zaman Serisi Modellemesi)*
* **Implement models:** Build and tune classical statistical models like **ARIMA** (*AutoRegressive Integrated Moving Average*) and **SARIMA** (*Seasonal ARIMA*).
    *(ARIMA ve SARIMA gibi klasik istatistiksel modelleri kurma ve ayarlama.)*
* **Core Concepts:** Understand **stationarity** (*durgunluk*), **differencing** (*fark alma*) and **seasonality** (*mevsimsellik*) handling in linear models.
    *(Lineer modellerde durgunluk, fark alma ve mevsimsellik yÃ¶netimini anlama.)*

### 2. ğŸŒ² Machine Learning Approaches
*(Makine Ã–ÄŸrenimi YaklaÅŸÄ±mlarÄ±)*
* **Apply Tree-Based Models:** Utilize powerful algorithms like **XGBoost**, **LightGBM**, or **Random Forest** for time-series forecasting.
    *(Zaman serisi tahmini iÃ§in XGBoost, LightGBM veya Random Forest gibi gÃ¼Ã§lÃ¼ algoritmalarÄ± kullanma.)*
* **Handling Non-Linearity:** Learn how these models capture non-linear relationships better than traditional methods.
    *(Bu modellerin doÄŸrusal olmayan iliÅŸkileri geleneksel yÃ¶ntemlerden nasÄ±l daha iyi yakaladÄ±ÄŸÄ±nÄ± Ã¶ÄŸrenme.)*

### 3. ğŸ§  Deep Learning Foundations
*(Derin Ã–ÄŸrenme Temelleri)*
* **Introduction to Neural Networks:** Get familiar with deep learning approaches tailored for sequential data, specifically:
    *(SÄ±ralÄ± veriler iÃ§in Ã¶zelleÅŸtirilmiÅŸ derin Ã¶ÄŸrenme yaklaÅŸÄ±mlarÄ±na aÅŸina olma, Ã¶zellikle:)*
    * **RNNs** (*Recurrent Neural Networks - Tekrarlayan Sinir AÄŸlarÄ±*)
    * **LSTMs** (*Long Short-Term Memory - Uzun KÄ±sa SÃ¼reli Bellek AÄŸlarÄ±*)
* **Sequence Modeling:** Understand how these networks manage **long-term dependencies** (*uzun vadeli baÄŸÄ±mlÄ±lÄ±klar*) in time series.

### 4. ğŸ› ï¸ Advanced Feature Engineering
*(Ä°leri Seviye Ã–zellik MÃ¼hendisliÄŸi)*
* **Data Preprocessing:** Perform preprocessing specifically tailored for supervised machine learning models.
    *(GÃ¶zetimli makine Ã¶ÄŸrenimi modelleri iÃ§in Ã¶zel olarak uyarlanmÄ±ÅŸ Ã¶n iÅŸleme gerÃ§ekleÅŸtirme.)*
* **Creating Signals:** Generate powerful features such as:
    *(GÃ¼Ã§lÃ¼ Ã¶zellikler Ã¼retme:)*
    * **Lag Features** (*Gecikmeli Ã–zellikler*)
    * **Rolling Window Statistics** (*Kayan Pencere Ä°statistikleri*)
    * **Time-Based Components** (*Zaman BazlÄ± BileÅŸenler - GÃ¼n, Ay, YÄ±l vb.*)

### 5. âš–ï¸ Strategic Comparison
*(Stratejik KarÅŸÄ±laÅŸtÄ±rma)*
* **Critical Analysis:** Understand the **differences**, **benefits**, and **challenges** of classical statistical methods versus machine learning approaches.
    *(Klasik istatistiksel yÃ¶ntemler ile makine Ã¶ÄŸrenimi yaklaÅŸÄ±mlarÄ± arasÄ±ndaki farklarÄ±, faydalarÄ± ve zorluklarÄ± anlama.)*
* **Decision Making:** Learn when to use which method based on data size, interpretability requirements, and computational resources.
    *(Veri boyutu, yorumlanabilirlik gereksinimleri ve hesaplama kaynaklarÄ±na baÄŸlÄ± olarak hangi yÃ¶ntemin ne zaman kullanÄ±lacaÄŸÄ±nÄ± Ã¶ÄŸrenme.)*

-------
-------
# ğŸ¯ Preparing Data & Introduction to Darts
*(Veri HazÄ±rlama ve Darts'a GiriÅŸ)*

Modelleme aÅŸamasÄ±na geÃ§meden Ã¶nce, Ã¼zerinde Ã§alÄ±ÅŸmaya hazÄ±r, temiz bir veri setine ihtiyacÄ±mÄ±z vardÄ±r. Bu bÃ¶lÃ¼mde, klasik **ARIMA/SARIMA** modellerini uygulamak iÃ§in Python'un gÃ¼Ã§lÃ¼ zaman serisi kÃ¼tÃ¼phanesi **Darts**'Ä± kullanacaÄŸÄ±z. Veri seti olarak yine "CorporaciÃ³n Favorita Grocery Sales Forecasting" verilerini kullanÄ±yoruz.

---

## ğŸ§ What is DARTS? Why use it?

**DARTS**, zaman serisi tahmini (*time-series forecasting*) iÃ§in Ã¶zel olarak tasarlanmÄ±ÅŸ, kullanÄ±mÄ± kolay ve Ã¼st dÃ¼zey (*high-level*) bir Python kÃ¼tÃ¼phanesidir.

> **ğŸ’¡ The Core Philosophy (Temel Felsefe):**
> DARTS, `scikit-learn` kÃ¼tÃ¼phanesinin kullanÄ±cÄ± dostu yapÄ±sÄ±nÄ± (fit/predict mantÄ±ÄŸÄ±) zaman serilerine uyarlar. KarmaÅŸÄ±k modelleri (ARIMA'dan Derin Ã–ÄŸrenme Transformer'larÄ±na kadar) tek bir satÄ±r kodla deÄŸiÅŸtirmenize ve test etmenize olanak tanÄ±r.

### Key Features (Temel Ã–zellikler)
1.  **Unified API (BirleÅŸik ArayÃ¼z):** Klasik istatistiksel modellerden (*ARIMA, Exponential Smoothing*) modern makine Ã¶ÄŸrenimi (*XGBoost, LightGBM*) ve derin Ã¶ÄŸrenme modellerine (*N-BEATS, LSTM, Transformers*) kadar her ÅŸeyi aynÄ± arayÃ¼zle kullanabilirsiniz.
2.  **Multivariate Support (Ã‡ok DeÄŸiÅŸkenli Destek):** Sadece hedef deÄŸiÅŸkeni deÄŸil, dÄ±ÅŸsal faktÃ¶rleri (*past/future covariates*) de modele kolayca dahil edebilirsiniz (Ã¶rn. Tatiller, Petrol FiyatlarÄ±).
3.  **Backtesting & Evaluation (Geriye DÃ¶nÃ¼k Test ve DeÄŸerlendirme):** Modelin geÃ§miÅŸ performansÄ±nÄ± simÃ¼le etmek iÃ§in gÃ¼Ã§lÃ¼ araÃ§lar sunar.
4.  **Probabilistic Forecasting (OlasÄ±lÄ±ksal Tahmin):** Sadece tek bir deÄŸer deÄŸil, gÃ¼ven aralÄ±klarÄ± (*confidence intervals*) ve olasÄ±lÄ±k daÄŸÄ±lÄ±mlarÄ± Ã¼retebilir.

---

## ğŸ› ï¸ Step-by-Step Workflow


### Step 0: Installing Darts
*(AdÄ±m 0: Darts Kurulumu)*

We will illustrate how to use classical time-series methods like ARIMA for forecasting using the DARTS library. First, ensure the library is installed in your environment.
*(DARTS kÃ¼tÃ¼phanesini kullanarak tahminleme iÃ§in ARIMA gibi klasik yÃ¶ntemlerin nasÄ±l kullanÄ±lacaÄŸÄ±nÄ± gÃ¶stereceÄŸiz. Ã–ncelikle kÃ¼tÃ¼phanenin ortamÄ±nÄ±zda kurulu olduÄŸundan emin olun.)*

### Step 1: Loading a Single Storeâ€“Item Series
*(AdÄ±m 1: Tek Bir MaÄŸaza-ÃœrÃ¼n Serisini YÃ¼kleme)*

Rather than juggling millions of rows, weâ€™ll extract **one product in one store** and truncate everything after March 31, 2014.
*(Milyonlarca satÄ±rla boÄŸuÅŸmak yerine, tek bir maÄŸazadaki tek bir Ã¼rÃ¼nÃ¼ Ã§Ä±karacaÄŸÄ±z ve 31 Mart 2014'ten sonraki verileri keseceÄŸiz.)*

* **Goal (Hedef):** Create a manageable dataset to learn the mechanics of the model.
* **Action (Eylem):** Filter by `store_nbr`, `item_nbr`, and apply `date < '2014-04-01'`.
* **Result (SonuÃ§):** A tidy DataFrame with just daily sales for our chosen series. (*SeÃ§ilen serimiz iÃ§in sadece gÃ¼nlÃ¼k satÄ±ÅŸlarÄ± iÃ§eren dÃ¼zenli bir DataFrame.*)

### Step 2: Prepare & Convert to `TimeSeries`
*(AdÄ±m 2: HazÄ±rlÄ±k ve `TimeSeries` Nesnesine DÃ¶nÃ¼ÅŸtÃ¼rme)*

This is the most critical step specific to Darts. Darts works with its own object type called `TimeSeries`, not standard Pandas DataFrames.
*(Bu, Darts'a Ã¶zgÃ¼ en kritik adÄ±mdÄ±r. Darts, standart Pandas DataFrame'leri ile deÄŸil, `TimeSeries` adÄ± verilen kendi nesne tÃ¼rÃ¼yle Ã§alÄ±ÅŸÄ±r.)*

**The Process:**
1.  **Datetime Conversion:** Convert the `date` column to datetime objects and set it as the index.
    *(Tarih sÃ¼tununu datetime nesnelerine Ã§evirin ve indeks olarak ayarlayÄ±n.)*
2.  **Aggregation:** Aggregate by date (summing all `unit_sales` for that day).
    *(Tarihe gÃ¶re toplulaÅŸtÄ±rma yapÄ±n [o gÃ¼nkÃ¼ tÃ¼m birim satÄ±ÅŸlarÄ± toplayarak].)*
3.  **Gap Filling (Reindexing):** Reindex to a complete daily calendar, filling any missing dates with **zero**.
    *(Eksik tarihleri sÄ±fÄ±r ile doldurarak tam bir gÃ¼nlÃ¼k takvime gÃ¶re yeniden indeksleyin.)*
4.  **Darts Conversion:** Finally, wrap it in Dartsâ€™ `TimeSeries` object using `TimeSeries.from_dataframe()`.
    *(Son olarak, `TimeSeries.from_dataframe()` kullanarak veriyi Darts TimeSeries nesnesine sarÄ±n.)*

> **âœ… Why?** This ensures all the libraryâ€™s modeling and backtesting tools work **out of the box** (*kurulum gerektirmeden/hazÄ±r olarak*).

---

### ğŸ“Š Visual Analysis & Interpretation
*(GÃ¶rsel Analiz ve Yorumlama)*



#### ğŸ’¡ Think First!


Before reading our interpretation, take a moment to reflect on the chart yourself:
*(Yorumumuzu okumadan Ã¶nce, grafik Ã¼zerinde dÃ¼ÅŸÃ¼nmek iÃ§in bir dakikanÄ±zÄ± ayÄ±rÄ±n:)*
* What do you notice about the **sales volume**? Is it consistent, volatile, or trending?
* Do the **spikes** (*sÄ±Ã§ramalar*) follow any visible pattern?
* What kinds of **features** might help the model capture this behavior?

#### ğŸ“‰ Our Analysis


**1. Key Observations (Temel GÃ¶zlemler)**
* **Consistently High Sales:** The product sells every day, typically between 300 and 800 units. It is a **high-volume item**.
* **No Missing/Zero Values:** We see activity on every day. This is ideal for training a model that learns from historical behavior without needing complex imputation (*eksik veri doldurma*).
* **Frequent, Sharp Fluctuations:** The series is **noisy** (*gÃ¼rÃ¼ltÃ¼lÃ¼*) â€” it goes up and down regularly â€” but mostly within a predictable range.
* **Occasional Large Peaks:** Some spikes rise sharply (>1000) above the usual range. These likely correspond to **promotions**, **holidays**, or **special events**.

**2. What This Suggests for Forecasting (Tahminleme Ä°Ã§in Ne Anlama Geliyor?)**
* **Lag Features:** The model will benefit from lags (e.g., `sales_lag_1`, `rolling_mean_7`) to learn local dynamics.
* **Calendar Features:** Including `day_of_week`, `is_weekend`, or `month` will help capture recurring patterns (*tekrarlayan kalÄ±plar*).
* **Volatility Handling:** Applying a **rolling average** or using a **log transformation** might improve model stability against noise.
* **External Signals:** To predict the huge spikes, adding **promotions** data (as *covariates*) would be valuable.

---

### Step 3: Splitting the Data into Training and Testing Sets
*(AdÄ±m 3: Veriyi EÄŸitim ve Test Setlerine AyÄ±rma)*

In time-series forecasting, we cannot use random splitting (like `train_test_split` in sklearn) because the **order of data matters**. We must split chronologically.
*(Zaman serisi tahmininde, rastgele bÃ¶lme kullanamayÄ±z Ã§Ã¼nkÃ¼ verinin sÄ±rasÄ± Ã¶nemlidir. Kronolojik olarak bÃ¶lmemiz gerekir.)*

* **Training Set (EÄŸitim Seti):** The past data used to teach the model patterns (e.g., usually the first 80-90% of the timeline).
    *(Model kalÄ±plarÄ±nÄ± Ã¶ÄŸretmek iÃ§in kullanÄ±lan geÃ§miÅŸ veriler.)*
* **Validation/Test Set (DoÄŸrulama/Test Seti):** The recent data used to evaluate how well the model predicts the future (the remaining 10-20%).
    *(Modelin geleceÄŸi ne kadar iyi tahmin ettiÄŸini deÄŸerlendirmek iÃ§in kullanÄ±lan son veriler.)*

**Darts Method:**
We use specific Darts methods (like `.split_before()`) to ensure no **data leakage** (*veri sÄ±zÄ±ntÄ±sÄ±*) occursâ€”meaning the model never sees the "future" it is trying to predict during training.

---
---

# ğŸ•°ï¸ Classical Time-Series Methods: ARIMA & Parameter `d`
*(Klasik Zaman Serisi YÃ¶ntemleri: ARIMA ve d Parametresi)*

**ARIMA** (*AutoRegressive Integrated Moving Average*), gÃ¼Ã§lÃ¼ mevsimsel kalÄ±plarÄ± olmayan zaman serisi verilerini anlamak ve tahmin etmek iÃ§in kullanÄ±lan temel bir modeldir.

---

## ğŸ§ How Does ARIMA Work?
*(ARIMA NasÄ±l Ã‡alÄ±ÅŸÄ±r?)*

Imagine youâ€™re trying to forecast the daily sales in a grocery store. ARIMA helps predict the sales for tomorrow by combining three components:
*(Bir marketin gÃ¼nlÃ¼k satÄ±ÅŸlarÄ±nÄ± tahmin etmeye Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ±zÄ± hayal edin. ARIMA, Ã¼Ã§ bileÅŸeni birleÅŸtirerek yarÄ±nÄ±n satÄ±ÅŸlarÄ±nÄ± tahmin etmeye yardÄ±mcÄ± olur:)*

1.  **AR (AutoRegression):** Looking at how past daysâ€™ sales have behaved.
    *(GeÃ§miÅŸ gÃ¼nlerin satÄ±ÅŸlarÄ±nÄ±n nasÄ±l davrandÄ±ÄŸÄ±na bakmak [Otokorelasyon].)*
2.  **I (Integrated):** Correcting for any trends via differencing.
    *(Fark alma yoluyla trendleri dÃ¼zeltmek [Entegrasyon].)*
3.  **MA (Moving Average):** Learning from the errors made in previous predictions.
    *(Ã–nceki tahminlerde yapÄ±lan hatalardan Ã¶ÄŸrenmek [Hareketli Ortalama].)*

> â˜ğŸ¼ **Summary:** ARIMA tries to use the **past values** (AR) and **past prediction errors** (MA) to make accurate forecasts, while adjusting for **trends** in the data (I).

---

## ğŸ“‰ The "I" in ARIMA and Parameter `d`
*(ARIMA'daki "I" ve d Parametresi)*

Sometimes data isnâ€™t "**stationary**," meaning it has a trend that increases or decreases over time.
*(Bazen veriler "durgun" deÄŸildir, yani zamanla artan veya azalan bir trende sahiptir.)*

* **The Integrated (I) part** helps by removing these trends to make the data easier to predict.
* **Method:** It does this by **differencing**â€”subtracting the previous value from the current value to smooth out the trend.
    *(YÃ¶ntem: Bunu fark alma yoluyla yaparâ€”trendi yumuÅŸatmak iÃ§in Ã¶nceki deÄŸeri mevcut deÄŸerden Ã§Ä±karÄ±r.)*



> **Definition:** The `d` in `ARIMA(p,d,q)` tells us **how many times** to difference the series to remove a trend and achieve stationarity (constant mean & variance).

---

## ğŸ› ï¸ Step-by-Step Guide: Choosing `d`
*(AdÄ±m AdÄ±m Rehber: d SeÃ§imi)*

### Step 1: Start with No Differencing (`d=0`)
*(AdÄ±m 1: Fark Alma Olmadan BaÅŸla)*

#### 1. Visual Check: Plot Your Raw Series
*(GÃ¶rsel Kontrol: Ham Seriyi Ã‡izdir)*
* **Visual Patterns & Drift:** Does a gentle upward or downward drift hide underneath the spikes?
    *(SÄ±Ã§ramalarÄ±n altÄ±nda hafif bir yukarÄ± veya aÅŸaÄŸÄ± sÃ¼rÃ¼klenme gizleniyor mu?)*
* **Mean Level:** Does it look like the mean level is roughly constant, or does it trend up/down?
    *(Ortalama seviye kabaca sabit mi gÃ¶rÃ¼nÃ¼yor, yoksa yukarÄ±/aÅŸaÄŸÄ± trend mi var?)*
* **Stationarity Assessment:** Based on your visual impression, would you call this series â€œstationaryâ€?

> **Our Analysis (Example):**
> * **Visual:** The series shows strong short-term fluctuations but no clear long-term trend. The mean level appears fairly stable around 500â€“600 units/day.
> * **Conclusion:** This series appears visually stationary, or at least close enough for many forecasting models (d=0).

#### 2. Rolling Mean Check
*(Hareketli Ortalama KontrolÃ¼)*
Letâ€™s smooth out the day-to-day spikes with a **30-day rolling average**. By averaging over a full month, the noisy zero-and-spike pattern flattens out, revealing whether there really is a gradual trend.



> **Analysis:** The mean level drops early in 2013 but stays stable (430â€“480 units) afterwards. It may be treated as **near-stationary**.

#### 3. Statistical Check: ADF Test
*(Ä°statistiksel Kontrol: ADF Testi)*

**What is ADF Test?** (*Augmented Dickey-Fuller Test*)
It is a statistical test used to check for **stationarity**.
* **Null Hypothesis ($H_0$):** The series has a unit root (it is **not stationary**).
* **Alternative Hypothesis ($H_1$):** The series has no unit root (it is **stationary**).

**Decision Rule:**
* If **p-value < 0.05**: Reject $H_0$ â†’ Series is **Stationary**. (Accept `d=0`)
* If **p-value â‰¥ 0.05**: Fail to reject $H_0$ â†’ Series is **Non-Stationary**. (Try `d=1`)

```python
# Code snippet for ADF Test
# Result Example:
# p-value: 0.000467...
```

## ğŸ“‰ Step 2: If Not Stationary, Try One Difference (`d=1`)
*(AdÄ±m 2: Durgun DeÄŸilse, Bir Fark AlmayÄ± Dene)*

If the visual check showed a **trend** or the **ADF p-value** was **â‰¥ 0.05**:
*(EÄŸer gÃ¶rsel kontrol bir trend gÃ¶sterdiyse veya ADF p-deÄŸeri â‰¥ 0.05 ise:)*

### ğŸ› ï¸ Process (SÃ¼reÃ§)

1.  **Compute the First Difference:**
    *(Birinci FarkÄ± Hesapla:)*
    $$y'_t = y_t - y_{t-1}$$

2.  **Visual Check:**
    *(GÃ¶rsel Kontrol:)*
    Does it now **oscillate around zero** with no clear drift?
    *(Åimdi belirgin bir sÃ¼rÃ¼klenme olmadan sÄ±fÄ±r etrafÄ±nda salÄ±nÄ±yor mu?)*

3.  **ADF Test Again:**
    *(Tekrar ADF Testi:)*
    If **p < 0.05**, accept `d=1`.
    *(EÄŸer p < 0.05 ise, d=1 olarak kabul et.)*

> âš ï¸ **Note (Not):** Even if `d=0` passed, we sometimes try `d=1` to see if it improves **model stability**, but be careful not to **over-difference**.
> *(d=0 geÃ§miÅŸ olsa bile, bazen model kararlÄ±lÄ±ÄŸÄ±nÄ± iyileÅŸtirip iyileÅŸtirmediÄŸini gÃ¶rmek iÃ§in d=1 deneriz, ancak aÅŸÄ±rÄ± fark alma [over-differencing] konusunda dikkatli olun.)*


## ğŸ“Œ Summary: Choosing `d`
*(Ã–zet: d SeÃ§imi)*

AÅŸaÄŸÄ±daki tablo, gÃ¶rsel analiz ve istatistiksel test sonuÃ§larÄ±na gÃ¶re `d` parametresini nasÄ±l seÃ§eceÄŸinizi Ã¶zetler.

| Durum (Condition) | Aksiyon (Action) |
| :--- | :--- |
| **Visual:** Flat mean, no trend.<br>*(DÃ¼z ortalama, trend yok.)*<br>**ADF:** p < 0.05 | **Stop.** Accept `d=0`.<br>*(Dur. d=0 kabul et.)* |
| **Visual:** Trend visible.<br>*(Trend gÃ¶rÃ¼nÃ¼r.)*<br>**ADF:** p â‰¥ 0.05 | **Difference once.** Try `d=1`.<br>*(Bir kez fark al. d=1 dene.)* |
| **Visual:** Still trending after `d=1`.<br>*(d=1 sonrasÄ± hala trend var.)*<br>**ADF:** p â‰¥ 0.05 | **Difference again (Rare).** Try `d=2`.<br>*(Tekrar fark al [Nadir]. d=2 dene.)* |

---

> âš ï¸ **Warning (UyarÄ±):** **Over-differencing** (*AÅŸÄ±rÄ± Fark Alma*) can introduce extra **noise** (*gÃ¼rÃ¼ltÃ¼*) and hurt your **forecast** (*tahmin*). Always balance **visual evidence** (*gÃ¶rsel kanÄ±t*) with **statistical tests** (*istatistiksel testler*).


# ğŸ“‰ Reading a PACF Plot: Choosing the AR Order (`p`)
*(PACF GrafiÄŸini Okuma: AR Derecesi `p` SeÃ§imi)*

ARIMA modelinin **AR (AutoRegressive)** bileÅŸeni olan `p` parametresini belirlemek iÃ§in birincil aracÄ±mÄ±z **Partial Autocorrelation Function (PACF)** grafiÄŸidir. 

### ğŸ§ What is PACF?


ACF (Autocorrelation Function), bir gecikmenin (*lag*) hem doÄŸrudan hem de dolaylÄ± etkilerini gÃ¶sterirken; **PACF**, aradaki gecikmelerin etkisini kaldÄ±rdÄ±ktan sonra, sadece o gecikmenin ÅŸimdiki zaman Ã¼zerindeki **saf ve doÄŸrudan etkisini** (*pure/direct effect*) Ã¶lÃ§er.

> ğŸ’¡ **Rule of Thumb:** AR (`p`) derecesini bulmak iÃ§in **PACF** grafiÄŸine, MA (`q`) derecesini bulmak iÃ§in **ACF** grafiÄŸine bakÄ±lÄ±r.

---

### ğŸ“Š How to Interpret the PACF Plot

PACF grafiÄŸindeki her bir Ã§ubuk (*bar*), ilgili gecikmenin korelasyon katsayÄ±sÄ±nÄ± temsil eder. Arka plandaki gÃ¶lgeli alan (genellikle mavi), **95% Confidence Interval** (GÃ¼ven AralÄ±ÄŸÄ±)'dÄ±r.


| Plot Feature (Grafik Ã–zelliÄŸi) | Interpretation (Yorumlama) |
| :--- | :--- |
| **Tall bar outside the grey band**<br>*(Gri bandÄ±n dÄ±ÅŸÄ±ndaki uzun Ã§ubuk)* | **Statistically Significant:** There is a significant, direct correlation at that lag.<br>*(Ä°statistiksel Olarak AnlamlÄ±: O gecikmede anlamlÄ± ve doÄŸrudan bir etki vardÄ±r.)* |
| **Bars drop inside the band and stay there**<br>*(Ã‡ubuklar bandÄ±n iÃ§ine dÃ¼ÅŸÃ¼yor ve orada kalÄ±yor)* | **Cut-off Point:** Useful memory ends here. This sharp drop indicates the order of the AR process.<br>*(Kesilme NoktasÄ±: YararlÄ± hafÄ±za burada biter. Bu keskin dÃ¼ÅŸÃ¼ÅŸ AR sÃ¼recinin derecesini gÃ¶sterir.)* |
| **First bar only**<br>*(Sadece birinci Ã§ubuk)* | **Classic AR(1):** Common in many time series. Only yesterday influences today.<br>*(Klasik AR(1): BirÃ§ok zaman serisinde yaygÄ±ndÄ±r. Sadece dÃ¼n bugÃ¼nÃ¼ etkiler.)* |
| **Several bars then sharp drop**<br>*(BirkaÃ§ Ã§ubuk sonra keskin dÃ¼ÅŸÃ¼ÅŸ)* | **AR(p):** Set `p` equal to the last significant lag before the drop.<br>*(AR(p): `p` deÄŸerini, dÃ¼ÅŸÃ¼ÅŸten Ã¶nceki son anlamlÄ± gecikmeye eÅŸitleyin.)* |

---

### ğŸ› ï¸ Workflow: Choosing the AR Order `p`
*(Ä°ÅŸ AkÄ±ÅŸÄ±: AR Derecesi `p` SeÃ§imi)*

1.  **Stationarity Check:** Ensure the series is stationary. Difference (`d`) if needed.
    *(Durgunluk KontrolÃ¼: Serinin durgun olduÄŸundan emin olun. Gerekirse fark alÄ±n.)*
2.  **Plot PACF:** Use `plot_pacf(series, lags=30)` from `statsmodels`.
    *(PACF Ã‡izimi: `statsmodels` kÃ¼tÃ¼phanesini kullanÄ±n.)*
3.  **Identify Cut-off:** Find the **last bar** that sticks out significantly above the confidence interval.
    *(Kesilme NoktasÄ±nÄ± Belirle: GÃ¼ven aralÄ±ÄŸÄ±nÄ±n dÄ±ÅŸÄ±na Ã§Ä±kan son Ã§ubuÄŸu bulun.)*
    * That lag number = **Candidate `p`** (*Aday p*).
4.  **Validate:** Don't rely solely on the plot. Compare models using **AIC/BIC** scores or **Cross-Validation**.
    *(DoÄŸrulama: Sadece grafiÄŸe gÃ¼venmeyin. AIC/BIC skorlarÄ± veya Ã‡apraz DoÄŸrulama ile modelleri karÅŸÄ±laÅŸtÄ±rÄ±n.)*

> **ğŸ” Technical Detail:** The Confidence Interval is typically calculated as $\pm 1.96 / \sqrt{T}$ where $T$ is the number of observations. Bars within this range are considered **White Noise** (statistical noise).

---

### â“ Common Questions & Troubleshooting

| Question | Quick Answer & Technical Reason  |
| :--- | :--- |
| **Why not pick a huge `p`?**<br>*(Neden Ã§ok bÃ¼yÃ¼k bir `p` seÃ§miyoruz?)* | **Overfitting Risk:** Adding too many lags captures random noise, not the signal. It increases model complexity without improving predictive power (penalized by AIC/BIC).<br>*(AÅŸÄ±rÄ± Ã–ÄŸrenme Riski: Ã‡ok fazla gecikme sinyali deÄŸil gÃ¼rÃ¼ltÃ¼yÃ¼ yakalar. Tahmin gÃ¼cÃ¼nÃ¼ artÄ±rmadan model karmaÅŸÄ±klÄ±ÄŸÄ±nÄ± yÃ¼kseltir.)* |
| **What if I see no bars above the band?**<br>*(Ya bandÄ±n Ã¼zerinde hiÃ§ Ã§ubuk gÃ¶rmezsem?)* | **White Noise or Over-differencing:** The series might differenced too much (`d` is too high), or it holds no predictive pattern. Try `p=0` or reduce `d`.<br>*(Beyaz GÃ¼rÃ¼ltÃ¼ veya AÅŸÄ±rÄ± Fark Alma: Seri gereÄŸinden fazla fark alÄ±nmÄ±ÅŸ olabilir veya tahmin edilebilir bir desen iÃ§ermiyordur.)* |
| **What if bars never drop (slow decay)?**<br>*(Ya Ã§ubuklar hiÃ§ dÃ¼ÅŸmezse / yavaÅŸ azalÄ±rsa?)* | **Non-Stationarity:** The series is likely still non-stationary. Re-examine **differencing** (`d`) or check for **seasonality**.<br>*(Durgun Olmama: Seri muhtemelen hala durgun deÄŸildir. Fark alma iÅŸlemini tekrar inceleyin veya mevsimsellik kontrolÃ¼ yapÄ±n.)* |

# ğŸ“‰ Choosing the MA Order (`q`) with ACF
*(ACF ile MA Derecesi `q` SeÃ§imi)*

ARIMA modelinin **MA (Moving Average - Hareketli Ortalama)** bileÅŸeni olan `q` parametresini belirlemek iÃ§in birincil aracÄ±mÄ±z **Autocorrelation Function (ACF)** grafiÄŸidir.

### ğŸ§ What is `q` in ARIMA?
*(ARIMA'da `q` Nedir?)*

`p` parametresi geÃ§miÅŸ *deÄŸerlere* (satÄ±ÅŸ rakamlarÄ±na) bakarken, **`q` parametresi geÃ§miÅŸ tahmin hatalarÄ±na (forecast errors/residuals)** bakar.
MA modelleri, serideki ÅŸoklarÄ±n veya hatalarÄ±n zaman iÃ§inde nasÄ±l yayÄ±ldÄ±ÄŸÄ±nÄ± modeller.

> **Key Takeaway:** ACF answers "How many past mistakes still impact today?"
> *(Temel Ã‡Ä±karÄ±m: ACF, "GeÃ§miÅŸteki kaÃ§ hata bugÃ¼nÃ¼ hala etkiliyor?" sorusuna cevap verir.)*

---

### ğŸ“Š How to Read an ACF Plot
*(ACF GrafiÄŸi NasÄ±l Okunur)*

MA (`q`) derecesini seÃ§erken, ACF grafiÄŸinde "Cut-off" (Kesilme) noktasÄ±na bakarÄ±z. PACF'in aksine, burada **ACF** grafiÄŸindeki ani dÃ¼ÅŸÃ¼ÅŸler MA derecesini iÅŸaret eder.



#### ğŸ› ï¸ Step-by-Step Workflow (AdÄ±m AdÄ±m Ä°ÅŸ AkÄ±ÅŸÄ±)

1.  **Stationarity First:** Ensure the series is stationary (`d` is fixed).
    *(Ã–nce Durgunluk: Serinin durgun olduÄŸundan emin olun.)*
2.  **Plot ACF:** Use `plot_acf(series)` from `statsmodels`.
    *(ACF Ã‡izimi: `statsmodels` Ã¼zerinden ACF grafiÄŸini Ã§izin.)*
3.  **Identify Cut-off:** Look for the lag where bars drop into the **grey band** (Confidence Interval) and stay there.
    *(Kesilmeyi Belirle: Ã‡ubuklarÄ±n gri banda [GÃ¼ven AralÄ±ÄŸÄ±na] dÃ¼ÅŸtÃ¼ÄŸÃ¼ ve orada kaldÄ±ÄŸÄ± gecikmeyi bulun.)*
4.  **Set `q`:** The last significant lag before the drop is your candidate `q`.
    *(q'yu Ayarla: DÃ¼ÅŸÃ¼ÅŸten Ã¶nceki son anlamlÄ± gecikme, aday q deÄŸerinizdir.)*

---

### ğŸ” Interpreting ACF Signatures
*(ACF Ä°mzalarÄ±nÄ± Yorumlama)*

| Plot Pattern (Grafik Deseni) | Model Implication (Model Ã‡Ä±karÄ±mÄ±) |
| :--- | :--- |
| **Sharp Cut-off after Lag `q`**<br>*(Gecikme `q`'dan sonra keskin kesilme)* | **MA(`q`) Candidate:** Strong evidence for a Moving Average model of order `q`.<br>*(MA(q) AdayÄ±: q derecesinden Hareketli Ortalama modeli iÃ§in gÃ¼Ã§lÃ¼ kanÄ±t.)* |
| **Gradual Decay (Sine Wave / Exponential)**<br>*(Kademeli Azalma / SinÃ¼s DalgasÄ±)* | **AR(`p`) Process:** Typically indicates an Autoregressive process. Look at **PACF** instead to find `p`.<br>*(AR(p) SÃ¼reci: Genellikle Otokorelasyon sÃ¼recini gÃ¶sterir. p'yi bulmak iÃ§in PACF'e bakÄ±n.)* |
| **Spikes at Regular Intervals (s, 2s...)**<br>*(DÃ¼zenli AralÄ±klarda SÄ±Ã§ramalar)* | **Seasonality:** Indicates seasonal patterns (e.g., lag 7, 14, 21). Needs SARIMA.<br>*(Mevsimsellik: Mevsimsel kalÄ±plarÄ± gÃ¶sterir. SARIMA gerektirir.)* |

> ğŸ’¡ **Technical Note:** For a pure MA(q) process:
> * **ACF:** Cuts off after lag `q`. (*Gecikme q'dan sonra kesilir.*)
> * **PACF:** Decays gradually (tails off). (*Kademeli olarak azalÄ±r.*)

---

### âš ï¸ Common Pitfalls & Fixes
*(YaygÄ±n Hatalar ve Ã‡Ã¶zÃ¼mler)*

Model kurarken karÅŸÄ±laÅŸabileceÄŸiniz yaygÄ±n ACF desenleri ve teknik Ã§Ã¶zÃ¼mleri:

| Pitfall (Tuzak) | Symptom on ACF (ACF Belirtisi) | Technical Fix (Teknik Ã‡Ã¶zÃ¼m) |
| :--- | :--- | :--- |
| **Over-differencing**<br>*(AÅŸÄ±rÄ± Fark Alma)* | **No significant bars:** First lag might even be significantly negative (approx -0.5).<br>*(AnlamlÄ± Ã§ubuk yok: Ä°lk gecikme negatif ve anlamsÄ±z olabilir.)* | **Try smaller `d`:** You likely differenced a stationary series unnecessarily. Revert to `d=0` or `d-1`.<br>*(Daha kÃ¼Ã§Ã¼k `d` dene: Muhtemelen durgun bir serinin gereksiz yere farkÄ±nÄ± aldÄ±nÄ±z.)* |
| **Under-differencing**<br>*(Yetersiz Fark Alma)* | **Very slow decay:** Bars stay high and positive for many lags, decreasing linearly.<br>*(Ã‡ok yavaÅŸ azalma: Ã‡ubuklar birÃ§ok gecikme boyunca yÃ¼ksek ve pozitif kalÄ±r, doÄŸrusal azalÄ±r.)* | **Increase `d`:** The series is still non-stationary (Unit Root present). Take an additional difference.<br>*(d'yi ArtÄ±r: Seri hala durgun deÄŸil. Bir fark daha alÄ±n.)* |
| **Seasonality Present**<br>*(Mevsimsellik Var)* | **Periodic Spikes:** Significant correlations appearing at specific lags (e.g., 7, 14 for weekly data).<br>*(Periyodik SÄ±Ã§ramalar: Belirli gecikmelerde [haftalÄ±k veride 7, 14 gibi] anlamlÄ± korelasyonlar.)* | **Seasonal Model:** Consider **SARIMA** (adding Seasonal MA term `Q`) or apply **Seasonal Differencing** (`D=1`, lag=7).<br>*(Mevsimsel Model: SARIMA dÃ¼ÅŸÃ¼nÃ¼n veya Mevsimsel Fark Alma uygulayÄ±n.)* |

---

### âœ… Summary Strategy: Choosing `p` and `q`
*(Ã–zet Strateji: p ve q SeÃ§imi)*

| Parameter | Plot to Watch | Pattern to Look For |
| :--- | :--- | :--- |
| **AR (`p`)** | **PACF** | **Cut-off:** Last significant spike determines `p`. |
| **MA (`q`)** | **ACF** | **Cut-off:** Last significant spike determines `q`. |

> **Final Check:** After choosing `p` and `q`, always validate with **Information Criteria (AIC/BIC)** and check the residuals of your model.
> *(Son Kontrol: SeÃ§imden sonra her zaman Bilgi Kriterleri [AIC/BIC] ile doÄŸrulayÄ±n ve model hatalarÄ±nÄ± [residuals] kontrol edin.)*


# ğŸ“Œ Summary & ARIMA Workflow
*(Ã–zet ve ARIMA Ä°ÅŸ AkÄ±ÅŸÄ±)*

### ğŸš€ Why ARIMA?
*(Neden ARIMA?)*

**ARIMA** (*AutoRegressive Integrated Moving Average*), daha aÄŸÄ±r makine Ã¶ÄŸrenimi yaklaÅŸÄ±mlarÄ±na (*Machine Learning approaches*) dalmadan Ã¶nce baÅŸvurmanÄ±z gereken, **kompakt**, **ÅŸeffaf** ve **istatistiksel olarak titiz** bir tahmin modelidir.

> **âš ï¸ Limitation (KÄ±sÄ±t):**
> ARIMA, by design, handles **trend** and short-term **autocorrelation** but assumes your series has **no built-in seasonality**. There is no term in the definition explicitly modeling repeating cycles.
> *(ARIMA, tasarÄ±mÄ± gereÄŸi trendi ve kÄ±sa vadeli otokorelasyonu yÃ¶netir ancak serinizin yerleÅŸik bir mevsimselliÄŸi olmadÄ±ÄŸÄ±nÄ± varsayar. TanÄ±mÄ±nda tekrarlayan dÃ¶ngÃ¼leri modelleyen aÃ§Ä±k bir terim yoktur.)*
>
> ğŸ’¡ **Solution:** If your data shows strong weekly or annual patterns, upgrade to **SARIMA**, which adds a "Seasonal" component.

---

### âš™ï¸ The Three Pillars: p, d, q
*(ÃœÃ§ Temel Direk: p, d, q)*

ARIMA requires three hyperparameters that define its structure:

| Parameter | Component | Description (AÃ§Ä±klama) |
| :--- | :--- | :--- |
| **p** | **AutoRegression (AR)**<br>*(Oto-Regresyon)* | **Past Values:** It looks back at the last `p` days of sales to detect patterns.<br>*(GeÃ§miÅŸ DeÄŸerler: KalÄ±plarÄ± tespit etmek iÃ§in son p gÃ¼nÃ¼n satÄ±ÅŸlarÄ±na bakar.)* |
| **d** | **Integration (I)**<br>*(Entegrasyon/Fark Alma)* | **Stationarity:** It removes smooth up-or-down trends by **differencing** the data `d` times.<br>*(Durgunluk: Verinin d kez farkÄ±nÄ± alarak yukarÄ± veya aÅŸaÄŸÄ± yÃ¶nlÃ¼ trendleri kaldÄ±rÄ±r.)* |
| **q** | **Moving Average (MA)**<br>*(Hareketli Ortalama)* | **Past Errors:** It learns from the last `q` days of **forecast errors** to correct its predictions.<br>*(GeÃ§miÅŸ Hatalar: Tahminlerini dÃ¼zeltmek iÃ§in son q gÃ¼nÃ¼n tahmin hatalarÄ±ndan Ã¶ÄŸrenir.)* |

---

### ğŸ› ï¸ The Professional Workflow
*(Profesyonel Ä°ÅŸ AkÄ±ÅŸÄ±)*

Follow this step-by-step pipeline to build a robust ARIMA model.

#### 1. Exploratory Analysis
*(KeÅŸifÃ§i Analiz)*
* **Visualize:** Plot the raw series, moving average, and variance.
* **Check Trend:** Is there an obvious upward/downward drift?
* **Test Stationarity:** Run the **ADF Test** (*Augmented Dickey-Fuller*).

#### 2. Differencing (Parameter `d`)
*(Fark Alma)*
* Difference the series (`d` times) until it looks flat (constant mean).
* **Verification:** Ensure ADF p-value < 0.05.
* *Note:* Usually `d=1` is sufficient. `d=2` is rare.

#### 3. Identification (Parameters `p` & `q`)
*(TanÄ±mlama)*
Plot **ACF** and **PACF** charts on the **differenced** series:
* **PACF Cut-off:** Suggests the AR order (**p**).
* **ACF Cut-off:** Suggests the MA order (**q**).

#### 4. Model Fitting
*(Model EÄŸitimi)*
Fit the model using `statsmodels`.

```python
from statsmodels.tsa.arima.model import ARIMA

# Define the model with identified orders
# order = (p, d, q)
model = ARIMA(series, order=(p, d, q))

# Train the model
results = model.fit()

# View statistical summary
print(results.summary())

```

### 5. ğŸ©º Diagnostics: Residual Analysis
*(TanÄ±lama: ArtÄ±k Analizi)*

This is a **critically important step** (*kritik derecede Ã¶nemli bir adÄ±m*). We must check if the **residuals** (*hatalar/artÄ±klar*) resemble **White Noise** (*Beyaz GÃ¼rÃ¼ltÃ¼*).

**Checklist for Residuals:**
*(ArtÄ±klar iÃ§in Kontrol Listesi:)*

* **Mean** (*Ortalama*): Should be close to **0**.
    *(0'a yakÄ±n olmalÄ±dÄ±r.)*
* **Variance** (*Varyans*): Should be **constant**.
    *(Sabit olmalÄ±dÄ±r.)*
* **No Correlation** (*Korelasyon Yok*): The **ACF** of residuals should show no significant **spikes**.
    *(ArtÄ±klarÄ±n ACF grafiÄŸi anlamlÄ± sivrilmeler gÃ¶stermemelidir.)*
* **Test:** Use the **Ljung-Box Test** to statistically confirm residuals are **random**.
    *(Test: ArtÄ±klarÄ±n rastgele olduÄŸunu istatistiksel olarak doÄŸrulamak iÃ§in Ljung-Box Testi kullanÄ±n.)*

---

### 6. ğŸ”® Forecast and Evaluate
*(Tahmin ve DeÄŸerlendirme)*

Once the diagnostics pass, we evaluate the model using specific metrics.

#### ğŸ“‰ Model Selection Metric: AIC
*(Model SeÃ§im MetriÄŸi: AIC)*

* **Definition:** **AIC (Akaike Information Criterion)**.
* **Purpose:** Used for **Model Selection**.
    *(Model SeÃ§imi iÃ§in kullanÄ±lÄ±r.)*
* **Interpretation:** **Lower is better**. It balances **model fit** vs. **complexity**.
    *(Daha dÃ¼ÅŸÃ¼k olmasÄ± daha iyidir. Model uyumu ile karmaÅŸÄ±klÄ±ÄŸÄ± dengeler.)*

#### ğŸ¯ Accuracy Metrics: MAE / RMSE
*(DoÄŸruluk Metrikleri: MAE / RMSE)*

* **Purpose:** Used for **Accuracy**.
    *(DoÄŸruluk iÃ§in kullanÄ±lÄ±r.)*
* **Interpretation:** Evaluate how close the **predictions** are to **actuals** on a test set.
    *(Test setindeki tahminlerin gerÃ§ek deÄŸerlere ne kadar yakÄ±n olduÄŸunu Ã¶lÃ§er.)*

# ğŸ—“ï¸ Classical Time-Series Methods: SARIMA
*(Klasik Zaman Serisi YÃ¶ntemleri: SARIMA)*

**SARIMA** (*Seasonal AutoRegressive Integrated Moving Average*), klasik ARIMA modelinin, verilerdeki **mevsimsel dÃ¶ngÃ¼leri** (*seasonal cycles*) modelleyebilecek ÅŸekilde geniÅŸletilmiÅŸ halidir.

### ğŸš€ Why "ARIMA with an S" is the Next Logical Step?
*(Neden "S" eklenmiÅŸ ARIMA bir sonraki mantÄ±ksal adÄ±mdÄ±r?)*

Standard ARIMA models capture a seriesâ€™ **short-term memory** (*kÄ±sa vadeli hafÄ±za*). However, real-world data (e.g., supermarket sales, airline passengers) often repeats patterns every week, month, or quarter.
*(Standart ARIMA modelleri serinin kÄ±sa vadeli hafÄ±zasÄ±nÄ± yakalar. Ancak gerÃ§ek dÃ¼nya verileri genellikle her hafta, ay veya Ã§eyrekte tekrarlayan desenler iÃ§erir.)*

> **The Problem:** Traditional ARIMA canâ€™t model repeating boosts (like a Saturday spike) unless you manually inject complex lags.
> **The Solution:** SARIMA adds a **second, parallel ARIMA layer** that only "wakes up" at the seasonal interval $s$.

---

### ğŸ“Š Comparative Analysis Matrix: SARIMA Architecture
*(KarÅŸÄ±laÅŸtÄ±rmalÄ± Analiz Matrisi: SARIMA Mimarisi)*

Bu tablo, SARIMA'nÄ±n bileÅŸenlerini, Ã§Ã¶zdÃ¼ÄŸÃ¼ problemleri ve teknik detaylarÄ±nÄ± analiz eder.

| Analysis Area (Analiz AlanÄ±) | Problems & Components (Sorunlar ve BileÅŸenler) | Technical Detail & Importance (Teknik Detay ve Ã–nem) | Solution Methods (Ã‡Ã¶zÃ¼m YÃ¶ntemleri) | Tools & Tests (AraÃ§lar ve Testler) |
| :--- | :--- | :--- | :--- | :--- |
| **1. Seasonality Handling**<br>*(Mevsimsellik YÃ¶netimi)* | **Problem:** ARIMA'nÄ±n mevsimsel ÅŸoklarÄ± (Ã¶rn. Noel satÄ±ÅŸlarÄ±) gÃ¶rememesi.<br>**Component:** **Seasonal Period ($s$)**. | **Detail:** $s$, dÃ¶ngÃ¼nÃ¼n uzunluÄŸudur.<br>â€¢ HaftalÄ±k veri iÃ§in $s=7$.<br>â€¢ AylÄ±k veri iÃ§in $s=12$.<br>**Importance:** Modelin hangi aralÄ±klarla geÃ§miÅŸe bakacaÄŸÄ±nÄ± belirler. | **Notation:**<br>$$SARIMA(p, d, q) \times (P, D, Q)_s$$<br>Mevsimsel olmayan ve mevsimsel parametrelerin Ã§arpÄ±mÄ±. | â€¢ **Seasonal Decomposition:** Trend ve mevsimselliÄŸi gÃ¶rsel ayÄ±rma.<br>â€¢ **ACF Plot:** $s, 2s, 3s$ gecikmelerindeki sÄ±Ã§ramalarÄ± kontrol etme. |
| **2. Seasonal AutoRegression ($P$)**<br>*(Mevsimsel Oto-Regresyon)* | **Problem:** Bu ayÄ±n satÄ±ÅŸlarÄ±nÄ±n, geÃ§en yÄ±lÄ±n aynÄ± ayÄ±ndaki satÄ±ÅŸlarla iliÅŸkisi.<br>**Component:** **Seasonal AR ($P$)**. | **Detail:** "KaÃ§ tane mevsimsel dÃ¼n ($t-s, t-2s$) bugÃ¼nÃ¼ etkiliyor?" sorusuna yanÄ±t verir.<br>**Importance:** GeÃ§miÅŸ sezonlarÄ±n momentumunu bugÃ¼ne taÅŸÄ±r. | **Interaction:**<br>Standart $p$ (dÃ¼n) ile Mevsimsel $P$ (geÃ§en yÄ±l bugÃ¼n) birlikte Ã§alÄ±ÅŸÄ±r. | â€¢ **PACF Plot:** $s$ katlarÄ±nda (12, 24...) keskin dÃ¼ÅŸÃ¼ÅŸler aranÄ±r.<br>â€¢ **Grid Search:** En iyi $P$ deÄŸerini deneme yanÄ±lma ile bulma. |
| **3. Seasonal Differencing ($D$)**<br>*(Mevsimsel Fark Alma)* | **Problem:** Mevsimsel trendler (yÄ±ldan yÄ±la artan yaz trafiÄŸi).<br>**Component:** **Seasonal Integrated ($D$)**. | **Detail:** Mevsimsel seviye kaymalarÄ±nÄ± (*Level Shifts*) kaldÄ±rmak iÃ§in fark alÄ±r.<br>FormÃ¼l: $y_t - y_{t-s}$.<br>**Importance:** Veriyi mevsimsel olarak durgunlaÅŸtÄ±rÄ±r (*Seasonally Stationary*). | **Method:**<br>Genellikle $D=1$ yeterlidir. Bu, seriden "geÃ§en yÄ±lÄ±n aynÄ± ayÄ±nÄ± Ã§Ä±karma" iÅŸlemidir. | â€¢ **Canova-Hansen Test:** Mevsimsel kararlÄ±lÄ±k testi.<br>â€¢ **Visual Check:** $s$ periyodunda tekrar eden dalgalarÄ±n dÃ¼zleÅŸmesi. |
| **4. Seasonal Moving Average ($Q$)**<br>*(Mevsimsel Hareketli Ortalama)* | **Problem:** GeÃ§miÅŸ sezonlardaki tahmin hatalarÄ±nÄ±n bugÃ¼ne etkisi.<br>**Component:** **Seasonal MA ($Q$)**. | **Detail:** "KaÃ§ tane mevsimsel hata ÅŸoku (*Error Shocks*) kalÄ±cÄ± oluyor?"<br>Ã–rn: GeÃ§en AralÄ±k'taki tahmin hatasÄ± bu AralÄ±k'Ä± dÃ¼zeltir.<br>**Importance:** Tahminlerin mevsimsel sapmalara karÅŸÄ± direnÃ§li olmasÄ±nÄ± saÄŸlar. | **Calculation:**<br>Model, $t-s$ zamanÄ±ndaki hatayÄ± ($e_{t-s}$) kullanarak bugÃ¼nkÃ¼ tahmini revize eder. | â€¢ **ACF Plot:** $s$ gecikmesindeki (lag $s$) negatif korelasyon veya kesilme noktasÄ±. |
| **5. Model Tuning & Selection**<br>*(Model Ayarlama ve SeÃ§im)* | **Problem:** Toplam 7 parametrenin ($p,d,q,P,D,Q,s$) optimizasyonu.<br>**Component:** **Hyperparameter Tuning**. | **Detail:** Ã‡ok sayÄ±da kombinasyon olduÄŸu iÃ§in model karmaÅŸÄ±klÄ±ÄŸÄ± artar.<br>**Importance:** YanlÄ±ÅŸ $s$ veya $D$ seÃ§imi, tamamen hatalÄ± tahminlere yol aÃ§ar. | **Auto-ARIMA:**<br>`pmdarima` gibi kÃ¼tÃ¼phanelerle parametrelerin otomatik denenmesi (AIC minimizasyonu). | â€¢ **AIC/BIC:** Model karÅŸÄ±laÅŸtÄ±rma.<br>â€¢ **Ljung-Box:** Mevsimsel hatalarÄ±n rastgeleliÄŸini test etme. |

---

### ğŸ§© The Structure of SARIMA
*(SARIMA'nÄ±n YapÄ±sÄ±)*

SARIMA model is denoted as:
*(SARIMA modeli ÅŸu ÅŸekilde gÃ¶sterilir:)*

$$SARIMA(p, d, q) \times (P, D, Q)_s$$



#### 1. Non-Seasonal Part $(p, d, q)$
*(Mevsimsel Olmayan KÄ±sÄ±m)*
Standart ARIMA parametreleri. GÃ¼nlÃ¼k/kÄ±sa vadeli trendleri ve otokorelasyonu yakalar.

#### 2. Seasonal Part $(P, D, Q)_s$
*(Mevsimsel KÄ±sÄ±m)*
* **P (Seasonal AR):** Looks at past seasonal values.
    *(GeÃ§miÅŸ mevsimsel deÄŸerlere bakar. Ã–rn: GeÃ§en yÄ±lÄ±n aynÄ± ayÄ±.)*
* **D (Seasonal Differencing):** Removes repeating seasonal patterns.
    *(Tekrarlayan mevsimsel desenleri kaldÄ±rÄ±r. $Series.diff(s)$ iÅŸlemi.)*
* **Q (Seasonal MA):** Looks at past seasonal errors.
    *(GeÃ§miÅŸ mevsimsel hatalara bakar.)*
* **s (Period):** The length of the cycle.
    *(DÃ¶ngÃ¼nÃ¼n uzunluÄŸu. HaftalÄ±k iÃ§in 7, YÄ±llÄ±k (aylÄ±k veri) iÃ§in 12.)*

---

### ğŸ’¡ Key Takeaway
*(Temel Ã‡Ä±karÄ±m)*

**SARIMA is like ARIMA, but itâ€™s designed for data that repeats in cycles.**
*(SARIMA, ARIMA gibidir ancak dÃ¶ngÃ¼ler halinde tekrarlayan [haftalÄ±k veya yÄ±llÄ±k satÄ±ÅŸ kalÄ±plarÄ± gibi] veriler iÃ§in tasarlanmÄ±ÅŸtÄ±r.)*

While standard ARIMA handles general trends, **SARIMA** is indispensable when "this December" depends heavily on "last December".
*(Standart ARIMA genel trendleri yÃ¶netirken, "bu AralÄ±k" ayÄ±nÄ±n bÃ¼yÃ¼k Ã¶lÃ§Ã¼de "geÃ§en AralÄ±k" ayÄ±na baÄŸlÄ± olduÄŸu durumlarda SARIMA vazgeÃ§ilmezdir.)*

# ğŸ—“ï¸ Choosing Seasonal Orders for SARIMA: Step 1
*(SARIMA Ä°Ã§in Mevsimsel Derecelerin SeÃ§imi: AdÄ±m 1)*

Todayâ€™s lecture walks you through picking the right **seasonal orders** ($P, D, Q$) and combining them with your **non-seasonal** ($p, d, q$) parameters.
*(BugÃ¼nkÃ¼ ders, doÄŸru mevsimsel dereceleri [P, D, Q] seÃ§menize ve bunlarÄ± mevsimsel olmayan [p, d, q] parametrelerinizle birleÅŸtirmenize rehberlik eder.)*

### ğŸ—ºï¸ Our Roadmap
*(Yol HaritamÄ±z)*

1.  **Confirm the seasonal period $s$.**
    *(Mevsimsel periyodu s'yi doÄŸrula.)*
2.  **Stationarize** with ordinary and seasonal **differencing**.
    *(Normal ve mevsimsel fark alma ile durgunlaÅŸtÄ±r.)*
3.  Read **seasonal ACF/PACF spikes** to guess $P$ and $Q$.
    *(P ve Q'yu tahmin etmek iÃ§in mevsimsel ACF/PACF sÄ±Ã§ramalarÄ±nÄ± oku.)*
4.  **Grid-search** a small set of $(p, q) \times (P, Q)$.
    *(KÃ¼Ã§Ã¼k bir (p, q) x (P, Q) seti Ã¼zerinde Ä±zgara aramasÄ± yap.)*
5.  **Fit the models** to the grid.
    *(Modelleri Ä±zgaraya uydur/eÄŸit.)*
6.  **Select the best model** by **AIC/BIC** and **hold-out error**.
    *(AIC/BIC ve dÄ±ÅŸarÄ±da tutulan set hatasÄ±na gÃ¶re en iyi modeli seÃ§.)*

---

## ğŸ•µï¸ Step 1: Confirm the Seasonal Period
*(AdÄ±m 1: Mevsimsel Periyodu DoÄŸrulama)*

Before you can set any seasonal parameters, you must know **how long the season is**. This is the **single most important input** to SARIMA.
*(Herhangi bir mevsimsel parametre ayarlamadan Ã¶nce, sezonun ne kadar sÃ¼rdÃ¼ÄŸÃ¼nÃ¼ bilmelisiniz. Bu, SARIMA iÃ§in en Ã¶nemli tek girdidir.)*

### 1. Domain Knowledge (Fast Check)
*(Alan Bilgisi - HÄ±zlÄ± Kontrol)*

* Does the business talk about "**weekly cycles**" (*haftalÄ±k dÃ¶ngÃ¼ler*), "**monthly billing peaks**" (*aylÄ±k fatura zirveleri*), or "**quarterly budgets**" (*Ã§eyreklik bÃ¼tÃ§eler*)?
* **Daily Data:** If operations close every weekend, start with **$s = 7$**.
* **Hourly Data:** Call-center data often cycle every 24 hours â†’ **$s = 24$**.

### 2. Visual Inspection
*(GÃ¶rsel Ä°nceleme)*

We will work with the same dataframe as we did with ARIMA.
Plot the raw series: `train.plot()`
Look for regularly spaced **ridges** (*sÄ±rtlar/tepeler*) or **troughs** (*Ã§ukurlar*). Count the spacing in days, hours, or monthsâ€”that spacing is your candidate $s$.



> **ğŸ’¡ Think First!**
> What do you see in the plot?
> *(Grafikte ne gÃ¶rÃ¼yorsunuz?)*

**ğŸ“‰ Our Analysis:**
* From a quick visual scan, you can see "**ridges**" (higher-than-average clusters of points) and "**troughs**" repeating at a fairly even **cadence** (*ritim/ahenk*).
* If you mark any spike and count forward to the next one of similar height, you hit roughly the same spot after about **seven days** each time.
* **Result:** That makes **7 days** the most plausible seasonal period for this daily seriesâ€”a classic **weekly cycle**.

### 3. Inspect the ACF for Spikes
*(SÄ±Ã§ramalar Ä°Ã§in ACF'yi Ä°nceleme)*

Plot the plain **ACF** up to, say, $3 \times$ your suspected period.
*(Normal ACF'yi, ÅŸÃ¼phelendiÄŸiniz periyodun 3 katÄ±na kadar Ã§izdirin.)*

```python
from statsmodels.graphics.tsaplots import plot_acf
plot_acf(train.values.flatten(), lags=30)  # Look for bars at s, 2s, 3s...
```

### ğŸ“Š Analyzing the ACF Plot for Seasonality
*(Mevsimsellik Ä°Ã§in ACF GrafiÄŸini Analiz Etme)*

ACF grafiÄŸi, mevsimsel periyodu ($s$) doÄŸrulamak iÃ§in en gÃ¼Ã§lÃ¼ kanÄ±tÄ± sunar.

#### ğŸš¦ Signals (Sinyaller)

* **Positive Signal** (*Pozitif Sinyal*): If vertical bars appear at lag $s, 2s, 3s \dots$ while the in-between lags are small, youâ€™ve found a **seasonal pulse** (*mevsimsel nabÄ±z*) at $s$.
    *(Dikey Ã§ubuklar s, 2s, 3s... gecikmelerinde beliriyor ve aradaki gecikmeler kÃ¼Ã§Ã¼k kalÄ±yorsa, s noktasÄ±nda mevsimsel bir etki bulmuÅŸsunuz demektir.)*
* **Negative Signal** (*Negatif Sinyal*): If no spikes appear, a **seasonal model** may not help.
    *(Herhangi bir sÄ±Ã§rama gÃ¶rÃ¼nmÃ¼yorsa, mevsimsel bir model yardÄ±mcÄ± olmayabilir.)*



---

### ğŸ’¡ Think First!


> **Reflection:** What do you see in the plot?
> *(Grafikte ne gÃ¶rÃ¼yorsunuz?)*

---

### ğŸ“‰ Our Analysis


Based on the ACF plot, here is the technical breakdown:
*(ACF grafiÄŸine dayanarak, iÅŸte teknik dÃ¶kÃ¼m:)*

#### 1. Strong Positive Lag 1 Spike
*(GÃ¼Ã§lÃ¼ Pozitif 1. Gecikme SÄ±Ã§ramasÄ±)*
* **Observation:** The correlation at Lag 1 is very high.
* **Interpretation:** Yesterdayâ€™s sales are a very good predictor of todayâ€™sâ€”this represents classic **short-memory momentum**.
    *(DÃ¼nkÃ¼ satÄ±ÅŸlar bugÃ¼nÃ¼n Ã§ok iyi bir tahmincisidirâ€”bu, klasik kÄ±sa vadeli momentumu/hafÄ±zayÄ± temsil eder.)*

#### 2. Regular Positive Spikes at Lags 7, 14, 21, 28
*(7, 14, 21, 28. Gecikmelerde DÃ¼zenli Pozitif SÄ±Ã§ramalar)*
* **Observation:** Every seventh day, the autocorrelation climbs back to roughly **0.5**.
* **Interpretation:** This confirms a clear **weekly cycle** (*haftalÄ±k dÃ¶ngÃ¼*). It validates the "**same-day-last-week**" (*geÃ§en hafta aynÄ± gÃ¼n*) effect.

#### 3. Alternating Negative Bars
*(ArdÄ±ÅŸÄ±k Negatif Ã‡ubuklar)*
* **Observation:** Values about half a week apart move in **opposite directions**.
* **Interpretation:** This is typical for a series with **steady seasonality** (*istikrarlÄ± mevsimsellik*) but no long-term trend.
    *(YaklaÅŸÄ±k yarÄ±m hafta aralÄ±klÄ± deÄŸerler zÄ±t yÃ¶nlerde hareket ediyor; bu, istikrarlÄ± mevsimselliÄŸi olan ancak uzun vadeli trendi olmayan bir seri iÃ§in tipiktir.)*

---

### âœ… Conclusion for Step 1
*(AdÄ±m 1 Ä°Ã§in SonuÃ§)*

Based on the Domain Knowledge, Visual Inspection, and ACF Analysis:
*(Alan Bilgisi, GÃ¶rsel Ä°nceleme ve ACF Analizine dayanarak:)*

* **Season length ($m$ or $s$) = 7.**
* The **weekly pattern** (*haftalÄ±k desen*) is dominant.
    *(Sezon uzunluÄŸu [m veya s] = 7. HaftalÄ±k desen baskÄ±n.)*

We will proceed with **$s=7$** for our SARIMA model.

# ğŸ—“ï¸ SARIMA Modelling: Steps 2 & 3
*(SARIMA Modelleme: AdÄ±m 2 ve 3)*

Mevsimsel periyodu ($s=7$) doÄŸruladÄ±ktan sonra, seriyi **durgunlaÅŸtÄ±rmalÄ±** (*stationarize*) ve mevsimsel parametreleri ($P, Q$) belirlemeliyiz.

---

## ğŸ“‰ Step 2: Seasonal Differencing ($D$)
*(AdÄ±m 2: Mevsimsel Fark Alma)*

To apply SARIMA effectively, we need to handle two types of trends:
*(SARIMA'yÄ± etkili bir ÅŸekilde uygulamak iÃ§in iki tÃ¼r trendi yÃ¶netmemiz gerekir:)*

1.  **Ordinary Differencing ($d$):** Wipes out the **long-term trend**.
    *(SÄ±radan Fark Alma: Uzun vadeli trendi siler.)*
2.  **Seasonal Differencing ($D$):** Flattens the **repeating level** every $s$ periods.
    *(Mevsimsel Fark Alma: Her s periyodunda tekrarlayan seviyeyi dÃ¼zleÅŸtirir.)*

> **Goal:** After differencing, the **ACF** should no longer show a slow, stair-step decay, and the **mean/variance** should look stable.
> *(Hedef: Fark alma iÅŸleminden sonra, ACF artÄ±k yavaÅŸ, basamaklÄ± bir azalma gÃ¶stermemeli ve ortalama/varyans kararlÄ± gÃ¶rÃ¼nmelidir.)*

---

## ğŸ•µï¸ Step 3: Guess ($P, Q$) from Seasonal Lags
*(AdÄ±m 3: Mevsimsel Gecikmelerden P ve Q Tahmini)*

We read the **seasonal lags** in ACF & PACF to guess the orders.
*(Dereceleri tahmin etmek iÃ§in ACF ve PACF'teki mevsimsel gecikmeleri okuruz.)*

### ğŸ› ï¸ The Process (SÃ¼reÃ§)
1.  **Run PACF/ACF** on the **seasonally differenced series** (`diff_season`).
    *(Mevsimsel farkÄ± alÄ±nmÄ±ÅŸ seri Ã¼zerinde PACF/ACF Ã§alÄ±ÅŸtÄ±rÄ±n.)*
2.  **Look at bars at lags $s, 2s, 3s \dots$** (e.g., 7, 14, 21).
    *(s, 2s, 3s... gecikmelerindeki Ã§ubuklara bakÄ±n.)*

### ğŸ“ Rules of Thumb (Pratik Kurallar)

| Plot | Pattern at Lag $s$ | Initial Guess |
| :--- | :--- | :--- |
| **Seasonal PACF** | Big spike at lag $s$ (*Lag s'de bÃ¼yÃ¼k sÄ±Ã§rama*) | Start with **$P = 1$** (Seasonal AR) |
| **Seasonal ACF** | Big spike at lag $s$ (*Lag s'de bÃ¼yÃ¼k sÄ±Ã§rama*) | Start with **$Q = 1$** (Seasonal MA) |

> **Note:** If spikes persist at $2 \times s$, test $P=2$ or $Q=2$; if they vanish after the first spike, one term is usually enough.

---



## ğŸ“‰ Our Analysis: Interpreting the Plots
*(Analizimiz: Grafikleri Yorumlama)*

We applied **Seasonal Differencing** ($D=1, m=7$). Here is what the plots tell us now:
*(Mevsimsel Fark Alma uyguladÄ±k. Ä°ÅŸte grafiklerin bize sÃ¶yledikleri:)*

<img width="441" height="534" alt="image" src="https://github.com/user-attachments/assets/795c4ccc-6d30-4cab-913a-717efee62867" />


### 1. ACF Analysis (Top Chart)
* **Seasonal Pattern Gone:** No more big spikes at lags 7, 14, 21â€¦ This means the **weekly seasonality is gone**.
    *(Mevsimsel Desen Gitti: 7, 14, 21... gecikmelerinde artÄ±k bÃ¼yÃ¼k sÄ±Ã§ramalar yok. HaftalÄ±k mevsimsellik ortadan kalktÄ±.)*
* **Success:** Seasonal differencing ($D=1, m=7$) worked; the series is now **seasonally stationary**.
    *(BaÅŸarÄ±: Mevsimsel fark alma iÅŸe yaradÄ±; seri artÄ±k mevsimsel olarak durgun.)*
* **Non-Seasonal Hint:** The strong **lag-1 negative autocorrelation** suggests we should keep a short non-seasonal MA term (**$q=1$**).
    *(Mevsimsel Olmayan Ä°pucu: GÃ¼Ã§lÃ¼ negatif lag-1 otokorelasyonu, kÄ±sa bir mevsimsel olmayan MA terimi tutmamÄ±z gerektiÄŸini Ã¶nerir.)*

### 2. PACF Analysis (Bottom Chart)
* **Non-Seasonal AR:** Significant negative partial autocorrelations at lags 1 to 5, then everything dies out.
    *(Mevsimsel Olmayan AR: 1'den 5'e kadar olan gecikmelerde anlamlÄ± negatif kÄ±smi otokorelasyonlar var, sonra sÃ¶nÃ¼mleniyor.)*
* **Implication:** A finite AR structure of about **$p \approx 5$** is enough to explain the remaining correlation. This matches our previous ARIMA choice ($p=6$).
    *(Ã‡Ä±karÄ±m: YaklaÅŸÄ±k p=5 olan sonlu bir AR yapÄ±sÄ±, kalan korelasyonu aÃ§Ä±klamak iÃ§in yeterlidir.)*

### 3. Choosing $P$ and $Q$ (The Decision)
* **Observation:** After seasonal differencing, youâ€™d expect remaining seasonality to show up as spikes at lags 7, 14, 21.
* **Reality:** We have **somehow still significant small spikes**, so it seems the weekly cycle has kind of been removed but traces remain.
    *(GerÃ§eklik: Hala bir ÅŸekilde anlamlÄ± kÃ¼Ã§Ã¼k sÄ±Ã§ramalarÄ±mÄ±z var, bu da haftalÄ±k dÃ¶ngÃ¼nÃ¼n kÄ±smen kaldÄ±rÄ±ldÄ±ÄŸÄ±nÄ± ancak izlerin kaldÄ±ÄŸÄ±nÄ± gÃ¶steriyor.)*

> **âœ… Final Decision:** Therefore, we will establish **$P = 1$** and **$Q = 1$**.
> *(Son Karar: Bu nedenle, P=1 ve Q=1 olarak belirleyeceÄŸiz.)*


# ğŸ—“ï¸ SARIMA Modelling: Step 4 - Combine & Grid Search
*(SARIMA Modelleme: AdÄ±m 4 - BirleÅŸtirme ve Izgara AramasÄ±)*

Mevsimsel ($P, D, Q$) ve Mevsimsel Olmayan ($p, d, q$) parametreler iÃ§in ilk tahminlerimizi yaptÄ±k. Åimdi, en iyi kombinasyonu bulmak iÃ§in sistematik bir arama yapacaÄŸÄ±z.

---

## ğŸ•¸ï¸ The Strategy: Grid-Search a Small Neighbourhood
*(Strateji: KÃ¼Ã§Ã¼k Bir KomÅŸulukta Izgara AramasÄ±)*

Instead of testing every possible number (which takes forever), we define a **small neighbourhood** (*kÃ¼Ã§Ã¼k bir komÅŸuluk/aralÄ±k*) around our initial guesses.
*(Her olasÄ± sayÄ±yÄ± test etmek yerine [ki bu sonsuza kadar sÃ¼rer], ilk tahminlerimizin etrafÄ±nda kÃ¼Ã§Ã¼k bir aralÄ±k tanÄ±mlÄ±yoruz.)*

### ğŸ› ï¸ The Rules (Kurallar)
1.  **Keep `d` and `D` fixed:** We already confirmed stationarity.
    *(d ve D'yi sabit tutun: DurgunluÄŸu zaten doÄŸruladÄ±k.)*
2.  **Try a short list of non-seasonal orders:** e.g., $(p, q) = (1,1), (2,1)$.
    *(KÄ±sa bir mevsimsel olmayan dereceler listesi deneyin.)*
3.  **Combine with seasonal pairs:** e.g., $(P, Q) = (1,1)$ or $(0,1)$.
    *(Mevsimsel Ã§iftlerle birleÅŸtirin.)*

> **ğŸš€ Efficiency Tip:** A handful of runs is usually enough to find the **sweet spot** (*en uygun nokta*) without **over-computing** (*aÅŸÄ±rÄ± hesaplama/iÅŸlem yÃ¼kÃ¼*).

---

## ğŸ“Š Designing the Grid
*(IzgarayÄ± Tasarlama)*

Based on our previous ACF/PACF analysis, here is our search space:

| Parameter | Initial Guess (*Ä°lk Tahmin*) | Neighbourhood to Try (*Denenen AralÄ±k*) | Reasoning (*MantÄ±k*) |
| :--- | :--- | :--- | :--- |
| **p** | 5 | **4 â€“ 6** | PACF showed significant lags up to ~5. (*PACF 5'e kadar anlamlÄ±ydÄ±.*) |
| **q** | 1 | **0 â€“ 1** | ACF lag-1 was strong. (*ACF lag-1 gÃ¼Ã§lÃ¼ydÃ¼.*) |
| **P** | 1 | **0 â€“ 1** | Only if seasonal PACF at lag 7 looks non-zero. (*Sadece lag 7'de PACF sÄ±fÄ±r deÄŸilse.*) |
| **Q** | 1 | **0 â€“ 1** | If seasonal ACF at lag 7 resurfaces. (*EÄŸer lag 7'de ACF tekrar belirirse.*) |
| **D** | 1 | **Fixed (1)** | Seasonal differencing already helped. (*Mevsimsel fark alma zaten iÅŸe yaradÄ±.*) |

---

## ğŸ’» The Resulting Grid Code
*(Ortaya Ã‡Ä±kan Izgara Kodu)*

We will create two lists of tuples to iterate through.
*(Ãœzerinde dÃ¶ngÃ¼ kuracaÄŸÄ±mÄ±z iki demet listesi oluÅŸturacaÄŸÄ±z.)*

```python
# 1. Non-Seasonal Combinations (p, d, q)
# We test p around 5, keeping d=0 (from Step 1 stationarity), and q around 1.
pdq_combinations = [
    (4, 0, 0), (4, 0, 1),
    (5, 0, 0), (5, 0, 1),
    (6, 0, 0), (6, 0, 1)
]

# 2. Seasonal Combinations (P, D, Q, s)
# We test P and Q around 0-1, keeping D=1 and s=7.
seasonal_combinations = [
    (0, 1, 0, 7),
    (0, 1, 1, 7),
    (1, 1, 0, 7),
    (1, 1, 1, 7)
]
```

# ğŸ—“ï¸ SARIMA Modelling: Step 5 - Forecast & Implementation
*(SARIMA Modelleme: AdÄ±m 5 - Tahmin ve Uygulama)*

Model parametrelerini belirledikten sonra, tahmin Ã¼retme ve performansÄ± deÄŸerlendirme aÅŸamasÄ±na geÃ§iyoruz. AyrÄ±ca, bu bÃ¶lÃ¼mde **DARTS** kÃ¼tÃ¼phanesinde SARIMA'nÄ±n nasÄ±l tanÄ±mlandÄ±ÄŸÄ±nÄ± gÃ¶receÄŸiz.

---

## ğŸ”® Forecasting Strategy
*(Tahmin Stratejisi)*

Once the model is fitted, we evaluate its performance using standard metrics.
*(Model eÄŸitildikten sonra, performansÄ±nÄ± standart metrikler kullanarak deÄŸerlendiririz.)*

1.  **Generate Forecasts:** Predict future values for the validation period.
    *(Gelecek tahminleri Ã¼retin: DoÄŸrulama periyodu iÃ§in gelecekteki deÄŸerleri tahmin edin.)*
2.  **Compare Against Baseline:** Compare **MAE** (*Mean Absolute Error*) and **RMSE** (*Root Mean Squared Error*) against a **NaÃ¯ve Seasonal Baseline**.
    *(Temel Referansla KarÅŸÄ±laÅŸtÄ±rÄ±n: MAE ve RMSE'yi Saf Mevsimsel Referans ile karÅŸÄ±laÅŸtÄ±rÄ±n.)*
    > **NaÃ¯ve Seasonal Baseline:** Repeats the value from $t-s$ (e.g., predicting next Monday's sales as exactly last Monday's sales).

---

## ğŸ› ï¸ Implementation in DARTS
*(DARTS ile Uygulama)*

To model seasonality with ARIMA in DARTS, we utilize the `seasonal_order` parameter. This effectively mimics the SARIMA functionality found in `statsmodels`.

### The `seasonal_order` Parameter
*(seasonal_order Parametresi)*

We define the tuple `(P, D, Q, m)`:

* **P:** Seasonal Autoregressive order (*Mevsimsel AR derecesi*)
* **D:** Seasonal Differencing order (*Mevsimsel Fark Alma derecesi*)
* **Q:** Seasonal Moving Average order (*Mevsimsel MA derecesi*)
* **m:** Periodicity of the seasonal component (*Mevsimsel bileÅŸenin periyodu*)
    * *Example:* $m=7$ for weekly seasonality in daily data.

### ğŸ’» Code Example
*(Kod Ã–rneÄŸi)*

We will configure and fit a SARIMA model using the specific seasonal order identified in our analysis `(1, 1, 1, 7)` combined with a standard ARIMA order (e.g., `p=5, d=0, q=1`).

```python
from darts.models import ARIMA

# 1. Initialize the Model
# order=(p, d, q) -> Non-seasonal parameters
# seasonal_order=(P, D, Q, m) -> Seasonal parameters
model_sarima = ARIMA(
    order=(5, 0, 1),           # From Grid Search Step 4
    seasonal_order=(1, 1, 1, 7) # P=1, D=1, Q=1, m=7
)

# 2. Fit the Model
# Assuming 'train' is your Darts TimeSeries object
model_sarima.fit(train)

# 3. Forecast
# Predict the next n steps (e.g., 30 days)
forecast = model_sarima.predict(n=30)

# 4. Visualization (Optional)
# train.plot(label="Train")
# forecast.plot(label="Forecast")

```

* After training the model on the training data, we forecast and plot the results:

* <img width="652" height="235" alt="image" src="https://github.com/user-attachments/assets/ce17741e-4dda-4f75-86d9-22d976ba966c" />

# ğŸ† Final Step: Evaluation & Success Criteria
*(Son AdÄ±m: DeÄŸerlendirme ve BaÅŸarÄ± Kriterleri)*

Bir model eÄŸitmek iÅŸin sadece yarÄ±sÄ±dÄ±r. DiÄŸer yarÄ±sÄ± ise ÅŸu soruyu yanÄ±tlamaktÄ±r: **"Bu model, basit bir tahminden gerÃ§ekten daha mÄ± iyi?"**

---

## ğŸ“ The Benchmark: NaÃ¯ve Seasonal Baseline
*(KÄ±yaslama NoktasÄ±: Saf Mevsimsel Referans)*

Before celebrating low error rates, we must compare our complex SARIMA model against a "dumb" but effective baseline.
*(DÃ¼ÅŸÃ¼k hata oranlarÄ±nÄ± kutlamadan Ã¶nce, karmaÅŸÄ±k SARIMA modelimizi "basit" ama etkili bir referans noktasÄ±yla kÄ±yaslamalÄ±yÄ±z.)*

### ğŸ§ What is NaÃ¯ve Seasonal Baseline?
*(Saf Mevsimsel Referans Nedir?)*
It simply repeats the value from the last season ($t-s$).
*(BasitÃ§e son sezonun [t-s] deÄŸerini tekrar eder.)*

* **Logic:** "Next Monday's sales will be exactly the same as last Monday's sales."
    *(MantÄ±k: "Gelecek Pazartesi'nin satÄ±ÅŸlarÄ±, geÃ§en Pazartesi'nin satÄ±ÅŸlarÄ±yla birebir aynÄ± olacak.")*
* **Formula:** $\hat{y}_t = y_{t-s}$

> **Why do we need this?** If your complex SARIMA model cannot beat this simple logic, then the model is **over-engineered** and useless.
> *(Neden buna ihtiyacÄ±mÄ±z var? EÄŸer karmaÅŸÄ±k SARIMA modeliniz bu basit mantÄ±ÄŸÄ± geÃ§emiyorsa, model aÅŸÄ±rÄ± mÃ¼hendislik Ã¼rÃ¼nÃ¼dÃ¼r ve yararsÄ±zdÄ±r.)*

---

## ğŸ“Š Performance Comparison
*(Performans KarÅŸÄ±laÅŸtÄ±rmasÄ±)*

We evaluate success by comparing error metrics (**MAE** or **RMSE**) between the SARIMA Forecast and the Baseline.



### ğŸ“‰ Metrics to Watch (Ä°zlenecek Metrikler)

| Metrik (Metric) | Baseline Score (Ã–rn.) | SARIMA Score (Ã–rn.) | Interpretation (Yorum) |
| :--- | :--- | :--- | :--- |
| **MAE** | 50.5 | **32.1** | **Significant Improvement:** SARIMA reduced the average error by ~36%.<br>*(AnlamlÄ± Ä°yileÅŸme: SARIMA ortalama hatayÄ± ~%36 azalttÄ±.)* |
| **RMSE** | 65.2 | **41.8** | **Better Stability:** The model handles large outliers better than the baseline.<br>*(Daha Ä°yi KararlÄ±lÄ±k: Model, bÃ¼yÃ¼k aykÄ±rÄ± deÄŸerleri referanstan daha iyi yÃ¶netiyor.)* |

---

## âœ… Success Criteria Checklist
*(BaÅŸarÄ± Kriterleri Kontrol Listesi)*

Your SARIMA model is considered **successful** only if:
*(SARIMA modeliniz yalnÄ±zca ÅŸu durumlarda **baÅŸarÄ±lÄ±** sayÄ±lÄ±r:)*

1.  [ ] **Lower Error:** MAE/RMSE is **significantly lower** (>10-15%) than the NaÃ¯ve Seasonal Baseline.
    *(Daha DÃ¼ÅŸÃ¼k Hata: MAE/RMSE, Saf Mevsimsel Referans'tan anlamlÄ± derecede dÃ¼ÅŸÃ¼ktÃ¼r.)*
2.  [ ] **Residuals are Random:** The errors look like **White Noise** (no leftover patterns).
    *(Hatalar Rastgele: Hatalar Beyaz GÃ¼rÃ¼ltÃ¼ gibi gÃ¶rÃ¼nÃ¼r [arta kalan desen yok].)*
3.  [ ] **Captured Complexity:** The model predicts **trend changes** or **holiday spikes** that the baseline misses.
    *(KarmaÅŸÄ±klÄ±ÄŸÄ± Yakalama: Model, referansÄ±n kaÃ§Ä±rdÄ±ÄŸÄ± trend deÄŸiÅŸimlerini veya tatil sÄ±Ã§ramalarÄ±nÄ± tahmin eder.)*

> **ğŸš€ Conclusion:** If the criteria above are met, the model has successfully learned **complex patterns** beyond simple repetition.
> *(SonuÃ§: YukarÄ±daki kriterler karÅŸÄ±lanÄ±yorsa, model basit tekrarlarÄ±n Ã¶tesindeki karmaÅŸÄ±k kalÄ±plarÄ± baÅŸarÄ±yla Ã¶ÄŸrenmiÅŸtir.)*


# ğŸ† SARIMA Modelling: Step 6 - Evaluate, Diagnose & Grid Search
*(SARIMA Modelleme: AdÄ±m 6 - DeÄŸerlendirme, TanÄ±lama ve Izgara AramasÄ±)*

Modelimizi eÄŸittikten sonra, baÅŸarÄ±sÄ±nÄ± sayÄ±sal olarak kanÄ±tlamalÄ± ve en iyi parametre kombinasyonunu bulmalÄ±yÄ±z.

---

## ğŸ“ Evaluation Metrics
*(DeÄŸerlendirme Metrikleri)*

We use two primary criteria to rank our models:
*(Modellerimizi sÄ±ralamak iÃ§in iki temel kriter kullanÄ±yoruz:)*

1.  **AIC / BIC:** computed for each fitted model â€” **lower is better**.
    *(Her eÄŸitilen model iÃ§in hesaplanÄ±r â€” daha dÃ¼ÅŸÃ¼k olmasÄ± daha iyidir.)*
2.  **Out-of-Sample MAE or RMSE:** evaluated on a hold-out set â€” **lower is better**.
    *(DÄ±ÅŸarÄ±da tutulan set [test seti] Ã¼zerinde deÄŸerlendirilen Ã¶rneklem dÄ±ÅŸÄ± hata â€” daha dÃ¼ÅŸÃ¼k olmasÄ± daha iyidir.)*

> **ğŸ¯ The Sweet Spot:** When the same model ranks best on **both criteria** (lowest AIC and lowest MAE), youâ€™ve found a well-tuned SARIMA ready for forecasting.

---

## ğŸ“Š Comparison: Plain ARIMA vs. SARIMA
*(KarÅŸÄ±laÅŸtÄ±rma: DÃ¼z ARIMA ve SARIMA)*

Before running the full grid search, let's look at the performance of our initial SARIMA configuration compared to the plain ARIMA we built earlier.



### Interpretation (Yorumlama)
* **Visual Performance:** Both the chart and the error numbers blow the plain ARIMA out of the water.
    *(Hem grafik hem de hata sayÄ±larÄ±, dÃ¼z ARIMA'yÄ± belirgin ÅŸekilde geride bÄ±rakÄ±yor.)*
* **Capturing Peaks:** The SARIMA traces the **weekly peaks** (*haftalÄ±k zirveleri takip ediyor*) instead of flattening them.
* **The "S" Factor:** Adding the **seasonal MA term** let the model capture the **7-day rhythm** that ARIMA kept missing.
    *(Mevsimsel MA terimini eklemek, modelin ARIMA'nÄ±n sÃ¼rekli kaÃ§Ä±rdÄ±ÄŸÄ± 7 gÃ¼nlÃ¼k ritmi yakalamasÄ±nÄ± saÄŸladÄ±.)*

---

## ğŸ’» Try it Yourself: The Mini Grid-Search
*(Kendin Dene: Mini Izgara AramasÄ±)*

Weâ€™ve already fit one SARIMA configuration. Now, we turn that single-model notebook cell into a **mini grid-search** loop that tries every combination we sketched in Step 4.

### Python Implementation
*(Python UygulamasÄ±)*

```python
from darts.models import ARIMA
from sklearn.metrics import mean_absolute_error

# Define the Grid
pdq      = [(4,0,0), (4,0,1), (5,0,0), (5,0,1), (6,0,0), (6,0,1)]
seasonal = [(0,1,0,7), (0,1,1,7)]

best_aic = float('inf')
best_cfg = None

# Loop through all combinations
for order in pdq:
    for s_order in seasonal:
        # Initialize and Fit
        model = ARIMA(order=order, seasonal_order=s_order)
        model.fit(train)

        # Predict and Evaluate
        pred = model.predict(len(test))
        mae  = mean_absolute_error(test.values.flatten(),
                                   pred.values.flatten())
        
        # Access AIC from the underlying statsmodels object
        aic  = model.model.aic          

        print(f"SARIMA{order}Ã—{s_order}   AIC = {aic:.1f}   MAE = {mae:.2f}")

        # Track the Winner (Lowest AIC)
        if aic < best_aic:
            best_aic, best_cfg = aic, (order, s_order)

print(f"\nğŸ† Best model: SARIMA{best_cfg[0]}Ã—{best_cfg[1]}   AIC = {best_aic:.1f}")
```

# ğŸ› ï¸ Common Troubleshooting & Key Takeaways
*(YaygÄ±n Sorun Giderme ve Temel Ã‡Ä±karÄ±mlar)*

SARIMA modelleri bazen beklenmedik sonuÃ§lar verebilir. AÅŸaÄŸÄ±daki tablo, sÄ±k karÅŸÄ±laÅŸÄ±lan belirtileri ve Ã§Ã¶zÃ¼m yollarÄ±nÄ± Ã¶zetler.

---

### ğŸ©º Troubleshooting Matrix
*(Sorun Giderme Matrisi)*

| Symptom (Belirti) | Likely Issue (OlasÄ± Sorun) | Fix (Ã‡Ã¶zÃ¼m) |
| :--- | :--- | :--- |
| **ACF still spikes at seasonal lags**<br>*(ACF hala mevsimsel gecikmelerde sÄ±Ã§rama yapÄ±yor)* | **D or (P,Q) too low**<br>*(D veya P/Q deÄŸerleri Ã§ok dÃ¼ÅŸÃ¼k)* | **Increase D to 1 or raise P/Q**<br>*(D'yi 1'e Ã§Ä±karÄ±n veya P/Q deÄŸerlerini artÄ±rÄ±n.)* |
| **Model diverges / fails to converge**<br>*(Model Ä±raksÄ±yor / yakÄ±nsamada baÅŸarÄ±sÄ±z oluyor)* | **Over-differenced or too many params**<br>*(AÅŸÄ±rÄ± fark alma veya Ã§ok fazla parametre)* | **Reduce D or drop extra terms**<br>*(D'yi azaltÄ±n veya fazladan terimleri Ã§Ä±karÄ±n.)* |
| **Forecast too flat**<br>*(Tahmin Ã§ok dÃ¼z / deÄŸiÅŸim gÃ¶stermiyor)* | **Count or intermittent data**<br>*(SayÄ±m veya kesintili/aralÄ±klÄ± veri)* | **Try SARIMAX or Poisson-based model**<br>*(DÄ±ÅŸsal deÄŸiÅŸkenli [exogenous regressors] SARIMAX veya Poisson tabanlÄ± bir model deneyin.)* |

> â˜ğŸ¼ **Pro Tip:** Selecting a set of values for the model parameters is crucial. We highly recommend following a structured guide on **parameter selection**.
> *(Model parametreleri iÃ§in bir deÄŸer seti seÃ§mek hayati Ã¶nem taÅŸÄ±r. Parametre seÃ§imi konusunda yapÄ±landÄ±rÄ±lmÄ±ÅŸ bir rehberi takip etmenizi ÅŸiddetle Ã¶neririz.)*

---

### ğŸ”‘ Key Take-aways
*(Temel Ã‡Ä±karÄ±mlar)*

1.  **Equation:** **SARIMA = ARIMA + Seasonal Layer**. You only need it when ACF/PACF show repeating **seasonal spikes**.
    *(SARIMA = ARIMA + Mevsimsel Katman. Sadece ACF/PACF tekrarlayan mevsimsel sÄ±Ã§ramalar gÃ¶sterdiÄŸinde buna ihtiyacÄ±nÄ±z vardÄ±r.)*
2.  **Season Length ($s$):** This is the **first big clue** and is almost always known from **business context** (weekly, monthly, quarterly).
    *(Sezon uzunluÄŸu [s] ilk bÃ¼yÃ¼k ipucudur ve neredeyse her zaman iÅŸ baÄŸlamÄ±ndan [haftalÄ±k, aylÄ±k, Ã§eyreklik] bilinir.)*
3.  **Start Simple:** Start with $(p,d,q)$ from ARIMA + $(P,D,Q) = (1,1,1)$ and **iterate**.
    *(Basit baÅŸlayÄ±n: ARIMA'dan gelen [p,d,q] ve [P,D,Q] = [1,1,1] ile baÅŸlayÄ±p tekrarlayarak ilerleyin.)*
4.  **Guidance:** Let **AIC/BIC** & **Validation Error** guide refinement, just like with ARIMA.
    *(Rehberlik: TÄ±pkÄ± ARIMA'da olduÄŸu gibi, iyileÅŸtirme sÃ¼recine AIC/BIC ve DoÄŸrulama HatasÄ±nÄ±n rehberlik etmesine izin verin.)*

# âš ï¸ Shortcomings of ARIMA & SARIMA Methods: A Critical Analysis
*(ARIMA ve SARIMA YÃ¶ntemlerinin KÄ±sÄ±tlamalarÄ±: Kritik Bir Analiz)*

ARIMA ve SARIMA, zaman serisi tahminciliÄŸinin "altÄ±n standardÄ±" olarak kabul edilse de, modern veri setlerinin karmaÅŸÄ±klÄ±ÄŸÄ± karÅŸÄ±sÄ±nda belirgin yapÄ±sal zayÄ±flÄ±klarÄ± vardÄ±r. AÅŸaÄŸÄ±daki analiz, bu modellerin neden ve nerede baÅŸarÄ±sÄ±z olabileceÄŸini teknik olarak detaylandÄ±rÄ±r.

---

### 1. ğŸ“‰ Strict Stationarity Assumption
*(KatÄ± Durgunluk VarsayÄ±mÄ±)*

**The Limitation:** Both models fundamentally assume the data is **stationary** ($Mean$, $Variance$, and $Covariance$ do not change over time).
*(KÄ±sÄ±tlama: Her iki model de verinin temel olarak durgun olduÄŸunu varsayar.)*

* **Technical Detail:**
    * **Over-differencing Risk:** DurgunluÄŸu saÄŸlamak iÃ§in uygulanan fark alma (*differencing, $d$*) iÅŸlemi, serideki sinyali yok edebilir (*over-differencing*) ve modelin "hafÄ±zasÄ±nÄ±" yapay olarak kÄ±saltabilir.
    * **Transformation Issues:** VaryansÄ± sabitlemek iÃ§in yapÄ±lan logaritmik veya Box-Cox dÃ¶nÃ¼ÅŸÃ¼mleri, tahminlerin geri dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmesinde (*inverse transformation*) sapmalara (*bias*) yol aÃ§abilir.
    * **Unit Root Problems:** KarmaÅŸÄ±k serilerde, birim kÃ¶k testleri (ADF, KPSS) Ã§eliÅŸkili sonuÃ§lar verebilir, bu da $d$ parametresinin yanlÄ±ÅŸ seÃ§ilmesine neden olur.

### 2. ğŸ“ Linearity Constraint
*(DoÄŸrusallÄ±k KÄ±sÄ±tÄ±)*

**The Limitation:** ARIMA is a **linear model**. It assumes the future is a linear combination of past values and past errors.
*(KÄ±sÄ±tlama: ARIMA doÄŸrusal bir modeldir. GeleceÄŸin, geÃ§miÅŸ deÄŸerlerin ve geÃ§miÅŸ hatalarÄ±n doÄŸrusal bir kombinasyonu olduÄŸunu varsayar.)*

* **Technical Detail:**
    * **Cannot Capture Volatility:** Finansal verilerde sÄ±k gÃ¶rÃ¼len **Heteroscedasticity** (*deÄŸiÅŸen varyans/oynaklÄ±k*) durumunu modelleyemez (Bunun iÃ§in GARCH ailesi gerekir).
    * **Structural Breaks:** Verideki ani yapÄ±sal kÄ±rÄ±lmalarÄ± (Ã¶rn. pandeminin baÅŸlamasÄ±) yÃ¶netemez; geÃ§miÅŸteki katsayÄ±larÄ± geleceÄŸe uygulamaya devam eder ve bÃ¼yÃ¼k hatalar Ã¼retir.
    * **Complex Interactions:** DeÄŸiÅŸkenler arasÄ±ndaki doÄŸrusal olmayan (*non-linear*) karmaÅŸÄ±k iliÅŸkileri (Ã¶rn. doygunluk noktalarÄ±, eÅŸik etkileri) yakalayamaz.

### 3. ğŸ—“ï¸ Rigidity in Seasonality (SARIMA)
*(Mevsimsellikte KatÄ±lÄ±k)*

**The Limitation:** SARIMA requires a **fixed integer seasonality** (*sabit tam sayÄ± mevsimsellik*) and struggles with multiple seasonal patterns.
*(KÄ±sÄ±tlama: SARIMA sabit tam sayÄ± mevsimsellik gerektirir ve Ã§oklu mevsimsel desenlerde zorlanÄ±r.)*

* **Technical Detail:**
    * **Single Seasonality:** SARIMA sadece tek bir dÃ¶ngÃ¼yÃ¼ (Ã¶rn. sadece haftalÄ±k) modeller. Hem haftalÄ±k hem yÄ±llÄ±k dÃ¶ngÃ¼sÃ¼ olan verilerde (Ã¶rn. gÃ¼nlÃ¼k elektrik tÃ¼ketimi) yetersiz kalÄ±r.
    * **Integer Constraint:** Periyot ($s$) tam sayÄ± olmalÄ±dÄ±r. Ancak gerÃ§ek hayatta bir yÄ±l 52 hafta deÄŸil, **52.14** haftadÄ±r. Bu kayma, uzun vadeli tahminlerde faz hatasÄ±na (*phase shift*) neden olur.
    * **Dynamic Seasonality:** MevsimselliÄŸin zamanla deÄŸiÅŸtiÄŸi (*Modulated Seasonality*) durumlarda (Ã¶rn. mevsimlerin kaymasÄ±) model adapte olamaz.

### 4. ğŸŒ High Computational Cost
*(YÃ¼ksek Hesaplama Maliyeti)*

**The Limitation:** Fitting these models, especially with **Grid Search** (Auto-ARIMA), is computationally expensive ($O(N^2)$ or worse).
*(KÄ±sÄ±tlama: Bu modelleri eÄŸitmek, Ã¶zellikle Izgara AramasÄ± ile, hesaplama aÃ§Ä±sÄ±ndan pahalÄ±dÄ±r.)*

* **Technical Detail:**
    * **Stepwise Estimation:** En iyi $(p,d,q)(P,D,Q)$ kombinasyonunu bulmak iÃ§in modelin yÃ¼zlerce kez yeniden eÄŸitilmesi ve her seferinde **AIC/BIC** hesaplanmasÄ± gerekir.
    * **Large Lags:** Uzun mevsimsel periyotlar (Ã¶rn. saatlik veride $s=168$ haftalÄ±k dÃ¶ngÃ¼) parametre uzayÄ±nÄ± patlatÄ±r ve optimizasyonun yakÄ±nsamamasÄ±na (*convergence failure*) neden olabilir.

### 5. ğŸš« Limited Exogenous Support (ARIMA vs. ARIMAX)
*(SÄ±nÄ±rlÄ± DÄ±ÅŸsal Destek)*

**The Limitation:** Standard ARIMA relies solely on endogenous (internal) data.
*(KÄ±sÄ±tlama: Standart ARIMA yalnÄ±zca iÃ§sel verilere dayanÄ±r.)*

* **Technical Detail:**
    * **Requirement for Future Values:** **ARIMAX** veya **SARIMAX** gibi uzantÄ±lar dÄ±ÅŸsal deÄŸiÅŸkenleri (*Exogenous Variables*) desteklese de, tahmin yapabilmek iÃ§in bu dÄ±ÅŸsal deÄŸiÅŸkenlerin **gelecekteki deÄŸerlerini** de bilmeniz gerekir (Ã¶rn. yarÄ±nÄ±n satÄ±ÅŸÄ±nÄ± tahmin etmek iÃ§in yarÄ±nÄ±n hava durumunu bilmek gerekir). Bu, pratikte uygulanabilirliÄŸi zorlaÅŸtÄ±rÄ±r.

---

### ğŸ”„ Alternative Approaches
*(Alternatif YaklaÅŸÄ±mlar)*

Bu kÄ±sÄ±tlamalarÄ± aÅŸmak iÃ§in kullanÄ±lan diÄŸer klasik ve modern yÃ¶ntemler:

| YÃ¶ntem (Method) | Ã‡Ã¶zdÃ¼ÄŸÃ¼ Sorun (Problem Solved) | AvantajÄ± (Advantage) |
| :--- | :--- | :--- |
| **ETS (Exponential Smoothing)** | **Non-Stationarity & Seasonality** | Veriyi duraÄŸanlaÅŸtÄ±rmaya gerek duymaz; trend ve mevsimselliÄŸi doÄŸrudan bileÅŸen olarak modeller. |
| **Prophet (Meta)** | **Multiple Seasonality & Missing Data** | Birden fazla dÃ¶ngÃ¼yÃ¼ (haftalÄ±k + yÄ±llÄ±k) ve tatilleri esnek bir ÅŸekilde modeller; eksik verilere dayanÄ±klÄ±dÄ±r. |
| **TBATS** | **Complex/Non-Integer Seasonality** | KarmaÅŸÄ±k ve tam sayÄ± olmayan mevsimsellikleri (Ã¶rn. 365.25) trigonometrik fonksiyonlarla Ã§Ã¶zer. |
| **Machine Learning (XGBoost/LightGBM)** | **Non-Linearity & Exogenous Vars** | DoÄŸrusal olmayan iliÅŸkileri mÃ¼kemmel yakalar; dÄ±ÅŸsal deÄŸiÅŸkenleri yÃ¶netmek Ã§ok daha kolaydÄ±r. |

---

> **ğŸ’¡ Expert Verdict:**
> ARIMA/SARIMA are excellent for **short-term forecasting** on **simple, stable datasets** where interpretability is key. However, for complex, volatile, or multi-seasonal real-world data, exploring **Machine Learning** or hybrid methods (like Prophet) is often necessary.
> *(Uzman KararÄ±: ARIMA/SARIMA, yorumlanabilirliÄŸin kilit olduÄŸu basit ve kararlÄ± veri setlerinde kÄ±sa vadeli tahminler iÃ§in mÃ¼kemmeldir. Ancak karmaÅŸÄ±k, oynak veya Ã§oklu mevsimselliÄŸe sahip gerÃ§ek dÃ¼nya verileri iÃ§in Makine Ã–ÄŸrenmesi veya hibrit yÃ¶ntemleri keÅŸfetmek genellikle gereklidir.)*

# ğŸ“ Quiz 3: ARIMA & SARIMA Conceptual Check
*(Quiz 3: ARIMA ve SARIMA Kavramsal Kontrol)*

AÅŸaÄŸÄ±daki sorular ve teknik aÃ§Ä±klamalar, Zaman Serisi Modelleme konusundaki temel kavramlarÄ± (ARIMA bileÅŸenleri, SARIMA'nÄ±n farkÄ±, DuraÄŸanlÄ±k ve Mevsimsellik) pekiÅŸtirmek iÃ§in hazÄ±rlanmÄ±ÅŸtÄ±r.

---

### â“ Question 1
**What does ARIMA stand for?**
*(ARIMA neyin kÄ±saltmasÄ±dÄ±r?)*

* **Correct Answer (DoÄŸru Cevap):** **B - Autoregressive Integrated Moving Average**

> **ğŸ’¡ Technical Explanation (Teknik AÃ§Ä±klama):**
> ARIMA, zaman serisi analizinin Ã¼Ã§ temel bileÅŸeninin matematiksel birleÅŸimidir. Ä°sim, modelin iÃ§ yapÄ±sÄ±nÄ± doÄŸrudan tarif eder:
> * **AR (AutoRegressive):** Gelecekteki deÄŸerin, geÃ§miÅŸ deÄŸerlerin doÄŸrusal bir kombinasyonu olduÄŸunu varsayar ($p$).
> * **I (Integrated):** Seriyi **duraÄŸan** (*stationary*) hale getirmek iÃ§in uygulanan fark alma iÅŸlemidir ($d$).
> * **MA (Moving Average):** Modelin tahmin hatasÄ±nÄ± simÃ¼le eder ($q$).
>
> *Ã–zetle ARIMA, verinin kendi geÃ§miÅŸiyle (AR) ve geÃ§miÅŸ hata paylarÄ±yla (MA) iliÅŸkisini kurarken, trend etkisinden arÄ±ndÄ±rÄ±lmÄ±ÅŸ (I) bir yapÄ± Ã¼zerinde Ã§alÄ±ÅŸÄ±r.*

---

### â“ Question 2
**Which of the following models is best suited for time-series data with strong seasonal patterns?**
*(AÅŸaÄŸÄ±daki modellerden hangisi gÃ¼Ã§lÃ¼ mevsimsel kalÄ±plara sahip zaman serisi verileri iÃ§in en uygundur?)*

* **Correct Answer (DoÄŸru Cevap):** **C - SARIMA**

> **ğŸ’¡ Technical Explanation (Teknik AÃ§Ä±klama):**
> Standart ARIMA modelleri "kÄ±sa vadeli hafÄ±zaya" sahiptir ve genel trendleri yakalar. Ancak veride dÃ¼zenli aralÄ±klarla tekrarlayan (Ã¶rn. her AralÄ±k ayÄ±nda artan satÄ±ÅŸlar) gÃ¼Ã§lÃ¼ bir **mevsimsellik** varsa yetersiz kalÄ±r.
>
> **SARIMA (Seasonal ARIMA)**, modele ikinci bir katman ekleyerek bu sorunu Ã§Ã¶zer. Sadece "dÃ¼n" ($t-1$) ile deÄŸil, "geÃ§en sezonun aynÄ± dÃ¶nemi" ($t-s$) ile de iliÅŸki kurar.

---

### â“ Question 3
**What is the purpose of the Seasonal Period (m) in SARIMA?**
*(SARIMA'da Mevsimsel Periyodun (m) amacÄ± nedir?)*

* **Correct Answer (DoÄŸru Cevap):** **A - To determine the length of the seasonal cycle**
*(Mevsimsel dÃ¶ngÃ¼nÃ¼n uzunluÄŸunu belirlemek)*

> **ğŸ’¡ Technical Explanation (Teknik AÃ§Ä±klama):**
> LiteratÃ¼rde genellikle $s$ veya $m$ olarak gÃ¶sterilen bu parametre, modelin "bir tam dÃ¶ngÃ¼yÃ¼ tamamlamak iÃ§in kaÃ§ zaman adÄ±mÄ±na ihtiyaÃ§ duyduÄŸunu" tanÄ±mlar.
> * **AylÄ±k Veri:** $m=12$ (YÄ±llÄ±k desen).
> * **GÃ¼nlÃ¼k Veri:** $m=7$ (HaftalÄ±k desen).
>
> Teknik olarak $m$, mevsimsel fark alma iÅŸleminde hangi gecikmedeki deÄŸerin Ã§Ä±karÄ±lacaÄŸÄ±nÄ± ($y_t - y_{t-m}$) belirler.

---

### â“ Question 4
**What is the primary difference between ARIMA and SARIMA?**
*(ARIMA ve SARIMA arasÄ±ndaki temel fark nedir?)*

* **Correct Answer (DoÄŸru Cevap):** **C - SARIMA includes additional terms to handle seasonal patterns, unlike ARIMA.**
*(SARIMA, ARIMA'dan farklÄ± olarak mevsimsel kalÄ±plarÄ± iÅŸlemek iÃ§in ek terimler iÃ§erir.)*

> **ğŸ’¡ Technical Explanation (Teknik AÃ§Ä±klama):**
> Fark, matematiksel yapÄ±dadÄ±r:
> * **ARIMA $(p,d,q)$:** YalnÄ±zca mevsimsel olmayan otokorelasyonu modeller.
> * **SARIMA $(p,d,q) \times (P,D,Q)_m$:** ARIMA'yÄ± kapsar ancak ona **Ã§arpÄ±msal** (*multiplicative*) bir yapÄ± ekler.
>
> SARIMA, hem "dÃ¼nkÃ¼ hatayÄ±" hem de "geÃ§en yÄ±lÄ±n aynÄ± gÃ¼nÃ¼ndeki hatayÄ±" denkleme dahil ederek Ã§ok katmanlÄ± serileri modeller.

---

### â“ Question 5
**Which of the following statements about ARIMA is true?**
*(ARIMA ile ilgili aÅŸaÄŸÄ±daki ifadelerden hangisi doÄŸrudur?)*

* **Correct Answer (DoÄŸru Cevap):** **B - ARIMA requires data to be stationary for accurate predictions.**
*(ARIMA, doÄŸru tahminler iÃ§in verilerin duraÄŸan olmasÄ±nÄ± gerektirir.)*

> **ğŸ’¡ Technical Explanation (Teknik AÃ§Ä±klama):**
> Bu, ARIMA'nÄ±n en temel varsayÄ±mÄ±dÄ±r. **DuraÄŸanlÄ±k** (*Stationarity*); ortalama, varyans ve otokovaryansÄ±n zamanla deÄŸiÅŸmemesi anlamÄ±na gelir.
>
> ARIMA doÄŸrusal bir model olduÄŸu iÃ§in, geÃ§miÅŸteki katsayÄ±larÄ± geleceÄŸe uygular. EÄŸer veride trend veya deÄŸiÅŸen varyans varsa, bu katsayÄ±lar geÃ§ersiz olur. Bu nedenle **"I" (Integrated)** bileÅŸeni ile fark alÄ±narak veri duraÄŸanlaÅŸtÄ±rÄ±lÄ±r.

---
---

# ğŸ¤– Machine Learning for Time Series: Beyond Classical Methods
*(Zaman Serileri iÃ§in Makine Ã–ÄŸrenimi: Klasik YÃ¶ntemlerin Ã–tesi)*

Geleneksel yÃ¶ntemler (ARIMA, SARIMA, ETS), verileriniz "uslu durduÄŸunda" (*low noise, clear seasonality*) oldukÃ§a gÃ¼Ã§lÃ¼dÃ¼r. Ancak modern iÅŸ dÃ¼nyasÄ± verileri genellikle karmaÅŸÄ±ktÄ±r: binlerce Ã¼rÃ¼n, onlarca dÄ±ÅŸsal sinyal (*exogenous signals*), rejim deÄŸiÅŸiklikleri (*regime shifts*) ve doÄŸrusal olmayan etkileÅŸimler iÃ§erir.

Bu dokÃ¼man, Zaman Serisi tahminciliÄŸinde neden ve nasÄ±l **Makine Ã–ÄŸrenimi (ML)** yÃ¶ntemlerine geÃ§iÅŸ yapÄ±ldÄ±ÄŸÄ±nÄ± teknik derinlikle aÃ§Ä±klar.

---

## 1. ğŸš€ Why Go Beyond "Classic" Forecasting?
*(Neden "Klasik" Tahminin Ã–tesine GeÃ§meliyiz?)*

ARIMA gibi modeller tek deÄŸiÅŸkenli (*univariate*) ve doÄŸrusal (*linear*) varsayÄ±mlara dayanÄ±r. Makine Ã¶ÄŸrenimi modelleri ise ÅŸu avantajlarÄ± sunar:

* **Non-Linear Patterns (DoÄŸrusal Olmayan Desenler):** SatÄ±ÅŸlar ve fiyat arasÄ±ndaki iliÅŸki genellikle doÄŸrusal deÄŸildir (Ã¶rn. fiyat belli bir eÅŸiÄŸi geÃ§ince satÄ±ÅŸlar Ã§akÄ±lÄ±r). ML modelleri (Ã¶zellikle aÄŸaÃ§ tabanlÄ±lar ve sinir aÄŸlarÄ±) bu karmaÅŸÄ±k etkileÅŸimleri otomatik Ã¶ÄŸrenir.
* **Covariates & Exogenous Variables (DÄ±ÅŸsal DeÄŸiÅŸkenler):** Klasik modellerde dÄ±ÅŸsal deÄŸiÅŸken eklemek (*ARIMAX*) zordur. ML modelleri hava durumu, promosyonlar, web trafiÄŸi gibi yÃ¼zlerce deÄŸiÅŸkeni (*feature*) zorlanmadan modele dahil eder.
* **Global Models & Cross-Learning (Global Modeller ve Ã‡apraz Ã–ÄŸrenme):**
    * *Classic:* Her Ã¼rÃ¼n iÃ§in ayrÄ± bir ARIMA modeli eÄŸitilir (1000 Ã¼rÃ¼n = 1000 model).
    * *ML:* Tek bir model, 1000 Ã¼rÃ¼nÃ¼n tamamÄ±ndan veriyi Ã¶ÄŸrenerek (*shared parameters*), geÃ§miÅŸi az olan yeni Ã¼rÃ¼nler (*cold-start problem*) iÃ§in bile diÄŸer Ã¼rÃ¼nlerden Ã¶ÄŸrendiÄŸi kalÄ±plarÄ± kullanarak tahmin yapabilir.
* **Forecasting Strategy (Tahmin Stratejisi):** ML modelleri, hatayÄ± adÄ±m adÄ±m biriktiren Ã¶zyinelemeli (*recursive*) yÃ¶ntemler yerine, geleceÄŸi doÄŸrudan tahmin eden (*Direct Multi-step Forecast*) stratejileri daha iyi uygulayabilir.

---

## 2. ğŸ”„ How ML Treats Time-Series Differently
*(ML Zaman Serilerine NasÄ±l FarklÄ± DavranÄ±r?)*

ML algoritmalarÄ± (XGBoost, Neural Networks) zamanÄ±n sÄ±ralÄ± yapÄ±sÄ±nÄ± doÄŸrudan anlamazlar; veriyi onlara "Ã¶ÄŸretmemiz" gerekir.

| Konu (Topic) | Classical View (ARIMA/ETS) | ML View (Machine Learning) |
| :--- | :--- | :--- |
| **Input Shape**<br>*(Girdi Åekli)* | **1-D Sequence:** Veri sÄ±ralÄ± bir vektÃ¶rdÃ¼r. Model, $t$'deki deÄŸeri $t-1$'e bakarak tahmin eder. | **Tabular / Matrix:** Veri, denetimli Ã¶ÄŸrenme (*Supervised Learning*) problemine dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmelidir. Kayan pencereler (*Sliding Windows*) kullanÄ±larak Ã–zellik Matrisi ($X$) ve Hedef VektÃ¶rÃ¼ ($y$) oluÅŸturulur. |
| **Stationarity**<br>*(Durgunluk)* | **Critical:** Trend ve varyans sabitlenmelidir (Fark alma, Log dÃ¶nÃ¼ÅŸÃ¼mÃ¼). | **Flexible:** AÄŸaÃ§ tabanlÄ± modeller duraÄŸan olmayan verilerle baÅŸa Ã§Ä±kabilir, ancak trendi "extrapolate" edemezler (eÄŸitim setindeki max deÄŸerin Ã¼zerine Ã§Ä±kamazlar). Bu yÃ¼zden trendin arÄ±ndÄ±rÄ±lmasÄ± (*Detrending*) ML iÃ§in de Ã¶nemlidir. |
| **Model Family**<br>*(Model Ailesi)* | **Parametric:** KatsayÄ±larÄ± bellidir, yorumlanabilirliÄŸi yÃ¼ksektir. | **Non-Parametric / Black Box:** Esnektir (AÄŸaÃ§lar, Sinir AÄŸlarÄ±), Ã§ok karmaÅŸÄ±k fonksiyonlarÄ± Ã¶ÄŸrenir ancak "neden" sorusunu yanÄ±tlamak (Feature Importance hariÃ§) zordur. |
| **Forecast Strategy**<br>*(Tahmin Stratejisi)* | **Local (One-by-One):** Her seri iÃ§in parametreler optimize edilir. | **Global:** Binlerce seri tek bir havuzda toplanÄ±r. Model genel kalÄ±plarÄ± (*global structure*) Ã¶ÄŸrenir. |
| **Feature Engineering**<br>*(Ã–zellik MÃ¼h.))* | **Minimal:** Lag ve Moving Average modelin iÃ§indedir. | **Heavy:** Lag, Rolling Mean, Calendar Features (Ay, GÃ¼n) manuel olarak Ã¼retilmelidir. |



---

## 3. ğŸ¤– Common ML Models for Time-Series
*(Zaman Serileri iÃ§in YaygÄ±n ML Modelleri)*

### A. Tree-Based Ensembles (AÄŸaÃ§ TabanlÄ± Topluluklar)
* **Models:** XGBoost, LightGBM, CatBoost, Random Forest.
* **Mechanism:** Karar aÄŸaÃ§larÄ±nÄ±n topluluÄŸunu oluÅŸturarak tahmin yapar. Veriyi "bÃ¶lerek" (*splitting*) Ã¶ÄŸrenir.
* **âœ… Pros:**
    * Tablo ÅŸeklindeki verilerde (*Tabular Data*) ve heterojen Ã¶zelliklerde (kategorik + sayÄ±sal) SOTA (*State-of-the-Art*) performansÄ± verir.
    * Eksik verileri (*Missing Values*) doÄŸal olarak yÃ¶netir.
    * HÄ±zlÄ±dÄ±r ve yorumlanabilir (*Feature Importance*).
* **âŒ Cons:**
    * **Extrapolation Problem:** EÄŸitim verisinde gÃ¶rdÃ¼ÄŸÃ¼ maksimum deÄŸerden daha yÃ¼ksek bir deÄŸer tahmin edemez. Trend varsa mutlaka veri temizlenmeli veya lineer bir modelle birleÅŸtirilmelidir.
    * Manuel **Feature Engineering** (Lag, Rolling) gerektirir.

### B. Recurrent Neural Networks (RNNs)
* **Models:** LSTM (*Long Short-Term Memory*), GRU (*Gated Recurrent Unit*).
* **Mechanism:** Veriyi sÄ±ralÄ± iÅŸler. "Hidden State" (Gizli Durum) sayesinde geÃ§miÅŸ bilgiyi hafÄ±zasÄ±nda tutar.
* **âœ… Pros:**
    * Uzun vadeli baÄŸÄ±mlÄ±lÄ±klarÄ± (*Long-term dependencies*) ve sÄ±ralÄ± kalÄ±plarÄ± doÄŸal olarak Ã¶ÄŸrenir.
    * Manuel Ã¶zellik mÃ¼hendisliÄŸi ihtiyacÄ± daha azdÄ±r.
* **âŒ Cons:**
    * SÄ±ralÄ± iÅŸlem yaptÄ±ÄŸÄ± iÃ§in eÄŸitimi yavaÅŸtÄ±r (*Non-parallelizable*).
    * "Vanishing Gradient" problemi yaÅŸayabilir.



### C. Temporal Convolutional Networks (TCNs) & 1-D CNNs
* **Mechanism:** GÃ¶rÃ¼ntÃ¼ iÅŸlemedeki CNN'lerin zaman serisine uyarlanmÄ±ÅŸ halidir. GeniÅŸleyen evriÅŸimler (*Dilated Convolutions*) kullanarak geniÅŸ bir geÃ§miÅŸe bakar.
* **âœ… Pros:**
    * RNN'lerden Ã§ok daha hÄ±zlÄ±dÄ±r (Paralel iÅŸlem yapÄ±labilir).
    * Uzun dizilerde kararlÄ±dÄ±r.
* **âŒ Cons:**
    * Ã‡ok fazla veri gerektirir.



### D. Transformers / Attention Models
* **Models:** TFT (*Temporal Fusion Transformer*), Informer, Autoformer.
* **Mechanism:** "Attention" (*Dikkat*) mekanizmasÄ± ile serinin hangi geÃ§miÅŸ noktalarÄ±nÄ±n o anki tahmin iÃ§in Ã¶nemli olduÄŸuna odaklanÄ±r.
* **âœ… Pros:**
    * Uzun ufuklu tahminlerde (*Long-horizon forecasts*) ÅŸu anki en ileri teknolojidir.
    * Yorumlanabilirlik (*Interpretability*) sunar (TFT).
* **âŒ Cons:**
    * GPU gÃ¼cÃ¼ ve Ã§ok bÃ¼yÃ¼k veri seti gerektirir.

### E. Hybrid Models (Statistical + ML)
* **Models:** Prophet (Trend + Seasonality + Regressors), ES-RNN, N-BEATS.
* **Mechanism:** Ä°statistiÄŸin (Trend/Mevsimsellik ayrÄ±ÅŸtÄ±rma - STL) gÃ¼cÃ¼nÃ¼ ML'in (ArtÄ±klar/Residuals Ã¼zerindeki Ã¶ÄŸrenme) gÃ¼cÃ¼yle birleÅŸtirir.
* **âœ… Pros:**
    * "Best of both worlds": Hem trendi iyi yÃ¶netir hem de karmaÅŸÄ±k iliÅŸkileri.

---

## ğŸ“… Next Steps

1.  **Tree-Based Methods:** XGBoost kullanarak **Feature Engineering** (Ã–zellik MÃ¼hendisliÄŸi) tekniklerine ve denetimli Ã¶ÄŸrenme dÃ¶nÃ¼ÅŸÃ¼mÃ¼ne odaklanacaÄŸÄ±z.
2.  **Deep Learning:** LSTM ve RNN mimarilerini inceleyerek sÄ±ralÄ± veri modellemeyi Ã¶ÄŸreneceÄŸiz.


2.2. Creating Rolling Statistics: <img width="652" height="351" alt="image" src="https://github.com/user-attachments/assets/2fec113a-327a-4fcb-9eca-fc62a18f8205" />


# ğŸ› ï¸ Feature Engineering for Time Series: Lags & Rolling Windows
*(Zaman Serileri iÃ§in Ã–zellik MÃ¼hendisliÄŸi: Gecikmeler ve Kayan Pencereler)*

AÄŸaÃ§ tabanlÄ± algoritmalar (*Decision Trees, XGBoost, LightGBM, Random Forest*), her bir veri satÄ±rÄ±nÄ± zamandan baÄŸÄ±msÄ±z, tekil bir anlÄ±k gÃ¶rÃ¼ntÃ¼ (*independent snapshot*) olarak ele alÄ±r. Bu modeller, biz onlara dÃ¼nÃ¼n deÄŸerini ayrÄ± bir sÃ¼tun olarak vermediÄŸimiz sÃ¼rece "dÃ¼nÃ¼" hatÄ±rlamazlar.

Modelin **momentum**, **ortalamaya dÃ¶nÃ¼ÅŸ** (*mean-reversion*) ve **mevsimsellik** (*seasonality*) gibi kalÄ±plarÄ± Ã¶ÄŸrenebilmesi iÃ§in, **Gecikme** (*Lag*) ve **Kayan Pencere** (*Rolling-Window*) Ã¶zelliklerini kullanarak veriye zamansal bir hafÄ±za enjekte etmeliyiz.

---

## 1. The Core Feature Types
*(Temel Ã–zellik TÃ¼rleri)*

Zaman serisi problemlerinde kullanÄ±lan en yaygÄ±n Ã¶zelliklerin teknik Ã¶zeti:

| Feature (Ã–zellik) | What it captures (Neyi Yakalar?) | Typical Notation (Tipik GÃ¶sterim) |
| :--- | :--- | :--- |
| **Lag**<br>*(Gecikme)* | **Exact Memory:** $k$ adÄ±m Ã¶nceki kesin deÄŸer. Modelin otokorelasyonu (*autocorrelation*) Ã¶ÄŸrenmesini saÄŸlar.<br>*Ã–rn: DÃ¼nkÃ¼ satÄ±ÅŸ.* | `lag_1`, `lag_7`, `lag_30` |
| **Rolling Mean**<br>*(Kayan Ortalama)* | **Local Trend / Level:** Belirli bir penceredeki ortalama seviye. Modelin "temel Ã§izgisini" (*baseline*) belirler.<br>*Ã–rn: Son 7 gÃ¼nÃ¼n ortalamasÄ±.* | `roll_mean_7`, `ma_7` |
| **Rolling Std / Var**<br>*(Kayan Standart Sapma)* | **Volatility / Uncertainty:** YakÄ±n geÃ§miÅŸteki oynaklÄ±k. Verinin ne kadar kararsÄ±z olduÄŸunu gÃ¶sterir.<br>*Ã–rn: Son 14 gÃ¼ndeki deÄŸiÅŸim.* | `roll_std_14` |
| **Count-since-last-zero**<br>*(Son sÄ±fÄ±rdan beri geÃ§en sÃ¼re)* | **Inter-arrival Info:** Kesintili talep (*intermittent demand*) veya nadir olaylar iÃ§in geÃ§en sÃ¼reyi Ã¶lÃ§er. | `days_since_last_sale` |



---

## 2. Step-by-Step Code Walkthrough
*(AdÄ±m AdÄ±m Kod Rehberi)*

### 2.1. Loading and Preparing Data
*(Veriyi YÃ¼kleme ve HazÄ±rlama)*

Veri manipÃ¼lasyonu iÃ§in gerekli kÃ¼tÃ¼phaneleri yÃ¼kleyerek ve veri setini hazÄ±rlayarak baÅŸlÄ±yoruz.

*(Not: Veri setinizin `datetime` indeksine sahip olduÄŸundan ve frekansÄ±nÄ±n (gÃ¼nlÃ¼k/saatlik) dÃ¼zgÃ¼n ayarlandÄ±ÄŸÄ±ndan emin olun.)*

### 2.2. Feature Engineering for Machine Learning
*(Makine Ã–ÄŸrenimi iÃ§in Ã–zellik MÃ¼hendisliÄŸi)*

GÃ¶zetimli makine Ã¶ÄŸrenimi modelleri (*Supervised ML models*), girdi olarak bir Ã¶zellik koleksiyonuna ihtiyaÃ§ duyar. XGBoost gibi modellerin zaman serisi tahmininde baÅŸarÄ±lÄ± olabilmesi iÃ§in ham veriyi anlamlÄ± sinyallere dÃ¶nÃ¼ÅŸtÃ¼rmeliyiz.

#### A. Creating Lag Features (Gecikme Ã–zellikleri OluÅŸturma)
Lag Ã¶zellikleri, zaman serisinin geÃ§miÅŸ deÄŸerlerini temsil eder. Bu Ã¶zellikler, modelin "DÃ¼n ne oldu?", "GeÃ§en hafta bugÃ¼n ne oldu?" sorularÄ±na cevap vermesini saÄŸlar.

```python
# Create lag features (e.g., sales from the previous day, previous week)
# Gecikme Ã¶zellikleri oluÅŸturma (Ã¶rn. Ã¶nceki gÃ¼nÃ¼n, Ã¶nceki haftanÄ±n satÄ±ÅŸlarÄ±)

# t-1: DÃ¼nkÃ¼ deÄŸer (En Ã¶nemli Ã¶zelliklerden biridir)
df_filtered['lag_1'] = df_filtered['unit_sales'].shift(1)

# t-7: GeÃ§en hafta aynÄ± gÃ¼nÃ¼n deÄŸeri (HaftalÄ±k mevsimselliÄŸi yakalar)
df_filtered['lag_7'] = df_filtered['unit_sales'].shift(7)

# t-30: GeÃ§en ayki deÄŸer (AylÄ±k dÃ¶ngÃ¼yÃ¼ yakalar)
df_filtered['lag_30'] = df_filtered['unit_sales'].shift(30)

# Drop NaN values created by shifting
# Lag iÅŸlemi (shift) ilk satÄ±rlarda NaN (boÅŸ) deÄŸerler oluÅŸturur, bunlarÄ± temizlemeliyiz.
df_filtered.dropna(inplace=True)
```

# ğŸš¨ Critical Concept: Data Leakage & Rolling Features
*(Kritik Kavram: Veri SÄ±zÄ±ntÄ±sÄ± ve Kayan Ã–zellikler)*

Zaman serisi Ã¶zellik mÃ¼hendisliÄŸinde en sÄ±k yapÄ±lan ve en tehlikeli hata, gelecekteki bilgiyi modele sÄ±zdÄ±rmaktÄ±r.

---

## 1. The "Shift" Imperative: Avoiding Data Leakage
*(KaydÄ±rma ZorunluluÄŸu: Veri SÄ±zÄ±ntÄ±sÄ±ndan KaÃ§Ä±nma)*

**The Rule:** You cannot use data from time $t$ to predict time $t$. You must use data from $t-1, t-2...$
*(Kural: t zamanÄ±nÄ± tahmin etmek iÃ§in t zamanÄ±ndaki veriyi kullanamazsÄ±nÄ±z. t-1, t-2... verilerini kullanmalÄ±sÄ±nÄ±z.)*

### ğŸš« The Mistake (Hata)
If you calculate a rolling mean **without shifting**:
`df['rolling_mean'] = df['sales'].rolling(7).mean()`
* The mean for "Today" includes "Today's Sales".
* The model sees the answer (Target) inside the input feature.
* **Result:** 99% accuracy in training, massive failure in production. This is called **Look-ahead Bias**.

### âœ… The Fix (Ã‡Ã¶zÃ¼m)
Always **shift first**, then roll.
`df['rolling_mean'] = df['sales'].shift(1).rolling(7).mean()`
* Now, "Today's" rolling mean is actually calculated using data from "Yesterday" backwards.



---

## 2. Mastering Rolling Statistics
*(Kayan Ä°statistiklerde UstalaÅŸmak)*

Kayan istatistikler, veriye **baÄŸlam** (*context*) kazandÄ±rÄ±r. Tek bir veri noktasÄ± gÃ¼rÃ¼ltÃ¼lÃ¼ olabilir, ancak bir pencerenin Ã¶zeti daha kararlÄ± bir sinyaldir.



### ğŸ“ˆ Rolling Mean (Kayan Ortalama)
* **Purpose:** Smooths out short-term noise and captures the **Local Level/Trend**.
    *(KÄ±sa vadeli gÃ¼rÃ¼ltÃ¼yÃ¼ yumuÅŸatÄ±r ve Yerel Seviyeyi/Trendi yakalar.)*
* **Usage:** Acts as a "baseline" prediction. If the rolling mean is rising, the tree model sets a higher starting point for the forecast.

### ğŸ“Š Rolling Standard Deviation (Kayan Standart Sapma)
* **Purpose:** Measures **Volatility** (*OynaklÄ±k*) and **Uncertainty** (*Belirsizlik*).
* **Usage:**
    * **Low Rolling Std:** The series is stable; the model can trust the `lag_1` value more.
    * **High Rolling Std:** The series is chaotic; the model may be more conservative or rely more on the rolling mean than the immediate lag.

---

## ğŸ’» Correct Implementation Pattern
*(DoÄŸru Uygulama Deseni)*

```python
# 1. Correct: Shift THEN Roll (No Leakage)
# DoÄŸru: Ã–nce KaydÄ±r SONRA Yuvarla (SÄ±zÄ±ntÄ± Yok)
df['feature_roll_mean_7'] = df['sales'].shift(1).rolling(window=7).mean()

# 2. Wrong: Roll on current values (Leakage!)
# YanlÄ±ÅŸ: Mevcut deÄŸerler Ã¼zerinde yuvarla (SÄ±zÄ±ntÄ±!)
# df['feature_roll_mean_7'] = df['sales'].rolling(window=7).mean()  <-- DO NOT DO THIS

```

# ğŸ“… Date-based Features & Model Logic
*(Tarih BazlÄ± Ã–zellikler ve Model MantÄ±ÄŸÄ±)*

Gecikme (*lag*) ve kayan pencere (*rolling window*) Ã¶zelliklerine ek olarak, zamanÄ±n kendisinden tÃ¼retilen Ã¶zellikler, modelin **periyodik desenleri** (*periodic patterns*) Ã¶ÄŸrenmesi iÃ§in kritiktir.

AÄŸaÃ§ tabanlÄ± modeller (*Tree-based models*), "zamanÄ±n akÄ±ÅŸÄ±nÄ±" bilmezler. Onlar iÃ§in `2023-12-31` ile `2024-01-01` arasÄ±ndaki iliÅŸki belirsizdir. Bu iliÅŸkiyi aÃ§Ä±k hale getirmek iÃ§in zaman damgasÄ±nÄ± parÃ§alarÄ±na ayÄ±rmalÄ±yÄ±z.

---

### 1. ğŸ“† Extracting Date Features
*(Tarih Ã–zelliklerini Ã‡Ä±karma)*

Zaman damgasÄ±ndan (*timestamp*) aÅŸaÄŸÄ±daki Ã¶zellikleri tÃ¼reterek modelin mevsimselliÄŸi yakalamasÄ±nÄ± saÄŸlarÄ±z:

```python
# Assuming the index is a datetime object
# Ä°ndeksin datetime objesi olduÄŸunu varsayÄ±yoruz

# 1. Basic Calendar Features (Temel Takvim Ã–zellikleri)
df_filtered['day_of_week'] = df_filtered.index.dayofweek  # 0=Mon, 6=Sun
df_filtered['day_of_month'] = df_filtered.index.day
df_filtered['month'] = df_filtered.index.month
df_filtered['quarter'] = df_filtered.index.quarter
df_filtered['year'] = df_filtered.index.year

# 2. Boolean Flags (MantÄ±ksal Bayraklar)
# Haftasonu etkisi iÃ§in (Cumartesi/Pazar)
df_filtered['is_weekend'] = df_filtered.index.dayofweek.isin([5, 6]).astype(int)

# YÄ±l sonu/baÅŸÄ± etkisi iÃ§in
df_filtered['is_month_start'] = df_filtered.index.is_month_start.astype(int)
df_filtered['is_month_end'] = df_filtered.index.is_month_end.astype(int)
```

## 2. ğŸ§  How the Model Uses These Features
*(Model Bu Ã–zellikleri NasÄ±l KullanÄ±r?)*

Bir karar aÄŸacÄ± (*Decision Tree*), bu Ã¶zellikleri kullanarak tahmin uzayÄ±nÄ± "bÃ¶ler" (*split*). Modelin bu Ã¶zellikleri nasÄ±l yorumladÄ±ÄŸÄ±nÄ± anlamak, model baÅŸarÄ±sÄ±nÄ± artÄ±rÄ±r.


| Feature Interaction (Ã–zellik EtkileÅŸimi) | Decision Tree Logic (Karar AÄŸacÄ± MantÄ±ÄŸÄ±) |
| :--- | :--- |
| **Seasonal Spikes**<br>*(Mevsimsel SÄ±Ã§ramalar)* | **Logic:** `IF is_weekend == 1 THEN Predict High`<br><br>AÄŸaÃ§, hafta sonu bayraÄŸÄ±nÄ± gÃ¶rdÃ¼ÄŸÃ¼nde dallanÄ±r ve Cumartesi/Pazar iÃ§in temel tahmin deÄŸerini yÃ¼kseltir (*boost*). |
| **Holiday Effects**<br>*(Tatil Etkileri)* | **Logic:** `IF month == 12 AND day > 20 THEN Predict Very High`<br><br>Model, AralÄ±k ayÄ±nÄ±n son gÃ¼nlerinde ortalamalarÄ±n yÃ¼ksek olduÄŸunu Ã¶ÄŸrenerek yÄ±l sonu yoÄŸunluÄŸunu yakalar. |
| **Local Smoothing**<br>*(Yerel YumuÅŸatma)* | **Logic:** `IF rolling_mean_7 > 50 THEN Predict > 45`<br><br>`rolling_mean` dinamik bir taban Ã§izgisi (*dynamic baseline*) gÃ¶revi gÃ¶rÃ¼r. Trend yukarÄ± yÃ¶nlÃ¼ ise, `lag_1` (dÃ¼nkÃ¼ satÄ±ÅŸ) 0 olsa bile (stok bitmesi vb.), model ortalamaya gÃ¼venerek tahmini yÃ¼ksek tutabilir. |
| **Volatility Awareness**<br>*(Volatilite FarkÄ±ndalÄ±ÄŸÄ±)* | **Logic:** `IF rolling_std_7 > 10 THEN Widen Interval`<br><br>YÃ¼ksek standart sapma, model iÃ§in bir uyarÄ±dÄ±r: "BÃ¼yÃ¼k deÄŸiÅŸimler olabilir." Model, bu durumda daha muhafazakar davranabilir veya tahmin aralÄ±ÄŸÄ±nÄ± geniÅŸletebilir. |

### ğŸ”‘ Key Take-aways


Zaman serisi Ã¶zellik mÃ¼hendisliÄŸinde kullanÄ±lan Ã¼Ã§ temel sÃ¼tun tÃ¼rÃ¼nÃ¼n Ã¶zeti:



* **Lag Columns** (*Gecikme SÃ¼tunlarÄ±*):
    * **Provide Exact Memory** (*Kesin HafÄ±za SaÄŸlar*).
    * **Captures Autocorrelation** (*Otokorelasyonu Yakalar*).
    * *Example:* "Yesterday's price directly affects today's price."
        *(Ã–rnek: "DÃ¼nÃ¼n fiyatÄ± bugÃ¼nÃ¼n fiyatÄ±nÄ± doÄŸrudan etkiler.")*

* **Rolling Columns** (*Kayan SÃ¼tunlar*):
    * **Provide Context** (*BaÄŸlam SaÄŸlar*).
    * **Captures Level** (*Seviye*) and **Volatility** (*OynaklÄ±k*).
    * *Example:* "Is the general trend rising or falling this week?"
        *(Ã–rnek: "Genel trend bu hafta yÃ¼kseliyor mu yoksa dÃ¼ÅŸÃ¼yor mu?")*

* **Calendar Columns** (*Takvim SÃ¼tunlarÄ±*):
    * **Encode Periodic Effects** (*Periyodik Etkileri Kodlar*).
    * Eliminates the need for manual one-hot encoding for every single date.
        *(Her bir tarih iÃ§in manuel one-hot encoding yapma ihtiyacÄ±nÄ± ortadan kaldÄ±rÄ±r.)*
    * *Example:* "Sales always spike on Fridays."
        *(Ã–rnek: "SatÄ±ÅŸlar her zaman Cuma gÃ¼nleri zirve yapar.")*

---

### ğŸ›¡ï¸ Safety First


> **âš ï¸ Avoid Leaking Future Info**
> *(Gelecek Bilgisini SÄ±zdÄ±rmaktan KaÃ§Ä±nÄ±n)*
>
> Always **shift** your data before calculating rolling statistics. If you include "today" in your "average of the last 7 days", the model will cheat by seeing the answer.
> *(Kayan istatistikleri hesaplamadan Ã¶nce verilerinizi her zaman **kaydÄ±rÄ±n**. EÄŸer "son 7 gÃ¼nÃ¼n ortalamasÄ±na" "bugÃ¼nÃ¼" dahil ederseniz, model cevabÄ± gÃ¶rerek hile yapar.)*
>
> **Correct Syntax:** `shift(1).rolling(7)`



# ğŸš€ XGBoost for Time-Series Forecasting
*(Zaman Serisi Tahmini Ä°Ã§in XGBoost)*

 <img width="905" height="443" alt="image" src="https://github.com/user-attachments/assets/572c3f18-add6-4445-913a-59eefed430c1" />

**XGBoost** (*Extreme Gradient Boosting*), son yÄ±llarda veri bilimi dÃ¼nyasÄ±nÄ± domine eden, Ã¶zellikle yapÄ±landÄ±rÄ±lmÄ±ÅŸ/tablosal verilerde (*structured/tabular data*) gÃ¶sterdiÄŸi Ã¼stÃ¼n performansla bilinen gÃ¼Ã§lÃ¼ bir makine Ã¶ÄŸrenimi algoritmasÄ±dÄ±r.

Bu bÃ¶lÃ¼mde, XGBoost'un teknik altyapÄ±sÄ±nÄ± ve zaman serisi tahminciliÄŸinde nasÄ±l bir regresyon aracÄ± olarak kullanÄ±ldÄ±ÄŸÄ±nÄ± inceleyeceÄŸiz.

---

## ğŸ§  What is XGBoost?
*(XGBoost Nedir?)*

XGBoost, **Karar AÄŸaÃ§larÄ±** (*Decision Trees*) temelli bir topluluk Ã¶ÄŸrenme (*Ensemble Learning*) yÃ¶ntemidir. Temel mantÄ±ÄŸÄ± **Gradient Boosting** prensibine dayanÄ±r:
* **Ensemble Strategy:** ZayÄ±f Ã¶ÄŸrenicileri (*weak learners - sÄ±ÄŸ aÄŸaÃ§lar*) bir araya getirerek gÃ¼Ã§lÃ¼ bir tahminci oluÅŸturur.
* **Gradient Descent:** Her yeni aÄŸaÃ§, bir Ã¶nceki aÄŸacÄ±n yaptÄ±ÄŸÄ± hatalarÄ± (*residuals*) tahmin etmek ve dÃ¼zeltmek Ã¼zerine eÄŸitilir.

> **ğŸ’¡ Expert Note:** While Random Forest builds trees independent of each other (bagging), XGBoost builds trees sequentially (boosting), where each tree corrects the errors of the previous one.
> *(Uzman Notu: Rastgele Orman aÄŸaÃ§larÄ± birbirinden baÄŸÄ±msÄ±z kurarken [bagging], XGBoost aÄŸaÃ§larÄ± sÄ±ralÄ± kurar [boosting]; her aÄŸaÃ§ bir Ã¶ncekinin hatasÄ±nÄ± dÃ¼zeltir.)*

---

## ğŸŒŸ Why is XGBoost So Popular?
*(XGBoost Neden Bu Kadar PopÃ¼ler?)*

XGBoost, sadece doÄŸruluÄŸu ile deÄŸil, mÃ¼hendislik harikasÄ± optimizasyonlarÄ± ile de Ã¶ne Ã§Ä±kar.

| Feature (Ã–zellik) | Technical Detail (Teknik Detay) |
| :--- | :--- |
| **Accuracy**<br>*(DoÄŸruluk)* | DÃ¼ÅŸÃ¼k varyans ve dÃ¼ÅŸÃ¼k yanlÄ±lÄ±k (*bias*) dengesini mÃ¼kemmel kurar. Kaggle yarÄ±ÅŸmalarÄ±nÄ±n vazgeÃ§ilmezidir. |
| **Speed & Performance**<br>*(HÄ±z ve Performans)* | **Parallel Processing:** AÄŸaÃ§ oluÅŸturma sÄ±rasÄ±nda Ã¶zellikleri paralel iÅŸler.<br>**Tree Pruning:** AÄŸacÄ± geriye doÄŸru budayarak (*max_depth*) gereksiz dallarÄ± temizler. |
| **Handling Missing Data**<br>*(Eksik Veri YÃ¶netimi)* | **Sparsity-aware Split Finding:** Eksik deÄŸerler iÃ§in "varsayÄ±lan" bir yÃ¶n (*default direction*) Ã¶ÄŸrenir. Ã–n iÅŸleme yapmadan (*imputation*) eksik veriyi yÃ¶netebilir. |
| **Feature Importance**<br>*(Ã–zellik Ã–nemi)* | Veri setindeki hangi Ã¶zelliklerin (Ã¶rn. `lag_7`, `rolling_mean`) tahmine en Ã§ok katkÄ± saÄŸladÄ±ÄŸÄ±nÄ± otomatik olarak hesaplar (`gain`, `weight`, `cover`). |
| **Regularization**<br>*(DÃ¼zenlileÅŸtirme)* | **L1 (Lasso) & L2 (Ridge):** AÅŸÄ±rÄ± Ã¶ÄŸrenmeyi (*overfitting*) engellemek iÃ§in modelin karmaÅŸÄ±klÄ±ÄŸÄ±nÄ± cezalandÄ±ran yerleÅŸik parametrelere sahiptir. |

---

## ğŸ› ï¸ Building XGBoost Model for Demand Forecasting
*(Talep Tahmini Ä°Ã§in XGBoost Modeli Kurma)*

Zaman serisi tahminini bir **Denetimli Ã–ÄŸrenme** (*Supervised Learning*) problemi olarak ele alÄ±yoruz.

### 1. Splitting Data: The Temporal Split
*(Veriyi BÃ¶lme: Zamansal AyrÄ±m)*

Zaman serilerinde rastgele bÃ¶lme (*random shuffle*) **yapÄ±lamaz**. GeleceÄŸi geÃ§miÅŸle tahmin etmeliyiz.

* **Training Set:** GeÃ§miÅŸ veriler (Ã¶rn. 2020-2022).
* **Testing Set:** En gÃ¼ncel veriler (Ã¶rn. 2023).
* **Goal:** Prevent **Data Leakage** (*Hedef: Veri sÄ±zÄ±ntÄ±sÄ±nÄ± Ã¶nlemek*).

### 2. Implementing XGBoost
*(XGBoost UygulamasÄ±)*

Modeli kurarken, Ã¶zellik mÃ¼hendisliÄŸi aÅŸamasÄ±nda Ã¼rettiÄŸimiz `lag` ve `rolling` Ã¶zelliklerini girdi olarak kullanÄ±rÄ±z.

```python
import xgboost as xgb

# Define the model with key hyperparameters
model = xgb.XGBRegressor(
    n_estimators=1000,     # Number of trees (AÄŸaÃ§ sayÄ±sÄ±)
    learning_rate=0.01,    # Step size shrinkage (Ã–ÄŸrenme oranÄ±)
    max_depth=5,           # Depth of trees (AÄŸaÃ§ derinliÄŸi)
    subsample=0.8,         # Row sampling (SatÄ±r Ã¶rnekleme)
    colsample_bytree=0.8,  # Feature sampling (Ã–zellik Ã¶rnekleme)
    objective='reg:squarederror' # Loss function (KayÄ±p fonksiyonu)
)

# Train the model
model.fit(X_train, y_train,
          eval_set=[(X_test, y_test)],
          early_stopping_rounds=50, # Stop if validation score doesn't improve
          verbose=False)
 
```

## 3. PerformansÄ± GÃ¶rselleÅŸtirme: GerÃ§ek ve Tahmin (Visualizing Performance: Actual vs Predicted)

Modeli test seti Ã¼zerinde deÄŸerlendirdiÄŸimizde genellikle ÅŸu davranÄ±ÅŸÄ± gÃ¶rÃ¼rÃ¼z:

* **BaÅŸarÄ± (Success):** Model haftalÄ±k zirve ve dip zamanlamalarÄ±nÄ± oldukÃ§a iyi takip eder; 7 gÃ¼nlÃ¼k ritmi yakalar (The model tracks the timing of weekly peaks and troughs fairly well).
* **KÄ±sÄ±t (Limitation):** Genlikler genellikle sapar. Model uÃ§ deÄŸerleri/sÄ±Ã§ramalarÄ± yumuÅŸatma eÄŸilimindedir (The amplitudes are often off. It tends to smooth out extreme spikes).
* **Sebep (Reason):** AÄŸaÃ§ tabanlÄ± modeller (Tree-based models), eÄŸitimde gÃ¶rdÃ¼klerinin Ã¶tesindeki deÄŸerleri tahmin edemez (cannot extrapolate). Bir yaprak dÃ¼ÄŸÃ¼mÃ¼n ortalamasÄ±nÄ± tahmin ederler (They predict the average of a leaf node).

## 4. DeÄŸerlendirme Metrikleri: MSE ve MAE (Evaluation Metrics: MSE vs MAE)

### Optimizasyon MetriÄŸi - EÄŸitim (Optimization Metric - Training)
`objective='reg:squarederror'`

* MSE/RMSE'yi minimize eder.
* BÃ¼yÃ¼k hatalarÄ±, kareli terim nedeniyle aÄŸÄ±r cezalandÄ±rÄ±r (Penalizes large errors heavily).
* Gradyan Ä°niÅŸi (Gradient Descent) iÃ§in tÃ¼revi alÄ±nabilir (Differentiable).

### Raporlama MetriÄŸi - DeÄŸerlendirme (Reporting Metric - Evaluation)
**MAE (Ortalama Mutlak Hata / Mean Absolute Error)**

* PerformansÄ± paydaÅŸlara (stakeholders) raporlamak iÃ§in kullanÄ±lÄ±r.
* HatayÄ± yorumlanabilir birimlerle ifade eder; Ã¶rneÄŸin, *"Ortalama Â±50 birim sapÄ±yoruz"* (Expresses error in interpretable units).

---

## ğŸ”‘ Temel Ã‡Ä±karÄ±mlar (Key Takeaways)

* **Ã–zellik MÃ¼hendisliÄŸi KraldÄ±r (Feature Engineering is King):** XGBoost "zamanÄ±" gÃ¶remez. Zamansal baÄŸÄ±mlÄ±lÄ±klarÄ± (temporal dependencies) anlamak iÃ§in tamamen oluÅŸturduÄŸumuz gecikme (lag) ve yuvarlanan (rolling) Ã¶zelliklere gÃ¼venir.
* **Esneklik (Flexibility):** DoÄŸrusal olmayan iliÅŸkileri (non-linear relationships) ve etkileÅŸimleri doÄŸrusal (linear) ARIMA modellerinden daha iyi yÃ¶netir.
* **Ekstrapolasyon UyarÄ±sÄ± (Extrapolation Warning):** XGBoost, eÄŸitim verisi aralÄ±ÄŸÄ±nÄ±n Ã§ok Ã¼zerine Ã§Ä±kan veya altÄ±na inen bir trendi tahmin edemez; doÄŸrusal regresyonun aksine (cannot predict a trend that goes significantly higher/lower than the training data range).


# Quiz 4 Solution: XGBoost Fundamentals

AÅŸaÄŸÄ±da XGBoost ile ilgili temel kavramlarÄ± iÃ§eren Quiz 4'Ã¼n Ã§Ã¶zÃ¼mleri ve teknik aÃ§Ä±klamalarÄ± yer almaktadÄ±r.

---

### 1. What does XGBoost stand for?
**(XGBoost neyin kÄ±saltmasÄ±dÄ±r?)**

* [ ] A - Extended Gradient Boosting
* [x] **B - Extreme Gradient Boosting**
* [ ] C - Experimental Gradient Boosting
* [ ] D - Exponential Gradient Boosting

> **Technical Note:** "Extreme" refers to the computational efficiency and engineering goal of pushing the limits of computing resources for boosted tree algorithms.
> *(Teknik Not: "Extreme", artÄ±rÄ±lmÄ±ÅŸ aÄŸaÃ§ algoritmalarÄ± iÃ§in hesaplama kaynaklarÄ±nÄ±n sÄ±nÄ±rlarÄ±nÄ± zorlayan mÃ¼hendislik hedefine ve verimliliÄŸe atÄ±fta bulunur.)*

### 2. What is the primary function of boosting in XGBoost?
**(XGBoost'ta boosting'in birincil iÅŸlevi nedir?)**

* [ ] A - Creating a single deep tree to model the data
* [ ] B - Removing irrelevant features from the dataset
* [x] **C - Correcting errors made by previous trees in the model**
* [ ] D - Optimizing the training process by skipping some data points

> **Technical Note:** Boosting is a sequential ensemble technique where new trees are added to predict and correct the residuals (errors) of prior trees.
> *(Teknik Not: Boosting, yeni aÄŸaÃ§larÄ±n Ã¶nceki aÄŸaÃ§larÄ±n kalÄ±ntÄ±larÄ±nÄ± [hatalarÄ±nÄ±] tahmin etmek ve dÃ¼zeltmek iÃ§in eklendiÄŸi sÄ±ralÄ± bir topluluk tekniÄŸidir.)*

### 3. Why is XGBoost popular for machine learning tasks?
**(XGBoost makine Ã¶ÄŸrenimi gÃ¶revleri iÃ§in neden popÃ¼lerdir?)**

* [ ] A - It automatically performs feature scaling and normalization
* [ ] B - It only works with structured/tabular data
* [ ] C - It is a deep learning algorithm optimized for unstructured data
* [x] **D - It provides accurate predictions, is fast, and handles missing data well**

> **Technical Note:** Its popularity stems from system optimization (speed), regularization (accuracy), and sparsity-aware algorithms (handling missing data natively).
> *(Teknik Not: PopÃ¼laritesi; sistem optimizasyonu [hÄ±z], dÃ¼zenlileÅŸtirme [doÄŸruluk] ve seyrekliÄŸe duyarlÄ± algoritmalarÄ±ndan [kayÄ±p veriyi doÄŸal olarak iÅŸleme] kaynaklanÄ±r.)*

### 4. What type of data is XGBoost best suited for?
**(XGBoost hangi veri tÃ¼rÃ¼ iÃ§in en uygundur?)**

* [ ] A - Image data
* [x] **B - Structured/tabular data**
* [ ] C - Text data
* [ ] D - Time-series data only

> **Technical Note:** Tree-based models excel at splitting heterogeneous features found in tabular data, whereas Deep Learning is better for unstructured data like images.
> *(Teknik Not: AÄŸaÃ§ tabanlÄ± modeller, tablosal verilerdeki heterojen Ã¶zellikleri bÃ¶lmede mÃ¼kemmeldir; Derin Ã–ÄŸrenme ise gÃ¶rÃ¼ntÃ¼ler gibi yapÄ±landÄ±rÄ±lmamÄ±ÅŸ verilerde daha iyidir.)*

### 5. How does XGBoost make its final predictions?
**(XGBoost son tahminlerini nasÄ±l yapar?)**

* [ ] A - By selecting the best-performing single tree
* [ ] B - By averaging the predictions from all trees
* [x] **C - By combining the results of all trees, with each tree correcting the previous one**
* [ ] D - By using only the last tree in the boosting process

> **Technical Note:** It uses an additive strategy where the final prediction is the sum of outputs from all trees ($\sum f_k(x)$).
> *(Teknik Not: Son tahminin tÃ¼m aÄŸaÃ§larÄ±n Ã§Ä±ktÄ±larÄ±nÄ±n toplamÄ± olduÄŸu toplamsal [additive] bir strateji kullanÄ±r.)*

### 6. Which of the following is a key advantage of XGBoost?
**(AÅŸaÄŸÄ±dakilerden hangisi XGBoost'un temel bir avantajÄ±dÄ±r?)**

* [ ] A - It requires no feature engineering to produce accurate results
* [x] **B - It can handle missing data naturally without preprocessing**
* [ ] C - It is only used for classification tasks
* [ ] D - It does not require regularization to control overfitting

> **Technical Note:** XGBoost employs a "Sparsity-aware Split Finding" algorithm that learns the optimal default direction for missing values during training.
> *(Teknik Not: XGBoost, eÄŸitim sÄ±rasÄ±nda kayÄ±p deÄŸerler iÃ§in en uygun varsayÄ±lan yÃ¶nÃ¼ Ã¶ÄŸrenen "SeyrekliÄŸe DuyarlÄ± BÃ¶lme Bulma" algoritmasÄ±nÄ± kullanÄ±r.)*

### 7. Which of the following describes feature importance in XGBoost?
**(AÅŸaÄŸÄ±dakilerden hangisi XGBoost'ta Ã¶zellik Ã¶nemini tanÄ±mlar?)**

* [ ] A - It automatically removes unimportant features from the dataset
* [x] **B - It identifies the most significant features in driving predictions**
* [ ] C - It assigns equal importance to all features in the dataset
* [ ] D - It requires manual calculation by the data scientist

> **Technical Note:** Importance is calculated based on metrics like "Gain" (how much a feature improves the tree's accuracy) or "Cover" (number of samples affected).
> *(Teknik Not: Ã–nem; "KazanÃ§" [bir Ã¶zelliÄŸin aÄŸacÄ±n doÄŸruluÄŸunu ne kadar artÄ±rdÄ±ÄŸÄ±] veya "Kapsama" [etkilenen Ã¶rnek sayÄ±sÄ±] gibi metriklere gÃ¶re hesaplanÄ±r.)*



# ğŸ§  Introduction to Deep Learning: A Time-Series Perspective


Derin Ã–ÄŸrenme (Deep Learning - DL), veriden Ã¶ÄŸrenmek ve modellemek iÃ§in yapay sinir aÄŸlarÄ±nÄ± (Artificial Neural Networks - ANNs) kullanan Makine Ã–ÄŸreniminin (Machine Learning - ML) geliÅŸmiÅŸ bir alt alanÄ±dÄ±r. Bu aÄŸlar, karmaÅŸÄ±k desenleri tanÄ±mlamada ve bu verilere dayanarak karar vermede mÃ¼kemmeldir.

Bir Zaman Serisi (Time-Series) uzmanÄ± gÃ¶zÃ¼yle baktÄ±ÄŸÄ±mÄ±zda DL, geleneksel istatistiksel yÃ¶ntemlerin (ARIMA, Exponential Smoothing) tÄ±kandÄ±ÄŸÄ± noktalarda devreye girer. Ã–zellikle ham veriden Ã¶nemli Ã¶zellikleri (features) otomatik olarak Ã§Ä±karma yeteneÄŸi; zaman serileri, gÃ¶rÃ¼ntÃ¼ tanÄ±ma, dil iÅŸleme gibi alanlarda devrim yaratmÄ±ÅŸtÄ±r.

---

## ğŸ—ï¸ (Deep) Neural Networks: The Architecture
**((Derin) Sinir AÄŸlarÄ±: Mimari)**

Tipik bir sinir aÄŸÄ± (Neural Network - NN), veriyi iÅŸlemek ve Ã¶ÄŸrenmek iÃ§in birlikte Ã§alÄ±ÅŸan, nÃ¶ron (neuron) adÄ± verilen birbirine baÄŸlÄ± katmanlardan oluÅŸur. Bu yapÄ±, insan beyninin Ã§alÄ±ÅŸma prensibinden esinlenmiÅŸtir ancak matematiksel bir optimizasyon makinesidir.


 <img width="914" height="430" alt="image" src="https://github.com/user-attachments/assets/1826729f-6ed9-49b8-a6e5-c02c3d9bcae7" />


### 1. Input Layer (Girdi KatmanÄ±)
Modelin dÃ¼nyaya aÃ§Ä±lan kapÄ±sÄ±dÄ±r. Ham Ã¶zellikler modele buradan girer.
* **Genel:** Pikseller, kelimeler, sensÃ¶r okumalarÄ±.
* **Time-Series Ã–zel:** Gecikmeli deÄŸerler (lags), kayan pencere istatistikleri (rolling stats), takvim Ã¶zellikleri (calendar features) veya ham sÄ±ralÄ± veriler (t, t-1, t-2...).

### 2. Hidden Layers (Gizli Katmanlar â‰¥ 1)
BurasÄ± "sihrin" gerÃ§ekleÅŸtiÄŸi yerdir. Girdilerin aÄŸÄ±rlÄ±klandÄ±rÄ±lÄ±p (weighted) iÅŸlendiÄŸi katmanlardÄ±r.
* EÄŸer birden fazla gizli katman varsa, aÄŸa **Derin Sinir AÄŸÄ± (Deep Neural Network - DNN)** denir.
* **Aktivasyon FonksiyonlarÄ± (Activation Functions):** Her nÃ¶ronun Ã§Ä±ktÄ±sÄ±, `ReLU`, `Sigmoid` veya `Tanh` gibi doÄŸrusal olmayan (non-linear) fonksiyonlardan geÃ§irilir. Bu, aÄŸÄ±n karmaÅŸÄ±k, eÄŸrisel ve Ã§ok boyutlu iliÅŸkileri Ã¶ÄŸrenmesini saÄŸlar.
* *Zaman serilerinde bu katmanlar genellikle LSTM veya GRU hÃ¼creleri ya da 1D-CNN filtreleri iÃ§erir.*

### 3. Output Layer (Ã‡Ä±ktÄ± KatmanÄ±)
Son aktivasyonlar tahminlere dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼r.
* **Regresyon (Time-Series):** Genellikle tek bir nÃ¶ron (gelecekteki satÄ±ÅŸ miktarÄ±, sÄ±caklÄ±k vb.) ve `Linear` aktivasyon.
* **SÄ±nÄ±flandÄ±rma:** OlasÄ±lÄ±klar (Softmax) veya sÄ±nÄ±flar.

> ğŸ“Œ **Technical Insight:** Sinir aÄŸÄ±nÄ±n katmanlarÄ±, girdi verisini bir dizi doÄŸrusal olmayan dÃ¶nÃ¼ÅŸÃ¼m (nonlinear transformations) yoluyla iÅŸler. Bu, aÄŸÄ±n "Evrensel YaklaÅŸÄ±klÄ±k Teoremi" (Universal Approximation Theorem) sayesinde teorik olarak herhangi bir fonksiyonu Ã¶ÄŸrenebilmesine olanak tanÄ±r.

---

## âš™ï¸ How Deep Neural Networks Learn
**(Derin Sinir AÄŸlarÄ± NasÄ±l Ã–ÄŸrenir?)**

Bir derin Ã¶ÄŸrenme modelinin faydalÄ± olabilmesi iÃ§in Ã¶nce eÄŸitilmesi (training) gerekir. Bu sÃ¼reÃ§, milyonlarca parametrenin (aÄŸÄ±rlÄ±klar ve sapmalar) optimize edildiÄŸi iteratif bir dÃ¶ngÃ¼dÃ¼r.

### 1. Forward Propagation (Ä°leri YayÄ±lÄ±m)
Veri, girdi katmanÄ±ndan Ã§Ä±ktÄ± katmanÄ±na doÄŸru akar. Her katman veriyi iÅŸler (matris Ã§arpÄ±mÄ± + aktivasyon) ve bir sonrakine iletir. BaÅŸlangÄ±Ã§ta aÄŸÄ±rlÄ±klar rastgeledir, bu yÃ¼zden ilk tahminler tamamen yanlÄ±ÅŸtÄ±r.

### 2. Error Calculation: Loss Function (Hata Hesaplama: KayÄ±p Fonksiyonu)
AÄŸ bir tahminde bulunduÄŸunda, model bu tahminin gerÃ§ek deÄŸerden (Ground Truth) ne kadar uzak olduÄŸunu hesaplar.
* **Time-Series iÃ§in:** Genellikle `MSE` (Mean Squared Error), `MAE` (Mean Absolute Error) veya olasÄ±lÄ±ksal tahminler iÃ§in `Quantile Loss` kullanÄ±lÄ±r.
* **Classification iÃ§in:** `Cross-Entropy` yaygÄ±ndÄ±r.

### 3. Back-propagation (Geri YayÄ±lÄ±m)
Bu adÄ±m, Ã¶ÄŸrenmenin kalbidir. KayÄ±p (Loss) hesaplandÄ±ktan sonra, aÄŸ hatalarÄ± azaltmak iÃ§in aÄŸÄ±rlÄ±klarÄ± (weights) nasÄ±l ayarlamasÄ± gerektiÄŸini matematiksel olarak hesaplar.
* Hata, Ã§Ä±ktÄ±dan girdiye doÄŸru geriye yayÄ±lÄ±r.
* Her bir aÄŸÄ±rlÄ±ÄŸÄ±n hataya ne kadar katkÄ±da bulunduÄŸu (kÄ±smi tÃ¼revler/gradyanlar) zincir kuralÄ± (chain rule) ile hesaplanÄ±r.



### 4. Gradient Descent & Optimization (Gradyan Ä°niÅŸi ve Optimizasyon)
Model, aÄŸÄ±rlÄ±klarÄ± gÃ¼ncellemek iÃ§in optimizasyon algoritmalarÄ± kullanÄ±r.
* AmaÃ§, kayÄ±p fonksiyonunun en dik iniÅŸ yÃ¶nÃ¼nÃ¼ (negatif gradyan) takip ederek global minimuma ulaÅŸmaktÄ±r.
* **Algorithm:** Klasik `SGD` (Stochastic Gradient Descent) yerine, gÃ¼nÃ¼mÃ¼zde genellikle adaptif Ã¶ÄŸrenme oranÄ±na sahip `Adam` (Adaptive Moment Estimation) optimizer tercih edilir.

> ğŸ“Œ **Training Note:** EÄŸitim genellikle bÃ¼yÃ¼k veri setleri ve birÃ§ok iterasyon (veya **epochs**) gerektirir. AÅŸÄ±rÄ± Ã¶ÄŸrenmeyi (Overfitting) Ã¶nlemek iÃ§in `Dropout` veya `Early Stopping` gibi teknikler de bu sÃ¼rece dahil edilir.

---

## ğŸ› ï¸ Common Deep Learning Frameworks
**(YaygÄ±n Derin Ã–ÄŸrenme Ã‡atÄ±larÄ±)**

Derin Ã¶ÄŸrenmeye baÅŸlamak, gÃ¼Ã§lÃ¼ kÃ¼tÃ¼phaneler sayesinde artÄ±k Ã§ok daha kolaydÄ±r. Bu Ã§erÃ§eveler, arkadaki karmaÅŸÄ±k tÃ¼rev ve matris iÅŸlemlerini (autograd) otomatik halleder.

| Framework | AÃ§Ä±klama (Description) | KullanÄ±m AlanÄ± (Use Case) |
| :--- | :--- | :--- |
| **TensorFlow (Keras)** | Google tarafÄ±ndan geliÅŸtirildi. Hem araÅŸtÄ±rma hem de Ã¼retim (production) ortamlarÄ±nda gÃ¼Ã§lÃ¼dÃ¼r. `Keras` API'si ile Ã§ok hÄ±zlÄ± prototip Ã¼retilir. | EndÃ¼striyel daÄŸÄ±tÄ±m, Mobil (TF Lite). |
| **PyTorch** | Meta (Facebook) tarafÄ±ndan geliÅŸtirildi. EsnekliÄŸi ve kullanÄ±m kolaylÄ±ÄŸÄ± ile bilinir. Dinamik hesaplama grafikleri (dynamic computation graphs) hata ayÄ±klamayÄ± kolaylaÅŸtÄ±rÄ±r. | Akademik araÅŸtÄ±rma, Modern Time-Series kÃ¼tÃ¼phaneleri (PyTorch Forecasting, Darts). |

> **Expert Opinion:** GeÃ§miÅŸte TensorFlow daha yaygÄ±ndÄ±, ancak modern araÅŸtÄ±rmalarda ve Ã¶zellikle zaman serisi iÃ§in geliÅŸtirilen yeni mimarilerde (Transformer tabanlÄ± modeller) **PyTorch** fiili standart haline gelmiÅŸtir. "Under the hood" (kaputun altÄ±nda) Ã§alÄ±ÅŸan birÃ§ok kÃ¼tÃ¼phane PyTorch kullanÄ±r.

---

## ğŸš€ Key Benefits of Deep Learning in Time-Series


Geleneksel makine Ã¶ÄŸrenimine ve klasik istatistiÄŸe (ARIMA vb.) kÄ±yasla DL'in Ã¶ne Ã§Ä±ktÄ±ÄŸÄ± noktalar:

1.  **Automated Feature Extraction (Otomatik Ã–zellik Ã‡Ä±karÄ±mÄ±):**
    * Geleneksel yÃ¶ntemlerde trendi, mevsimselliÄŸi ve dÃ¶ngÃ¼leri elle ayrÄ±ÅŸtÄ±rmanÄ±z gerekir. DL (Ã¶zellikle CNN ve RNN'ler), ham veriden bu kalÄ±plarÄ± otomatik olarak Ã¶ÄŸrenir.
2.  **Handling Complex & High-Dimensional Data (KarmaÅŸÄ±k ve Ã‡ok Boyutlu Veri YÃ¶netimi):**
    * DL, yapÄ±landÄ±rÄ±lmamÄ±ÅŸ verilerle (gÃ¶rÃ¼ntÃ¼, metin) Ã§alÄ±ÅŸabildiÄŸi gibi, zaman serilerinde **Global Modeller** (Global Models) oluÅŸturabilir. Yani, 1000 farklÄ± Ã¼rÃ¼nÃ¼n satÄ±ÅŸ verisini tek bir modelde eÄŸiterek, Ã¼rÃ¼nler arasÄ± iliÅŸkileri (cross-learning) Ã¶ÄŸrenebilir.
3.  **Non-Linearity & Generalization (DoÄŸrusallÄ±k DÄ±ÅŸÄ± ve GenelleÅŸtirme):**
    * Zaman serileri nadiren doÄŸrusaldÄ±r. DL, karmaÅŸÄ±k, kaotik ve doÄŸrusal olmayan iliÅŸkileri modellemede ve gÃ¶rÃ¼lmemiÅŸ verilere genellemede (generalization) Ã¼stÃ¼ndÃ¼r.

---

## âš ï¸ Challenges in Deep Learning


Pratikte DL kullanmak bazÄ± engelleri aÅŸmayÄ± gerektirir:

* **Data Requirements (Veri Gereksinimleri):** DL modelleri "veri aÃ§lÄ±ÄŸÄ±" Ã§eker. YÃ¼ksek performans iÃ§in genellikle bÃ¼yÃ¼k miktarda etiketli geÃ§miÅŸ veriye ihtiyaÃ§ duyarlar. Az veriyle (Small Data) klasik yÃ¶ntemler bazen daha iyi Ã§alÄ±ÅŸabilir.
* **Computational Resources (Hesaplama KaynaklarÄ±):** Derin aÄŸlarÄ± eÄŸitmek iÅŸlemci gÃ¼cÃ¼ ister. GPU'lar (Graphics Processing Units) veya TPU'lar olmadan bÃ¼yÃ¼k modelleri eÄŸitmek gÃ¼nler sÃ¼rebilir.
* **Interpretability (Yorumlanabilirlik):** Derin aÄŸlar, Ã¶zellikle Ã§ok katmanlÄ± yapÄ±lar, genellikle "Kara Kutu" (Black Box) olarak adlandÄ±rÄ±lÄ±r. Bir tahminin *neden* yapÄ±ldÄ±ÄŸÄ±nÄ± anlamak (Feature Importance), karar aÄŸaÃ§larÄ±na gÃ¶re daha zordur. Finans veya saÄŸlÄ±k gibi alanlarda bu bir risk faktÃ¶rÃ¼dÃ¼r (gerÃ§i `TFT - Temporal Fusion Transformer` gibi modern mimariler bunu Ã§Ã¶zmeye odaklanmaktadÄ±r).

---

## ğŸ Conclusion


Derin Ã–ÄŸrenme, makine Ã¶ÄŸrenimi gÃ¶revlerine yaklaÅŸÄ±mÄ±mÄ±zÄ± kÃ¶kten deÄŸiÅŸtirdi. BÃ¼yÃ¼k veri setleriyle baÅŸa Ã§Ä±kma ve ham veriden anlamlÄ± desenler Ã§Ä±karma yeteneÄŸi, onu modern veri biliminin en gÃ¼Ã§lÃ¼ aracÄ± yapar.

Sinir aÄŸlarÄ±nÄ± kullanarak, DL modelleri karmaÅŸÄ±k temsilleri otomatik olarak Ã¶ÄŸrenir. Bu derste/kapsamda, genel DL mimarilerinin Ã¶tesine geÃ§ip, Zaman Serisi kullanÄ±m durumlarÄ±mÄ±z (Time Series Use Cases) iÃ§in Ã¶zelleÅŸmiÅŸ mimarilere odaklanacaÄŸÄ±z:
* **RNNs (Recurrent Neural Networks - LSTM/GRU):** SÄ±ralÄ± baÄŸÄ±mlÄ±lÄ±klarÄ± hatÄ±rlamak iÃ§in.
* **1D-CNNs:** Zaman iÃ§indeki yerel desenleri yakalamak iÃ§in.
* **Transformers:** Dikkat mekanizmasÄ± (Attention) ile uzun vadeli iliÅŸkileri modellemek iÃ§in.
 



# ğŸ”„ Recurrent Neural Networks (RNNs) for Time-Series
**(Zaman Serileri iÃ§in Tekrarlayan Sinir AÄŸlarÄ±)**

Zaman serisi tahmini (Time-Series Forecasting) iÃ§in kullanÄ±lan Derin Ã–ÄŸrenme mimarileri arasÄ±nda en temel ve yaygÄ±n bilinen iki yapÄ± **Recurrent Neural Networks (RNNs)** ve onlarÄ±n geliÅŸmiÅŸ versiyonu olan **Long Short-Term Memory Networks (LSTMs)**'dir.

Bu bÃ¶lÃ¼mde, modern sÄ±ralÄ± modellemenin (sequential modeling) atasÄ± olan RNN'lerin teknik altyapÄ±sÄ±nÄ± ve sÄ±nÄ±rlamalarÄ±nÄ± inceleyeceÄŸiz.

---

## ğŸ§  Recurrent Neural Networks (RNNs)

**Recurrent Neural Networks (RNNs)**, Ã¶nceki girdilerin bir "hafÄ±zasÄ±nÄ±" (memory) koruyarak sÄ±ralÄ± verileri (sequential data) iÅŸlemek Ã¼zere tasarlanmÄ±ÅŸ Ã¶zel bir sinir aÄŸÄ± sÄ±nÄ±fÄ±dÄ±r.

<img width="661" height="310" alt="image" src="https://github.com/user-attachments/assets/3033e0fc-264f-46b2-ba4f-9b7ebfe3f977" />

Bu hafÄ±za yeteneÄŸi, bir zaman adÄ±mÄ±ndaki (time step) tahminin, Ã¶nceki zaman adÄ±mlarÄ±ndaki verilere baÄŸlÄ± olduÄŸu zaman serisi tahmini gibi gÃ¶revler iÃ§in onlarÄ± ideal kÄ±lar. Geleneksel Ä°leri Beslemeli (Feed-Forward) aÄŸlarÄ±n aksine, RNN'ler zamanÄ± bir boyut olarak kabul eder.

### Geleneksel AÄŸlardan FarkÄ± (The Difference)

* **Traditional Neural Networks (Feed-Forward):** Girdiler birbirinden baÄŸÄ±msÄ±z kabul edilir (inputs are treated independently). Ã–rneÄŸin, bir kedi fotoÄŸrafÄ±nÄ± tanÄ±yan model, bir Ã¶nceki fotoÄŸrafta ne gÃ¶rdÃ¼ÄŸÃ¼nÃ¼ hatÄ±rlamaz. Veri akÄ±ÅŸÄ± tek yÃ¶nlÃ¼dÃ¼r: Girdi -> Gizli Katman -> Ã‡Ä±ktÄ±.
* **Recurrent Neural Networks (RNNs):** Ã–nceki zaman adÄ±mlarÄ±ndan gelen Ã§Ä±ktÄ±yÄ± (output), mevcut zaman adÄ±mÄ± iÃ§in girdinin bir parÃ§asÄ± olarak kullanÄ±r. Bu, RNN'lerin zaman iÃ§indeki kalÄ±plarÄ± (patterns over time) yakalamasÄ±nÄ± saÄŸlar.

> ğŸ“Œ **Expert Note:** RNN'leri, kendi Ã§Ä±ktÄ±sÄ±nÄ± bir sonraki adÄ±mda kendine girdi olarak veren "dÃ¶ngÃ¼sel" (looping) bir yapÄ± olarak dÃ¼ÅŸÃ¼nebilirsiniz. Bu yapÄ± "aÃ§Ä±ldÄ±ÄŸÄ±nda" (unfolded), her zaman adÄ±mÄ± iÃ§in birbirinin kopyasÄ± olan bir aÄŸ zinciri ortaya Ã§Ä±kar.



---

## âš™ï¸ How RNNs Work: The "Hidden State"
**(RNN'ler NasÄ±l Ã‡alÄ±ÅŸÄ±r: "Gizli Durum")**

Bir RNN'deki her dÃ¼ÄŸÃ¼m (node/neuron) sadece mevcut girdiyi iÅŸlemekle kalmaz, aynÄ± zamanda aÄŸÄ±n Ã¶nceki durumunu da hatÄ±rlar. Bu hafÄ±za mekanizmasÄ±na **Gizli Durum (Hidden State)** denir.

Matematiksel olarak sÃ¼reÃ§ ÅŸu ÅŸekilde iÅŸler:

1.  **Input ($x_t$):** $t$ zamanÄ±ndaki veri.
2.  **Previous Hidden State ($h_{t-1}$):** AÄŸÄ±n $t-1$ anÄ±ndaki hafÄ±zasÄ±.
3.  **Current Hidden State ($h_t$):** AÄŸ, mevcut girdiyi ve eski hafÄ±zayÄ± birleÅŸtirerek yeni bir hafÄ±za durumu oluÅŸturur.
    * FormÃ¼l: $h_t = \tanh(W_h \cdot h_{t-1} + W_x \cdot x_t)$
4.  **Output ($y_t$):** Yeni gizli durum kullanÄ±larak o anki tahmin yapÄ±lÄ±r.

Bu mekanizma, RNN'lerin tarihsel verilere (historical data) dayalÄ± tahminler yapmasÄ±nÄ± saÄŸlar ve onlarÄ± zaman serisi gÃ¶revleri iÃ§in doÄŸal bir seÃ§im haline getirir.

### Comparison: FFN vs RNN
**(KarÅŸÄ±laÅŸtÄ±rma: Ä°leri Beslemeli vs Tekrarlayan AÄŸlar)**

* **(a) Fully-Connected (Dense) Networks:** Her girdi baÄŸÄ±msÄ±zdÄ±r. Zaman kavramÄ± yoktur. $x \to y$
* **(b) Recurrent Networks:** Girdiler sÄ±ralÄ±dÄ±r. Åimdiki karar, geÃ§miÅŸe baÄŸlÄ±dÄ±r. $x_{t}, h_{t-1} \to y_{t}$

---

## âš ï¸ The Limitation: Vanishing Gradient Problem
**(KÄ±sÄ±t: Kaybolan Gradyan Problemi)**

Teoride RNN'ler, sonsuz geÃ§miÅŸe bakabilir. Ancak pratikte, temel RNN'lerin (Vanilla RNNs) Ã§ok ciddi bir sÄ±nÄ±rlamasÄ± vardÄ±r: **Vanishing Gradient Problem (Kaybolan Gradyan Problemi)**.

### Bu Problem Nedir?
AÄŸÄ± eÄŸitirken **Zaman Ä°Ã§inde Geri YayÄ±lÄ±m (Backpropagation Through Time - BPTT)** algoritmasÄ±nÄ± kullanÄ±rÄ±z. Hata (loss), zamandan geriye doÄŸru (bugÃ¼nden geÃ§miÅŸe) yayÄ±lÄ±rken aÄŸÄ±rlÄ±klar (weights) gÃ¼ncellenir.

* EÄŸer aÄŸÄ±rlÄ±klar kÃ¼Ã§Ã¼kse (< 1), hata geriye doÄŸru her adÄ±mda Ã§arpÄ±larak kÃ¼Ã§Ã¼lÃ¼r.
* Zincirleme Ã§arpÄ±m sonucu (Ã¶rn. $0.9 \times 0.9 \times 0.9 \dots$), gradyanlar hÄ±zla sÄ±fÄ±ra yaklaÅŸÄ±r.
* **SonuÃ§:** AÄŸ, serinin baÅŸÄ±ndaki (uzak geÃ§miÅŸteki) verileri Ã¶ÄŸrenemez. AÄŸÄ±rlÄ±klar gÃ¼ncellenemediÄŸi iÃ§in aÄŸÄ±n "hafÄ±zasÄ±" kÄ±salÄ±r.

> **Impact:** AÄŸ sadece yakÄ±n geÃ§miÅŸe (short-term memory) odaklanÄ±r, uzun vadeli baÄŸÄ±mlÄ±lÄ±klarÄ± (long-term dependencies) Ã¶ÄŸrenemez. Ã–rneÄŸin, geÃ§en yÄ±lki bir trendin bugÃ¼nkÃ¼ satÄ±ÅŸÄ± etkilediÄŸini RNN ile modellemek Ã§ok zordur.

---

## â­ï¸ Why We Move to LSTMs
**(Neden LSTM'lere GeÃ§iyoruz?)**

Temel RNN'lerin uzun vadeli bilgiyi "unutma" eÄŸilimi, karmaÅŸÄ±k zaman serileri iÃ§in yetersiz kalmalarÄ±na neden olur. Ä°ÅŸte bu yÃ¼zden, RNN'ler Ã¼zerine uzun bir sohbeti atlayÄ±p, doÄŸrudan bu problemin Ã§Ã¶zÃ¼mÃ¼ olan **LSTM (Long Short-Term Memory)** ve **GRU (Gated Recurrent Unit)** aÄŸlarÄ±na geÃ§iyoruz!

LSTM'ler, iÃ§erdikleri Ã¶zel "kapÄ±" (gate) mekanizmalarÄ± sayesinde hangi bilginin saklanacaÄŸÄ±nÄ± ve hangisinin unutulacaÄŸÄ±nÄ± seÃ§erek kaybolan gradyan problemini Ã§Ã¶zerler.



# ğŸ§  Long Short-Term Memory Networks (LSTMs) for Time-Series
**(Zaman Serileri iÃ§in Uzun KÄ±sa-Vadeli HafÄ±za AÄŸlarÄ±)**

 <img width="727" height="391" alt="image" src="https://github.com/user-attachments/assets/87415182-6893-444f-abce-092e982be577" />

Zaman serisi tahminciliÄŸinde (Time-Series Forecasting) "altÄ±n standart" olarak kabul edilen mimarilerden biri **LSTM (Long Short-Term Memory)** aÄŸlarÄ±dÄ±r.

LSTM'ler, standart RNN'lerin (Recurrent Neural Networks) en bÃ¼yÃ¼k zaafÄ± olan **Kaybolan Gradyan Problemini (Vanishing Gradient Problem)** Ã§Ã¶zmek iÃ§in tasarlanmÄ±ÅŸ Ã¶zelleÅŸmiÅŸ bir mimaridir. Standart RNN'ler zaman adÄ±mlarÄ± arttÄ±kÃ§a geÃ§miÅŸi unuturken, LSTM'ler **KapÄ±lar (Gates)** adÄ± verilen mekanizmalar sayesinde bilginin akÄ±ÅŸÄ±nÄ± kontrol eder. Bu sayede aÄŸ, uzun diziler boyunca hangi bilginin saklanacaÄŸÄ±nÄ±, hangisinin unutulacaÄŸÄ±nÄ± ve hangisinin bir sonraki adÄ±ma aktarÄ±lacaÄŸÄ±nÄ± "Ã¶ÄŸrenir".



---

## ğŸš€ Key Benefits for Time-Series
**(Zaman Serileri Ä°Ã§in Temel Faydalar)**

Neden klasik yÃ¶ntemler veya basit RNN'ler yerine LSTM kullanmalÄ±yÄ±z?

1.  **Handling Long-Term Dependencies (Uzun Vadeli BaÄŸÄ±mlÄ±lÄ±klarÄ± YÃ¶netme):**
    * Tahminlerin Ã§ok eski verilere dayandÄ±ÄŸÄ± durumlar iÃ§in idealdir.
    * *Ã–rnek:* Bir perakendeci iÃ§in bugÃ¼nkÃ¼ satÄ±ÅŸlar, 12 ay Ã¶nceki "yÄ±llÄ±k mevsimsellikten" (seasonal effects) etkilenebilir. LSTM bu bilgiyi taÅŸÄ±yabilir.
2.  **Complex & Non-Linear Patterns (KarmaÅŸÄ±k ve DoÄŸrusal Olmayan Desenler):**
    * Verilerdeki karmaÅŸÄ±k, Ã§ok adÄ±mlÄ± ve doÄŸrusal olmayan kalÄ±plarÄ± yakalar.
    * Geleneksel modellerin (ARIMA) veya basit RNN'lerin modellemekte zorlandÄ±ÄŸÄ± ani deÄŸiÅŸimleri (shocks) ve rejim deÄŸiÅŸikliklerini yÃ¶netebilir.
3.  **Variable Length Sequences (DeÄŸiÅŸken Uzunluklu Diziler):**
    * Sabit uzunluklu girdilere sÄ±kÄ±ÅŸÄ±p kalmaz, farklÄ± uzunluktaki tarihsel verileri iÅŸleyebilir.

> ğŸ“Š **Retail Forecasting Context:** LSTM aÄŸlarÄ± perakende tahminciliÄŸi iÃ§in Ã¶zellikle etkilidir Ã§Ã¼nkÃ¼ geÃ§miÅŸ verilerden bilgiyi saklayarak hem kÄ±sa vadeli dalgalanmalarÄ± (haftalÄ±k dÃ¶ngÃ¼ler) hem de uzun vadeli trendleri (yÄ±llÄ±k bÃ¼yÃ¼me) aynÄ± anda yakalayabilirler. Promosyonlar, tatiller ve mevsimsellik gibi faktÃ¶rlerin satÄ±ÅŸ Ã¼zerindeki etkisini Ã¶ÄŸrenirler.

---

## âš™ï¸ Key Components of LSTMs: The "Mini-Factory"
**(LSTM'in Temel BileÅŸenleri: "Mini Fabrika")**

Bir LSTM katmanÄ±nÄ±, her zaman adÄ±mÄ±nda (time step) neyi saklayacaÄŸÄ±na, neyi Ã§Ã¶pe atacaÄŸÄ±na ve bir sonraki adÄ±ma neyi aktaracaÄŸÄ±na karar veren bir "mini fabrika" gibi dÃ¼ÅŸÃ¼nebilirsiniz.

Bu sÃ¼reci Ã¼Ã§ kÃ¼Ã§Ã¼k karar verici (**Gates/KapÄ±lar**) ve **Cell State (HÃ¼cre Durumu)** adÄ± verilen uzun vadeli bir taÅŸÄ±ma bandÄ± yÃ¶netir.



### 1. Forget Gate â€“ â€œWhat can I safely ignore?â€
**(Unutma KapÄ±sÄ± â€“ "Neyi gÃ¼venle gÃ¶rmezden gelebilirim?")**

* **Job (GÃ¶revi):** Ã–nceki zaman adÄ±mlarÄ±ndan gelen hangi bilginin artÄ±k gereksiz olduÄŸuna karar verir ve onu siler.
* **How (NasÄ±l):** Son gizli duruma ($h_{t-1}$) ve mevcut girdiye ($x_t$) bakar. Sigmoid fonksiyonu kullanarak 0 ile 1 arasÄ±nda bir sayÄ± Ã¼retir.
    * `0` â†’ Kesinlikle unut (throw it away).
    * `1` â†’ Kesinlikle sakla (keep it).
* **Retail Example:** Model, Åubat ayÄ± satÄ±ÅŸlarÄ±nÄ± tahmin ederken geÃ§en yÄ±lki "Black Friday" (Efsane Cuma) sÄ±Ã§ramasÄ±nÄ±n artÄ±k bir gÃ¼rÃ¼ltÃ¼ (noise) olduÄŸuna karar verir. Bu bilgiye `0`'a yakÄ±n bir deÄŸer atar ve hafÄ±zadan siler.

### 2. Input Gate â€“ â€œWhat new info is worth storing?â€
**(Girdi KapÄ±sÄ± â€“ "Hangi yeni bilgi saklamaya deÄŸer?")**

* **Job (GÃ¶revi):** Hangi yeni bilginin aÄŸÄ±n hafÄ±zasÄ±na (Cell State) ekleneceÄŸini belirler.
* **How (NasÄ±l):** Ä°ki aÅŸamalÄ± Ã§alÄ±ÅŸÄ±r:
    1.  **Sigmoid Filtresi (SarÄ±):** Hangi deÄŸerlerin gÃ¼ncelleneceÄŸine karar verir (0-1 arasÄ± Ã¶nem derecesi).
    2.  **Tanh Aday KatmanÄ± (Pembe):** HafÄ±zaya eklenebilecek yeni deÄŸer vektÃ¶rÃ¼nÃ¼ (adaylarÄ±) oluÅŸturur (-1 ile 1 arasÄ±).
    * Bu ikisinin Ã§arpÄ±mÄ± hafÄ±zaya eklenir.
* **Retail Example:** Model ani bir "3 GÃ¼nlÃ¼k Ä°ndirim" kampanyasÄ± gÃ¶rÃ¼r. Bunun Ã¶nemli olduÄŸuna karar verir (Sigmoid $\approx$ 1) ve kampanya etkisini taÅŸÄ±ma bandÄ±na (belt) yazar.

### 3. Cell State â€“ â€œLong-term memory laneâ€
**(HÃ¼cre Durumu â€“ "Uzun vadeli hafÄ±za ÅŸeridi")**

* **Job (GÃ¶revi):** LSTM'in "gerÃ§ek" hafÄ±zasÄ±dÄ±r. Bilgiyi Ã§ok uzun sÃ¼reler boyunca bozulmadan taÅŸÄ±masÄ±nÄ± saÄŸlar.
* **How (NasÄ±l):** HÃ¼crenin Ã¼zerinden dÃ¼mdÃ¼z akan bir taÅŸÄ±ma bandÄ± (conveyor belt) gibidir. Sadece "Unutma" ve "Ekleme" adÄ±mlarÄ±yla Ã¼zerinde kÃ¼Ã§Ã¼k deÄŸiÅŸiklikler yapÄ±lÄ±r. Matematiksel iÅŸlemler lineer olduÄŸu iÃ§in (Ã§arpma yerine toplama aÄŸÄ±rlÄ±klÄ±), gradyanlar kaybolmadan geriye akabilir.
* **In Retail:** Mevsimsellik bilgisini veya genel trendi (trend), yÃ¼zlerce gÃ¼n boyunca solmadan (without fading) taÅŸÄ±yan yapÄ±dÄ±r.

### 4. Output Gate â€“ â€œWhat should I reveal right now?â€
**(Ã‡Ä±ktÄ± KapÄ±sÄ± â€“ "Åu an neyi aÃ§Ä±ÄŸa Ã§Ä±karmalÄ±yÄ±m?")**

* **Job (GÃ¶revi):** Mevcut hafÄ±zaya dayanarak, ÅŸu anki zaman adÄ±mÄ±nda (t) ne Ã§Ä±ktÄ± verileceÄŸini seÃ§er.
* **How (NasÄ±l):** GÃ¼ncellenmiÅŸ hÃ¼cre durumunu (Cell State) alÄ±r, bir `tanh` iÅŸleminden geÃ§irir ve bunu yeni bir `sigmoid` filtresiyle Ã§arparak bugÃ¼nÃ¼n gizli Ã§Ä±ktÄ±sÄ±nÄ± ($h_t$) oluÅŸturur. Bu Ã§Ä±ktÄ±:
    1.  Bir sonraki LSTM Ã¼nitesine ($t+1$) gider.
    2.  GerÃ§ek tahmini yapan yoÄŸun katmana (Dense Layer) gider.

#### ğŸª Retail Scenario: Output Gate Logic
**(Perakende Senaryosu: Ã‡Ä±ktÄ± KapÄ±sÄ± MantÄ±ÄŸÄ±)**

Bir LSTM'in hafÄ±zasÄ±nda (Cell State) halihazÄ±rda mevsimsellik ("Hafta sonlarÄ± Ã§ok satar") ve promosyon ("%20 kupon satÄ±ÅŸÄ± artÄ±rÄ±r") bilgisinin saklÄ± olduÄŸunu hayal edin. Takvim **15 Mart SalÄ±**'yÄ± gÃ¶sterdiÄŸinde, Ã‡Ä±ktÄ± KapÄ±sÄ± hafÄ±za vektÃ¶rÃ¼nÃ¼n her parÃ§asÄ± iÃ§in ÅŸu sorularÄ± sorar:

| Memory Component <br> (HafÄ±za BileÅŸeni) | Forget Gate Decision <br> (Unutma KapÄ±sÄ± Durumu) | Output Gate Decision (Today) <br> (Ã‡Ä±ktÄ± KapÄ±sÄ± KararÄ± - BugÃ¼n) | Why? <br> (Neden?) |
| :--- | :--- | :--- | :--- |
| **Weekend Boost** <br> *(Hafta Sonu Etkisi)* | Kept at 100% <br> *(Cuma geliyor, sakla)* | **0.1 â†’ Reveal only 10%** <br> *(Sadece %10'unu gÃ¶ster)* | BugÃ¼n SalÄ±, hafta sonu bilgisi bugÃ¼nkÃ¼ satÄ±ÅŸ tahmini iÃ§in henÃ¼z yararlÄ± deÄŸil. |
| **Coupon-Promo Effect** <br> *(Kupon Ä°ndirim Etkisi)* | Kept at 60% <br> *(Kupon hala geÃ§erli)* | **0.8 â†’ Reveal most of it** <br> *(Ã‡oÄŸunu gÃ¶ster)* | Kupon Ã‡arÅŸamba bitiyor, yani bugÃ¼n talebi etkilemeli. |
| **Christmas Peak** <br> *(Noel Zirvesi)* | Kept at 100% <br> *(Uzun vadeli hafÄ±za)* | **0.0 â†’ Reveal nothing** <br> *(HiÃ§bir ÅŸey gÃ¶sterme)* | Mart ayÄ±ndayÄ±z; Noel bilgisinin bugÃ¼nkÃ¼ tahmini ÅŸiÅŸirmesine izin verme. |

---

## ğŸ§  Conceptual Flow: The Conveyor Belt
**(Kavramsal AkÄ±ÅŸ: TaÅŸÄ±ma BandÄ±)**

AÅŸaÄŸÄ±daki ÅŸema, bilginin LSTM hÃ¼cresi iÃ§indeki akÄ±ÅŸÄ±nÄ± Ã¶zetler:

<img width="700" height="350" alt="image" src="https://github.com/user-attachments/assets/e8dccd36-58c8-4d4c-b3fd-8a4c0d62b97b" />


## ğŸ¯ Expert Summary: The Power of Gates
**(Uzman Ã–zeti: KapÄ±larÄ±n GÃ¼cÃ¼)**

> **Core Logic:** Her kapÄ± (gate) eÄŸitim sÄ±rasÄ±nda kendi aÄŸÄ±rlÄ±klarÄ±nÄ± (weights) Ã¶ÄŸrenir. Bu dinamik yapÄ± sayesinde LSTM, Ã§eliÅŸkili gibi gÃ¶rÃ¼nen gÃ¶revleri **tek bir model iÃ§inde** (all in one model) baÅŸarÄ±yla yÃ¶netir:
>
> 1.  **Forget:** HaftalÄ±k gÃ¼rÃ¼ltÃ¼yÃ¼ ve gereksiz veriyi unutur (Forgetting weekly noise).
> 2.  **Remember:** Mevsimsel dÃ¶ngÃ¼leri ve uzun vadeli trendleri hatÄ±rlar (Remembering seasonal cycles).
> 3.  **React:** Ani geliÅŸen olaylara ve ÅŸoklara tepki verir (Reacting to sudden events).

---

## ğŸ† Powerhouse Use Cases
**(GÃ¼Ã§ Merkezi KullanÄ±m AlanlarÄ±)**

LSTM, kÄ±sa vadeli oynaklÄ±k (volatility) ile uzun vadeli trendlerin iÃ§ iÃ§e geÃ§tiÄŸi alanlarda endÃ¼stri standardÄ± bir "GÃ¼Ã§ Merkezi"dir:

* ğŸ“ˆ **Sales Forecasting (SatÄ±ÅŸ Tahmini):**
    * *Short-term:* Promosyon kaynaklÄ± ani sÄ±Ã§ramalar (Promo spikes).
    * *Long-term:* Noel/Bayram sezonu etkileri (Seasonal effects).
* ğŸ’¹ **Stock-Price Moves (Hisse Senedi Hareketleri):**
    * *Short-term:* GÃ¼n iÃ§i dalgalanmalar/gÃ¼rÃ¼ltÃ¼ (Intraday jitter).
    * *Long-term:* Makroekonomik trendler ve dÃ¶ngÃ¼ler (Macroeconomic trends).
* ğŸŒ¦ **Weather Prediction (Hava Durumu Tahmini):**
    * *Short-term:* Saatlik sÄ±caklÄ±k deÄŸiÅŸimleri ve ani yaÄŸÄ±ÅŸlar (Hourly fluctuations).
    * *Long-term:* YÄ±llÄ±k iklim dÃ¶ngÃ¼leri (Yearly climate cycles).
* âš¡ **Energy Consumption (Enerji TÃ¼ketimi):**
    * *Short-term:* AnlÄ±k yÃ¼k deÄŸiÅŸimleri ve talep artÄ±ÅŸlarÄ± (Instant load changes).
    * *Long-term:* HaftalÄ±k ve mevsimsel kullanÄ±m kalÄ±plarÄ± (Weekly usage patterns).
 
    * 

# ğŸ”„ Summary: End-to-End ML/DL Workflow for Time Series
**(Ã–zet: Zaman Serileri iÃ§in UÃ§tan Uca ML/DL Ä°ÅŸ AkÄ±ÅŸÄ±)**

Zaman serisi problemleri, standart denetimli Ã¶ÄŸrenme (supervised learning) problemlerinden farklÄ±dÄ±r. Veri noktalarÄ± baÄŸÄ±msÄ±z deÄŸildir (not i.i.d.); zamanÄ±n akÄ±ÅŸÄ±, otokorelasyon ve sÄ±ralama kritiktir. AÅŸaÄŸÄ±daki iÅŸ akÄ±ÅŸÄ±, modern bir veri bilimcisinin **Klasik ML** (XGBoost, LightGBM) ve **Derin Ã–ÄŸrenme** (RNN, LSTM, Transformer) yaklaÅŸÄ±mlarÄ±nÄ± uygularken izlemesi gereken standart prosedÃ¼rÃ¼ tanÄ±mlar.

---

## 1. Problem Framing
**(Problemin Ã‡erÃ§evelenmesi)**

BaÅŸarÄ±lÄ± bir model, kod yazmadan Ã¶nce doÄŸru tanÄ±mla baÅŸlar.
* **Pick the Granularity (GranÃ¼lariteyi SeÃ§in):** Veri sÄ±klÄ±ÄŸÄ±nÄ± belirleyin (saatlik, gÃ¼nlÃ¼k, haftalÄ±k...).
    * *Trade-off:* Veri Ã§ok seyrekse (aylÄ±k) sinyal azdÄ±r; Ã§ok sÄ±ksa (dakikalÄ±k) gÃ¼rÃ¼ltÃ¼ (noise) fazladÄ±r.
* **Decide the Forecast Horizon (Tahmin Ufkuna Karar Verin):** Ne kadar ileriye tahmin yapÄ±lacak? (Ã¶nÃ¼mÃ¼zdeki 24 saat, gelecek 12 hafta...).
    * *Strategy:* KÄ±sa vade iÃ§in "One-step ahead", uzun vade iÃ§in "Multi-step direct" veya "Recursive" stratejiler seÃ§ilir.
* **Choose the Business Metric (Ä°ÅŸ MetriÄŸini SeÃ§in):** Optimizasyon hedefi iÅŸ ihtiyacÄ±na uymalÄ±dÄ±r.
    * **MAE/MAPE:** Talep tahmini (Demand) iÃ§in yaygÄ±ndÄ±r (yorumlanabilirdir).
    * **RMSE:** BÃ¼yÃ¼k hatalarÄ± aÄŸÄ±r cezalandÄ±rmak gerekiyorsa.
    * **Quantile Loss / Service Level:** Envanter yÃ¶netimi (Inventory) iÃ§in (stoksuz kalmama veya aÅŸÄ±rÄ± stok maliyeti dengesi).

---

## 2. Feature Engineering
**(Ã–zellik MÃ¼hendisliÄŸi)**

Veriyi modele nasÄ±l sunduÄŸunuz, algoritma tÃ¼rÃ¼ne gÃ¶re radikal biÃ§imde deÄŸiÅŸir.

### A. For Classical ML Models (Trees, Linear, SVR)
Model "zamanÄ±" ve "sÄ±rayÄ±" bilmez, ona biz Ã¶ÄŸretmeliyiz.
* **Lag Columns (Gecikme SÃ¼tunlarÄ±):** $t-1, t-7, t-30$ gibi geÃ§miÅŸ deÄŸerler. Otokorelasyonu yakalar.
* **Rolling Window Statistics (Kayan Pencere Ä°statistikleri):** Trend ve volatiliteyi yakalamak iÃ§in kayan ortalamalar (rolling means) ve standart sapmalar (rolling stds).
* **Calendar Flags (Takvim Ä°ÅŸaretÃ§ileri):** MevsimselliÄŸi yakalamak iÃ§in. HaftanÄ±n gÃ¼nÃ¼ (weekday dummies), ay, tatil (holiday binary) bilgileri.
* **External Regressors (DÄ±ÅŸsal DeÄŸiÅŸkenler):** Hava durumu, promosyon bayraÄŸÄ±, web trafiÄŸi.
* **Target Transforms (Hedef DÃ¶nÃ¼ÅŸÃ¼mleri):**
    * Log veya Box-Cox dÃ¶nÃ¼ÅŸÃ¼mÃ¼, varyansÄ± stabilize etmek iÃ§in kullanÄ±lÄ±r.
    * *Critical Note:* Bu, Lineer Regresyon, SVR, kNN iÃ§in ÅŸarttÄ±r (Gaussian varsayÄ±mÄ±). AÄŸaÃ§ tabanlÄ± (Tree-based) algoritmalar iÃ§in zorunlu deÄŸildir ancak performansÄ± artÄ±rabilir.

### B. For DL Models (RNN/LSTM/Transformers)
Derin Ã¶ÄŸrenme, ham dizilerden Ã¶zellik Ã§Ä±karabilir.
* **Raw History (Ham GeÃ§miÅŸ):** Genellikle Lag veya Rolling Ã¶zelliklere manuel ihtiyaÃ§ yoktur; sÄ±ralÄ± modeller (sequence models) ham geÃ§miÅŸi okuyarak bu kalÄ±plarÄ± Ã¶ÄŸrenir.
* **Embeddings (GÃ¶mÃ¼lÃ¼ Ã–znitelikler):** Kategorik deÄŸiÅŸkenler (shop_id, item_id) iÃ§in "One-Hot Encoding" yerine, Ã¶ÄŸrenilebilir vektÃ¶rler olan Embedding katmanlarÄ± kullanÄ±lÄ±r. YÃ¼ksek kardinalite (high cardinality) iÃ§in kritiktir.
* **Scaling (Ã–lÃ§eklendirme):**
    * SÃ¼rekli kanallarÄ± (continuous channels) normalleÅŸtirin/standartlaÅŸtÄ±rÄ±n (MinMax veya Z-Score). *DL modelleri Ã¶lÃ§eklendirilmemiÅŸ veride yakÄ±nsamaz (converge).*
    * Ä°kili (binary) deÄŸiÅŸkenleri 0/1 olarak bÄ±rakÄ±n.
* **Known-Future Features (Bilinen Gelecek Ã–zellikleri):** Gelecek zaman adÄ±mlarÄ± iÃ§in bilinen veriler (fiyat takvimi, promosyon planÄ±) modele "decoder" veya ek girdi olarak verilir.

---

## 3. Train / Validation Split
**(EÄŸitim / DoÄŸrulama AyrÄ±mÄ±)**

Zaman serilerinde **Asla KarÄ±ÅŸtÄ±rma YapÄ±lmaz (No Shuffling)!** Gelecek verisi geÃ§miÅŸe sÄ±zmamalÄ±dÄ±r (Data Leakage).

### Time-Based Only Strategy
* **Classical ML (Expanding Window / Walk-Forward):**
    * **Expanding-Window Back-test:** Ä°lk N gÃ¶zlemle baÅŸla, eÄŸit $\rightarrow$ sonraki bloÄŸu test et. Sonra pencereyi geniÅŸlet, tekrar eÄŸit ve test et. GerÃ§ek hayat performansÄ±nÄ± en iyi simÃ¼le eden yÃ¶ntemdir.
    * **Walk-Forward:** Her adÄ±mda modeli yeniden sÄ±ÄŸdÄ±rÄ±r (re-fit). KÃ¼Ã§Ã¼k veri setleri iÃ§in iyidir ancak hesaplama maliyeti yÃ¼ksektir.
* **Deep Learning (Early Stopping):**
    * **Early-Stopping Split:** Serinin son %10-20'sini bir "doÄŸrulama bloÄŸu" olarak ayÄ±rÄ±n. EÄŸitim hatasÄ± dÃ¼ÅŸerken doÄŸrulama hatasÄ± artmaya baÅŸladÄ±ÄŸÄ±nda (overfitting) eÄŸitimi durdurun.
    * **Rolling Validation:** EÄŸer kaynaklar elveriyorsa, DL modelleri iÃ§in de Ã§oklu yeniden eÄŸitim (multiple re-trains) yapÄ±labilir.
    * *Note:* Batch'lerin kronolojik sÄ±rayÄ± takip ettiÄŸinden emin olun (Ã¶zellikle stateful RNN'ler iÃ§in).

---

## 4. Model Selection & Tuning
**(Model SeÃ§imi ve Ayarlama)**

### ML (Machine Learning)
* **Grid / Bayesian Search:** AÄŸaÃ§ derinliÄŸi (depth), Ã¶ÄŸrenme oranÄ± (learning rate) gibi hiperparametreler iÃ§in Optuna veya basit Grid Search kullanÄ±n.
* **Evaluation:** Her konfigÃ¼rasyonu AdÄ±m 3'teki "Expanding-window" testi ile deÄŸerlendirin.
* **Selection Principle:** Ä°ÅŸ hedeflerini tutturan en dÃ¼ÅŸÃ¼k hataya (MAE/MAPE) sahip ve **en basit** modeli seÃ§in (Occam's Razor).

### DL (Deep Learning)
* **Architecture Tuning:** Katman sayÄ±sÄ±, gizli birimler (hidden units), Dropout oranÄ±, Dikkat baÅŸlÄ±klarÄ± (Attention heads).
* **Optimizer Schedule:** Ã–ÄŸrenme oranÄ± (Learning Rate) en kritik parametredir. Batch size ve Epoch sayÄ±sÄ± ayarlanmalÄ±dÄ±r.
* **Metric:** Erken durdurma (early stopping) iÃ§in doÄŸrulama kaybÄ±nÄ± (validation loss) kullanÄ±n. EÄŸer birden fazla konfigÃ¼rasyon yakÄ±nsarsa, onlarÄ± ayÄ±rdÄ±ÄŸÄ±nÄ±z test setindeki (hold-out) MAE/MAPE'ye gÃ¶re sÄ±ralayÄ±n.

---

## 5. Residual Diagnostics
**(ArtÄ±k DeÄŸer TeÅŸhisi)**

Model eÄŸitildikten sonra hatalarÄ± (residuals = GerÃ§ek - Tahmin) analiz edin.
* **Check:** ArtÄ±klar hala bir desen (pattern) gÃ¶steriyor mu?
* **Autocorrelation:** EÄŸer artÄ±klar arasÄ±nda otokorelasyon varsa, model bazÄ± sinyalleri kaÃ§Ä±rmÄ±ÅŸtÄ±r. Daha fazla Lag ekleyin.
* **Seasonality:** EÄŸer hatalarda dÃ¶nemsellik varsa, mevsimsel kukla deÄŸiÅŸkenler (seasonal dummies) ekleyin.

---

## 6. Deployment & Monitoring
**(DaÄŸÄ±tÄ±m ve Ä°zleme)**

Model canlÄ±ya alÄ±ndÄ±ÄŸÄ±nda iÅŸ bitmez.
* **Automate Retraining:** Yeni veriler geldikÃ§e modelin periyodik olarak yeniden eÄŸitilmesini otomatize edin.
* **Track Drift:** CanlÄ± hata oranÄ±nÄ± (live error drift) izleyin. Veri daÄŸÄ±lÄ±mÄ± deÄŸiÅŸti mi? (Concept Drift).
* **Alarms:** Rejim deÄŸiÅŸiklikleri (regime changes) veya beklenmedik anormallikler iÃ§in alarmlar kurun.

---

### ğŸ“Œ Expert Advice (Uzman Tavsiyesi)

> EÄŸer zaman serileriniz **kÄ±sa, temiz ve dÃ¼ÅŸÃ¼k gÃ¼rÃ¼ltÃ¼lÃ¼** (short, clean, low-noise) ise, karmaÅŸÄ±k modellere girmeden klasik **ARIMA/SARIMA** ile baÅŸlayÄ±n.
>
> Ancak veri karmaÅŸÄ±ksa (yÃ¼ksek gÃ¼rÃ¼ltÃ¼, Ã§oklu dÄ±ÅŸsal deÄŸiÅŸkenler), bir ML yaklaÅŸÄ±mÄ± â€”Ã¶nce **Boosted Trees (XGBoost/LightGBM)** ile baÅŸlayÄ±p, gerekirse **Deep Learning (LSTM/TFT)** tarafÄ±na geÃ§mekâ€” genellikle daha dÃ¼ÅŸÃ¼k hata oranlarÄ± ve daha zengin iÃ§gÃ¶rÃ¼ler (richer insights) saÄŸlar.
