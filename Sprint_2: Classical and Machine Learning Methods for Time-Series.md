# ğŸ“…  Classical and Machine Learning Methods for Time-Series
*(Hafta 2: Zaman Serileri iÃ§in Klasik ve Makine Ã–ÄŸrenimi YÃ¶ntemleri)*

> **Date Range:** Dec 8 - Dec 15

In this module, we will explore both **classical time-series methods** (*klasik zaman serisi yÃ¶ntemleri*) and **machine learning approaches** (*makine Ã¶ÄŸrenimi yaklaÅŸÄ±mlarÄ±*) for forecasting. We aim to bridge the gap between traditional statistical theory and modern predictive algorithms.
*(Bu modÃ¼lde, tahminleme iÃ§in hem klasik zaman serisi yÃ¶ntemlerini hem de makine Ã¶ÄŸrenimi yaklaÅŸÄ±mlarÄ±nÄ± inceleyeceÄŸiz. Geleneksel istatistiksel teori ile modern tahmin algoritmalarÄ± arasÄ±ndaki boÅŸluÄŸu doldurmayÄ± hedefliyoruz.)*

---

## ğŸ¯ Learning Objectives & Key Outcomes
*(Ã–ÄŸrenme Hedefleri ve Temel Ã‡Ä±karÄ±mlar)*

By the end of this week, you will be able to:
*(Bu haftanÄ±n sonunda ÅŸunlarÄ± yapabileceksiniz:)*

### 1. ğŸ“‰ Classical Time-Series Modelling
*(Klasik Zaman Serisi Modellemesi)*
* **Implement models:** Build and tune classical statistical models like **ARIMA** (*AutoRegressive Integrated Moving Average*) and **SARIMA** (*Seasonal ARIMA*).
    *(ARIMA ve SARIMA gibi klasik istatistiksel modelleri kurma ve ayarlama.)*
* **Core Concepts:** Understand **stationarity** (*durgunluk*), **differencing** (*fark alma*) and **seasonality** (*mevsimsellik*) handling in linear models.
    *(Lineer modellerde durgunluk, fark alma ve mevsimsellik yÃ¶netimini anlama.)*

### 2. ğŸŒ² Machine Learning Approaches
*(Makine Ã–ÄŸrenimi YaklaÅŸÄ±mlarÄ±)*
* **Apply Tree-Based Models:** Utilize powerful algorithms like **XGBoost**, **LightGBM**, or **Random Forest** for time-series forecasting.
    *(Zaman serisi tahmini iÃ§in XGBoost, LightGBM veya Random Forest gibi gÃ¼Ã§lÃ¼ algoritmalarÄ± kullanma.)*
* **Handling Non-Linearity:** Learn how these models capture non-linear relationships better than traditional methods.
    *(Bu modellerin doÄŸrusal olmayan iliÅŸkileri geleneksel yÃ¶ntemlerden nasÄ±l daha iyi yakaladÄ±ÄŸÄ±nÄ± Ã¶ÄŸrenme.)*

### 3. ğŸ§  Deep Learning Foundations
*(Derin Ã–ÄŸrenme Temelleri)*
* **Introduction to Neural Networks:** Get familiar with deep learning approaches tailored for sequential data, specifically:
    *(SÄ±ralÄ± veriler iÃ§in Ã¶zelleÅŸtirilmiÅŸ derin Ã¶ÄŸrenme yaklaÅŸÄ±mlarÄ±na aÅŸina olma, Ã¶zellikle:)*
    * **RNNs** (*Recurrent Neural Networks - Tekrarlayan Sinir AÄŸlarÄ±*)
    * **LSTMs** (*Long Short-Term Memory - Uzun KÄ±sa SÃ¼reli Bellek AÄŸlarÄ±*)
* **Sequence Modeling:** Understand how these networks manage **long-term dependencies** (*uzun vadeli baÄŸÄ±mlÄ±lÄ±klar*) in time series.

### 4. ğŸ› ï¸ Advanced Feature Engineering
*(Ä°leri Seviye Ã–zellik MÃ¼hendisliÄŸi)*
* **Data Preprocessing:** Perform preprocessing specifically tailored for supervised machine learning models.
    *(GÃ¶zetimli makine Ã¶ÄŸrenimi modelleri iÃ§in Ã¶zel olarak uyarlanmÄ±ÅŸ Ã¶n iÅŸleme gerÃ§ekleÅŸtirme.)*
* **Creating Signals:** Generate powerful features such as:
    *(GÃ¼Ã§lÃ¼ Ã¶zellikler Ã¼retme:)*
    * **Lag Features** (*Gecikmeli Ã–zellikler*)
    * **Rolling Window Statistics** (*Kayan Pencere Ä°statistikleri*)
    * **Time-Based Components** (*Zaman BazlÄ± BileÅŸenler - GÃ¼n, Ay, YÄ±l vb.*)

### 5. âš–ï¸ Strategic Comparison
*(Stratejik KarÅŸÄ±laÅŸtÄ±rma)*
* **Critical Analysis:** Understand the **differences**, **benefits**, and **challenges** of classical statistical methods versus machine learning approaches.
    *(Klasik istatistiksel yÃ¶ntemler ile makine Ã¶ÄŸrenimi yaklaÅŸÄ±mlarÄ± arasÄ±ndaki farklarÄ±, faydalarÄ± ve zorluklarÄ± anlama.)*
* **Decision Making:** Learn when to use which method based on data size, interpretability requirements, and computational resources.
    *(Veri boyutu, yorumlanabilirlik gereksinimleri ve hesaplama kaynaklarÄ±na baÄŸlÄ± olarak hangi yÃ¶ntemin ne zaman kullanÄ±lacaÄŸÄ±nÄ± Ã¶ÄŸrenme.)*

-------
-------
# ğŸ¯ Preparing Data & Introduction to Darts
*(Veri HazÄ±rlama ve Darts'a GiriÅŸ)*

Modelleme aÅŸamasÄ±na geÃ§meden Ã¶nce, Ã¼zerinde Ã§alÄ±ÅŸmaya hazÄ±r, temiz bir veri setine ihtiyacÄ±mÄ±z vardÄ±r. Bu bÃ¶lÃ¼mde, klasik **ARIMA/SARIMA** modellerini uygulamak iÃ§in Python'un gÃ¼Ã§lÃ¼ zaman serisi kÃ¼tÃ¼phanesi **Darts**'Ä± kullanacaÄŸÄ±z. Veri seti olarak yine "CorporaciÃ³n Favorita Grocery Sales Forecasting" verilerini kullanÄ±yoruz.

---

## ğŸ§ What is DARTS? Why use it?

**DARTS**, zaman serisi tahmini (*time-series forecasting*) iÃ§in Ã¶zel olarak tasarlanmÄ±ÅŸ, kullanÄ±mÄ± kolay ve Ã¼st dÃ¼zey (*high-level*) bir Python kÃ¼tÃ¼phanesidir.

> **ğŸ’¡ The Core Philosophy (Temel Felsefe):**
> DARTS, `scikit-learn` kÃ¼tÃ¼phanesinin kullanÄ±cÄ± dostu yapÄ±sÄ±nÄ± (fit/predict mantÄ±ÄŸÄ±) zaman serilerine uyarlar. KarmaÅŸÄ±k modelleri (ARIMA'dan Derin Ã–ÄŸrenme Transformer'larÄ±na kadar) tek bir satÄ±r kodla deÄŸiÅŸtirmenize ve test etmenize olanak tanÄ±r.

### Key Features (Temel Ã–zellikler)
1.  **Unified API (BirleÅŸik ArayÃ¼z):** Klasik istatistiksel modellerden (*ARIMA, Exponential Smoothing*) modern makine Ã¶ÄŸrenimi (*XGBoost, LightGBM*) ve derin Ã¶ÄŸrenme modellerine (*N-BEATS, LSTM, Transformers*) kadar her ÅŸeyi aynÄ± arayÃ¼zle kullanabilirsiniz.
2.  **Multivariate Support (Ã‡ok DeÄŸiÅŸkenli Destek):** Sadece hedef deÄŸiÅŸkeni deÄŸil, dÄ±ÅŸsal faktÃ¶rleri (*past/future covariates*) de modele kolayca dahil edebilirsiniz (Ã¶rn. Tatiller, Petrol FiyatlarÄ±).
3.  **Backtesting & Evaluation (Geriye DÃ¶nÃ¼k Test ve DeÄŸerlendirme):** Modelin geÃ§miÅŸ performansÄ±nÄ± simÃ¼le etmek iÃ§in gÃ¼Ã§lÃ¼ araÃ§lar sunar.
4.  **Probabilistic Forecasting (OlasÄ±lÄ±ksal Tahmin):** Sadece tek bir deÄŸer deÄŸil, gÃ¼ven aralÄ±klarÄ± (*confidence intervals*) ve olasÄ±lÄ±k daÄŸÄ±lÄ±mlarÄ± Ã¼retebilir.

---

## ğŸ› ï¸ Step-by-Step Workflow


### Step 0: Installing Darts
*(AdÄ±m 0: Darts Kurulumu)*

We will illustrate how to use classical time-series methods like ARIMA for forecasting using the DARTS library. First, ensure the library is installed in your environment.
*(DARTS kÃ¼tÃ¼phanesini kullanarak tahminleme iÃ§in ARIMA gibi klasik yÃ¶ntemlerin nasÄ±l kullanÄ±lacaÄŸÄ±nÄ± gÃ¶stereceÄŸiz. Ã–ncelikle kÃ¼tÃ¼phanenin ortamÄ±nÄ±zda kurulu olduÄŸundan emin olun.)*

### Step 1: Loading a Single Storeâ€“Item Series
*(AdÄ±m 1: Tek Bir MaÄŸaza-ÃœrÃ¼n Serisini YÃ¼kleme)*

Rather than juggling millions of rows, weâ€™ll extract **one product in one store** and truncate everything after March 31, 2014.
*(Milyonlarca satÄ±rla boÄŸuÅŸmak yerine, tek bir maÄŸazadaki tek bir Ã¼rÃ¼nÃ¼ Ã§Ä±karacaÄŸÄ±z ve 31 Mart 2014'ten sonraki verileri keseceÄŸiz.)*

* **Goal (Hedef):** Create a manageable dataset to learn the mechanics of the model.
* **Action (Eylem):** Filter by `store_nbr`, `item_nbr`, and apply `date < '2014-04-01'`.
* **Result (SonuÃ§):** A tidy DataFrame with just daily sales for our chosen series. (*SeÃ§ilen serimiz iÃ§in sadece gÃ¼nlÃ¼k satÄ±ÅŸlarÄ± iÃ§eren dÃ¼zenli bir DataFrame.*)

### Step 2: Prepare & Convert to `TimeSeries`
*(AdÄ±m 2: HazÄ±rlÄ±k ve `TimeSeries` Nesnesine DÃ¶nÃ¼ÅŸtÃ¼rme)*

This is the most critical step specific to Darts. Darts works with its own object type called `TimeSeries`, not standard Pandas DataFrames.
*(Bu, Darts'a Ã¶zgÃ¼ en kritik adÄ±mdÄ±r. Darts, standart Pandas DataFrame'leri ile deÄŸil, `TimeSeries` adÄ± verilen kendi nesne tÃ¼rÃ¼yle Ã§alÄ±ÅŸÄ±r.)*

**The Process:**
1.  **Datetime Conversion:** Convert the `date` column to datetime objects and set it as the index.
    *(Tarih sÃ¼tununu datetime nesnelerine Ã§evirin ve indeks olarak ayarlayÄ±n.)*
2.  **Aggregation:** Aggregate by date (summing all `unit_sales` for that day).
    *(Tarihe gÃ¶re toplulaÅŸtÄ±rma yapÄ±n [o gÃ¼nkÃ¼ tÃ¼m birim satÄ±ÅŸlarÄ± toplayarak].)*
3.  **Gap Filling (Reindexing):** Reindex to a complete daily calendar, filling any missing dates with **zero**.
    *(Eksik tarihleri sÄ±fÄ±r ile doldurarak tam bir gÃ¼nlÃ¼k takvime gÃ¶re yeniden indeksleyin.)*
4.  **Darts Conversion:** Finally, wrap it in Dartsâ€™ `TimeSeries` object using `TimeSeries.from_dataframe()`.
    *(Son olarak, `TimeSeries.from_dataframe()` kullanarak veriyi Darts TimeSeries nesnesine sarÄ±n.)*

> **âœ… Why?** This ensures all the libraryâ€™s modeling and backtesting tools work **out of the box** (*kurulum gerektirmeden/hazÄ±r olarak*).

---

### ğŸ“Š Visual Analysis & Interpretation
*(GÃ¶rsel Analiz ve Yorumlama)*



#### ğŸ’¡ Think First!


Before reading our interpretation, take a moment to reflect on the chart yourself:
*(Yorumumuzu okumadan Ã¶nce, grafik Ã¼zerinde dÃ¼ÅŸÃ¼nmek iÃ§in bir dakikanÄ±zÄ± ayÄ±rÄ±n:)*
* What do you notice about the **sales volume**? Is it consistent, volatile, or trending?
* Do the **spikes** (*sÄ±Ã§ramalar*) follow any visible pattern?
* What kinds of **features** might help the model capture this behavior?

#### ğŸ“‰ Our Analysis


**1. Key Observations (Temel GÃ¶zlemler)**
* **Consistently High Sales:** The product sells every day, typically between 300 and 800 units. It is a **high-volume item**.
* **No Missing/Zero Values:** We see activity on every day. This is ideal for training a model that learns from historical behavior without needing complex imputation (*eksik veri doldurma*).
* **Frequent, Sharp Fluctuations:** The series is **noisy** (*gÃ¼rÃ¼ltÃ¼lÃ¼*) â€” it goes up and down regularly â€” but mostly within a predictable range.
* **Occasional Large Peaks:** Some spikes rise sharply (>1000) above the usual range. These likely correspond to **promotions**, **holidays**, or **special events**.

**2. What This Suggests for Forecasting (Tahminleme Ä°Ã§in Ne Anlama Geliyor?)**
* **Lag Features:** The model will benefit from lags (e.g., `sales_lag_1`, `rolling_mean_7`) to learn local dynamics.
* **Calendar Features:** Including `day_of_week`, `is_weekend`, or `month` will help capture recurring patterns (*tekrarlayan kalÄ±plar*).
* **Volatility Handling:** Applying a **rolling average** or using a **log transformation** might improve model stability against noise.
* **External Signals:** To predict the huge spikes, adding **promotions** data (as *covariates*) would be valuable.

---

### Step 3: Splitting the Data into Training and Testing Sets
*(AdÄ±m 3: Veriyi EÄŸitim ve Test Setlerine AyÄ±rma)*

In time-series forecasting, we cannot use random splitting (like `train_test_split` in sklearn) because the **order of data matters**. We must split chronologically.
*(Zaman serisi tahmininde, rastgele bÃ¶lme kullanamayÄ±z Ã§Ã¼nkÃ¼ verinin sÄ±rasÄ± Ã¶nemlidir. Kronolojik olarak bÃ¶lmemiz gerekir.)*

* **Training Set (EÄŸitim Seti):** The past data used to teach the model patterns (e.g., usually the first 80-90% of the timeline).
    *(Model kalÄ±plarÄ±nÄ± Ã¶ÄŸretmek iÃ§in kullanÄ±lan geÃ§miÅŸ veriler.)*
* **Validation/Test Set (DoÄŸrulama/Test Seti):** The recent data used to evaluate how well the model predicts the future (the remaining 10-20%).
    *(Modelin geleceÄŸi ne kadar iyi tahmin ettiÄŸini deÄŸerlendirmek iÃ§in kullanÄ±lan son veriler.)*

**Darts Method:**
We use specific Darts methods (like `.split_before()`) to ensure no **data leakage** (*veri sÄ±zÄ±ntÄ±sÄ±*) occursâ€”meaning the model never sees the "future" it is trying to predict during training.


