# ğŸ“…  Classical and Machine Learning Methods for Time-Series
*(Hafta 2: Zaman Serileri iÃ§in Klasik ve Makine Ã–ÄŸrenimi YÃ¶ntemleri)*

> **Date Range:** Dec 8 - Dec 15

In this module, we will explore both **classical time-series methods** (*klasik zaman serisi yÃ¶ntemleri*) and **machine learning approaches** (*makine Ã¶ÄŸrenimi yaklaÅŸÄ±mlarÄ±*) for forecasting. We aim to bridge the gap between traditional statistical theory and modern predictive algorithms.
*(Bu modÃ¼lde, tahminleme iÃ§in hem klasik zaman serisi yÃ¶ntemlerini hem de makine Ã¶ÄŸrenimi yaklaÅŸÄ±mlarÄ±nÄ± inceleyeceÄŸiz. Geleneksel istatistiksel teori ile modern tahmin algoritmalarÄ± arasÄ±ndaki boÅŸluÄŸu doldurmayÄ± hedefliyoruz.)*

---

## ğŸ¯ Learning Objectives & Key Outcomes
*(Ã–ÄŸrenme Hedefleri ve Temel Ã‡Ä±karÄ±mlar)*

By the end of this week, you will be able to:
*(Bu haftanÄ±n sonunda ÅŸunlarÄ± yapabileceksiniz:)*

### 1. ğŸ“‰ Classical Time-Series Modelling
*(Klasik Zaman Serisi Modellemesi)*
* **Implement models:** Build and tune classical statistical models like **ARIMA** (*AutoRegressive Integrated Moving Average*) and **SARIMA** (*Seasonal ARIMA*).
    *(ARIMA ve SARIMA gibi klasik istatistiksel modelleri kurma ve ayarlama.)*
* **Core Concepts:** Understand **stationarity** (*durgunluk*), **differencing** (*fark alma*) and **seasonality** (*mevsimsellik*) handling in linear models.
    *(Lineer modellerde durgunluk, fark alma ve mevsimsellik yÃ¶netimini anlama.)*

### 2. ğŸŒ² Machine Learning Approaches
*(Makine Ã–ÄŸrenimi YaklaÅŸÄ±mlarÄ±)*
* **Apply Tree-Based Models:** Utilize powerful algorithms like **XGBoost**, **LightGBM**, or **Random Forest** for time-series forecasting.
    *(Zaman serisi tahmini iÃ§in XGBoost, LightGBM veya Random Forest gibi gÃ¼Ã§lÃ¼ algoritmalarÄ± kullanma.)*
* **Handling Non-Linearity:** Learn how these models capture non-linear relationships better than traditional methods.
    *(Bu modellerin doÄŸrusal olmayan iliÅŸkileri geleneksel yÃ¶ntemlerden nasÄ±l daha iyi yakaladÄ±ÄŸÄ±nÄ± Ã¶ÄŸrenme.)*

### 3. ğŸ§  Deep Learning Foundations
*(Derin Ã–ÄŸrenme Temelleri)*
* **Introduction to Neural Networks:** Get familiar with deep learning approaches tailored for sequential data, specifically:
    *(SÄ±ralÄ± veriler iÃ§in Ã¶zelleÅŸtirilmiÅŸ derin Ã¶ÄŸrenme yaklaÅŸÄ±mlarÄ±na aÅŸina olma, Ã¶zellikle:)*
    * **RNNs** (*Recurrent Neural Networks - Tekrarlayan Sinir AÄŸlarÄ±*)
    * **LSTMs** (*Long Short-Term Memory - Uzun KÄ±sa SÃ¼reli Bellek AÄŸlarÄ±*)
* **Sequence Modeling:** Understand how these networks manage **long-term dependencies** (*uzun vadeli baÄŸÄ±mlÄ±lÄ±klar*) in time series.

### 4. ğŸ› ï¸ Advanced Feature Engineering
*(Ä°leri Seviye Ã–zellik MÃ¼hendisliÄŸi)*
* **Data Preprocessing:** Perform preprocessing specifically tailored for supervised machine learning models.
    *(GÃ¶zetimli makine Ã¶ÄŸrenimi modelleri iÃ§in Ã¶zel olarak uyarlanmÄ±ÅŸ Ã¶n iÅŸleme gerÃ§ekleÅŸtirme.)*
* **Creating Signals:** Generate powerful features such as:
    *(GÃ¼Ã§lÃ¼ Ã¶zellikler Ã¼retme:)*
    * **Lag Features** (*Gecikmeli Ã–zellikler*)
    * **Rolling Window Statistics** (*Kayan Pencere Ä°statistikleri*)
    * **Time-Based Components** (*Zaman BazlÄ± BileÅŸenler - GÃ¼n, Ay, YÄ±l vb.*)

### 5. âš–ï¸ Strategic Comparison
*(Stratejik KarÅŸÄ±laÅŸtÄ±rma)*
* **Critical Analysis:** Understand the **differences**, **benefits**, and **challenges** of classical statistical methods versus machine learning approaches.
    *(Klasik istatistiksel yÃ¶ntemler ile makine Ã¶ÄŸrenimi yaklaÅŸÄ±mlarÄ± arasÄ±ndaki farklarÄ±, faydalarÄ± ve zorluklarÄ± anlama.)*
* **Decision Making:** Learn when to use which method based on data size, interpretability requirements, and computational resources.
    *(Veri boyutu, yorumlanabilirlik gereksinimleri ve hesaplama kaynaklarÄ±na baÄŸlÄ± olarak hangi yÃ¶ntemin ne zaman kullanÄ±lacaÄŸÄ±nÄ± Ã¶ÄŸrenme.)*

-------
-------
# ğŸ¯ Preparing Data & Introduction to Darts
*(Veri HazÄ±rlama ve Darts'a GiriÅŸ)*

Modelleme aÅŸamasÄ±na geÃ§meden Ã¶nce, Ã¼zerinde Ã§alÄ±ÅŸmaya hazÄ±r, temiz bir veri setine ihtiyacÄ±mÄ±z vardÄ±r. Bu bÃ¶lÃ¼mde, klasik **ARIMA/SARIMA** modellerini uygulamak iÃ§in Python'un gÃ¼Ã§lÃ¼ zaman serisi kÃ¼tÃ¼phanesi **Darts**'Ä± kullanacaÄŸÄ±z. Veri seti olarak yine "CorporaciÃ³n Favorita Grocery Sales Forecasting" verilerini kullanÄ±yoruz.

---

## ğŸ§ What is DARTS? Why use it?

**DARTS**, zaman serisi tahmini (*time-series forecasting*) iÃ§in Ã¶zel olarak tasarlanmÄ±ÅŸ, kullanÄ±mÄ± kolay ve Ã¼st dÃ¼zey (*high-level*) bir Python kÃ¼tÃ¼phanesidir.

> **ğŸ’¡ The Core Philosophy (Temel Felsefe):**
> DARTS, `scikit-learn` kÃ¼tÃ¼phanesinin kullanÄ±cÄ± dostu yapÄ±sÄ±nÄ± (fit/predict mantÄ±ÄŸÄ±) zaman serilerine uyarlar. KarmaÅŸÄ±k modelleri (ARIMA'dan Derin Ã–ÄŸrenme Transformer'larÄ±na kadar) tek bir satÄ±r kodla deÄŸiÅŸtirmenize ve test etmenize olanak tanÄ±r.

### Key Features (Temel Ã–zellikler)
1.  **Unified API (BirleÅŸik ArayÃ¼z):** Klasik istatistiksel modellerden (*ARIMA, Exponential Smoothing*) modern makine Ã¶ÄŸrenimi (*XGBoost, LightGBM*) ve derin Ã¶ÄŸrenme modellerine (*N-BEATS, LSTM, Transformers*) kadar her ÅŸeyi aynÄ± arayÃ¼zle kullanabilirsiniz.
2.  **Multivariate Support (Ã‡ok DeÄŸiÅŸkenli Destek):** Sadece hedef deÄŸiÅŸkeni deÄŸil, dÄ±ÅŸsal faktÃ¶rleri (*past/future covariates*) de modele kolayca dahil edebilirsiniz (Ã¶rn. Tatiller, Petrol FiyatlarÄ±).
3.  **Backtesting & Evaluation (Geriye DÃ¶nÃ¼k Test ve DeÄŸerlendirme):** Modelin geÃ§miÅŸ performansÄ±nÄ± simÃ¼le etmek iÃ§in gÃ¼Ã§lÃ¼ araÃ§lar sunar.
4.  **Probabilistic Forecasting (OlasÄ±lÄ±ksal Tahmin):** Sadece tek bir deÄŸer deÄŸil, gÃ¼ven aralÄ±klarÄ± (*confidence intervals*) ve olasÄ±lÄ±k daÄŸÄ±lÄ±mlarÄ± Ã¼retebilir.

---

## ğŸ› ï¸ Step-by-Step Workflow


### Step 0: Installing Darts
*(AdÄ±m 0: Darts Kurulumu)*

We will illustrate how to use classical time-series methods like ARIMA for forecasting using the DARTS library. First, ensure the library is installed in your environment.
*(DARTS kÃ¼tÃ¼phanesini kullanarak tahminleme iÃ§in ARIMA gibi klasik yÃ¶ntemlerin nasÄ±l kullanÄ±lacaÄŸÄ±nÄ± gÃ¶stereceÄŸiz. Ã–ncelikle kÃ¼tÃ¼phanenin ortamÄ±nÄ±zda kurulu olduÄŸundan emin olun.)*

### Step 1: Loading a Single Storeâ€“Item Series
*(AdÄ±m 1: Tek Bir MaÄŸaza-ÃœrÃ¼n Serisini YÃ¼kleme)*

Rather than juggling millions of rows, weâ€™ll extract **one product in one store** and truncate everything after March 31, 2014.
*(Milyonlarca satÄ±rla boÄŸuÅŸmak yerine, tek bir maÄŸazadaki tek bir Ã¼rÃ¼nÃ¼ Ã§Ä±karacaÄŸÄ±z ve 31 Mart 2014'ten sonraki verileri keseceÄŸiz.)*

* **Goal (Hedef):** Create a manageable dataset to learn the mechanics of the model.
* **Action (Eylem):** Filter by `store_nbr`, `item_nbr`, and apply `date < '2014-04-01'`.
* **Result (SonuÃ§):** A tidy DataFrame with just daily sales for our chosen series. (*SeÃ§ilen serimiz iÃ§in sadece gÃ¼nlÃ¼k satÄ±ÅŸlarÄ± iÃ§eren dÃ¼zenli bir DataFrame.*)

### Step 2: Prepare & Convert to `TimeSeries`
*(AdÄ±m 2: HazÄ±rlÄ±k ve `TimeSeries` Nesnesine DÃ¶nÃ¼ÅŸtÃ¼rme)*

This is the most critical step specific to Darts. Darts works with its own object type called `TimeSeries`, not standard Pandas DataFrames.
*(Bu, Darts'a Ã¶zgÃ¼ en kritik adÄ±mdÄ±r. Darts, standart Pandas DataFrame'leri ile deÄŸil, `TimeSeries` adÄ± verilen kendi nesne tÃ¼rÃ¼yle Ã§alÄ±ÅŸÄ±r.)*

**The Process:**
1.  **Datetime Conversion:** Convert the `date` column to datetime objects and set it as the index.
    *(Tarih sÃ¼tununu datetime nesnelerine Ã§evirin ve indeks olarak ayarlayÄ±n.)*
2.  **Aggregation:** Aggregate by date (summing all `unit_sales` for that day).
    *(Tarihe gÃ¶re toplulaÅŸtÄ±rma yapÄ±n [o gÃ¼nkÃ¼ tÃ¼m birim satÄ±ÅŸlarÄ± toplayarak].)*
3.  **Gap Filling (Reindexing):** Reindex to a complete daily calendar, filling any missing dates with **zero**.
    *(Eksik tarihleri sÄ±fÄ±r ile doldurarak tam bir gÃ¼nlÃ¼k takvime gÃ¶re yeniden indeksleyin.)*
4.  **Darts Conversion:** Finally, wrap it in Dartsâ€™ `TimeSeries` object using `TimeSeries.from_dataframe()`.
    *(Son olarak, `TimeSeries.from_dataframe()` kullanarak veriyi Darts TimeSeries nesnesine sarÄ±n.)*

> **âœ… Why?** This ensures all the libraryâ€™s modeling and backtesting tools work **out of the box** (*kurulum gerektirmeden/hazÄ±r olarak*).

---

### ğŸ“Š Visual Analysis & Interpretation
*(GÃ¶rsel Analiz ve Yorumlama)*



#### ğŸ’¡ Think First!


Before reading our interpretation, take a moment to reflect on the chart yourself:
*(Yorumumuzu okumadan Ã¶nce, grafik Ã¼zerinde dÃ¼ÅŸÃ¼nmek iÃ§in bir dakikanÄ±zÄ± ayÄ±rÄ±n:)*
* What do you notice about the **sales volume**? Is it consistent, volatile, or trending?
* Do the **spikes** (*sÄ±Ã§ramalar*) follow any visible pattern?
* What kinds of **features** might help the model capture this behavior?

#### ğŸ“‰ Our Analysis


**1. Key Observations (Temel GÃ¶zlemler)**
* **Consistently High Sales:** The product sells every day, typically between 300 and 800 units. It is a **high-volume item**.
* **No Missing/Zero Values:** We see activity on every day. This is ideal for training a model that learns from historical behavior without needing complex imputation (*eksik veri doldurma*).
* **Frequent, Sharp Fluctuations:** The series is **noisy** (*gÃ¼rÃ¼ltÃ¼lÃ¼*) â€” it goes up and down regularly â€” but mostly within a predictable range.
* **Occasional Large Peaks:** Some spikes rise sharply (>1000) above the usual range. These likely correspond to **promotions**, **holidays**, or **special events**.

**2. What This Suggests for Forecasting (Tahminleme Ä°Ã§in Ne Anlama Geliyor?)**
* **Lag Features:** The model will benefit from lags (e.g., `sales_lag_1`, `rolling_mean_7`) to learn local dynamics.
* **Calendar Features:** Including `day_of_week`, `is_weekend`, or `month` will help capture recurring patterns (*tekrarlayan kalÄ±plar*).
* **Volatility Handling:** Applying a **rolling average** or using a **log transformation** might improve model stability against noise.
* **External Signals:** To predict the huge spikes, adding **promotions** data (as *covariates*) would be valuable.

---

### Step 3: Splitting the Data into Training and Testing Sets
*(AdÄ±m 3: Veriyi EÄŸitim ve Test Setlerine AyÄ±rma)*

In time-series forecasting, we cannot use random splitting (like `train_test_split` in sklearn) because the **order of data matters**. We must split chronologically.
*(Zaman serisi tahmininde, rastgele bÃ¶lme kullanamayÄ±z Ã§Ã¼nkÃ¼ verinin sÄ±rasÄ± Ã¶nemlidir. Kronolojik olarak bÃ¶lmemiz gerekir.)*

* **Training Set (EÄŸitim Seti):** The past data used to teach the model patterns (e.g., usually the first 80-90% of the timeline).
    *(Model kalÄ±plarÄ±nÄ± Ã¶ÄŸretmek iÃ§in kullanÄ±lan geÃ§miÅŸ veriler.)*
* **Validation/Test Set (DoÄŸrulama/Test Seti):** The recent data used to evaluate how well the model predicts the future (the remaining 10-20%).
    *(Modelin geleceÄŸi ne kadar iyi tahmin ettiÄŸini deÄŸerlendirmek iÃ§in kullanÄ±lan son veriler.)*

**Darts Method:**
We use specific Darts methods (like `.split_before()`) to ensure no **data leakage** (*veri sÄ±zÄ±ntÄ±sÄ±*) occursâ€”meaning the model never sees the "future" it is trying to predict during training.

---
---

# ğŸ•°ï¸ Classical Time-Series Methods: ARIMA & Parameter `d`
*(Klasik Zaman Serisi YÃ¶ntemleri: ARIMA ve d Parametresi)*

**ARIMA** (*AutoRegressive Integrated Moving Average*), gÃ¼Ã§lÃ¼ mevsimsel kalÄ±plarÄ± olmayan zaman serisi verilerini anlamak ve tahmin etmek iÃ§in kullanÄ±lan temel bir modeldir.

---

## ğŸ§ How Does ARIMA Work?
*(ARIMA NasÄ±l Ã‡alÄ±ÅŸÄ±r?)*

Imagine youâ€™re trying to forecast the daily sales in a grocery store. ARIMA helps predict the sales for tomorrow by combining three components:
*(Bir marketin gÃ¼nlÃ¼k satÄ±ÅŸlarÄ±nÄ± tahmin etmeye Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ±zÄ± hayal edin. ARIMA, Ã¼Ã§ bileÅŸeni birleÅŸtirerek yarÄ±nÄ±n satÄ±ÅŸlarÄ±nÄ± tahmin etmeye yardÄ±mcÄ± olur:)*

1.  **AR (AutoRegression):** Looking at how past daysâ€™ sales have behaved.
    *(GeÃ§miÅŸ gÃ¼nlerin satÄ±ÅŸlarÄ±nÄ±n nasÄ±l davrandÄ±ÄŸÄ±na bakmak [Otokorelasyon].)*
2.  **I (Integrated):** Correcting for any trends via differencing.
    *(Fark alma yoluyla trendleri dÃ¼zeltmek [Entegrasyon].)*
3.  **MA (Moving Average):** Learning from the errors made in previous predictions.
    *(Ã–nceki tahminlerde yapÄ±lan hatalardan Ã¶ÄŸrenmek [Hareketli Ortalama].)*

> â˜ğŸ¼ **Summary:** ARIMA tries to use the **past values** (AR) and **past prediction errors** (MA) to make accurate forecasts, while adjusting for **trends** in the data (I).

---

## ğŸ“‰ The "I" in ARIMA and Parameter `d`
*(ARIMA'daki "I" ve d Parametresi)*

Sometimes data isnâ€™t "**stationary**," meaning it has a trend that increases or decreases over time.
*(Bazen veriler "durgun" deÄŸildir, yani zamanla artan veya azalan bir trende sahiptir.)*

* **The Integrated (I) part** helps by removing these trends to make the data easier to predict.
* **Method:** It does this by **differencing**â€”subtracting the previous value from the current value to smooth out the trend.
    *(YÃ¶ntem: Bunu fark alma yoluyla yaparâ€”trendi yumuÅŸatmak iÃ§in Ã¶nceki deÄŸeri mevcut deÄŸerden Ã§Ä±karÄ±r.)*



> **Definition:** The `d` in `ARIMA(p,d,q)` tells us **how many times** to difference the series to remove a trend and achieve stationarity (constant mean & variance).

---

## ğŸ› ï¸ Step-by-Step Guide: Choosing `d`
*(AdÄ±m AdÄ±m Rehber: d SeÃ§imi)*

### Step 1: Start with No Differencing (`d=0`)
*(AdÄ±m 1: Fark Alma Olmadan BaÅŸla)*

#### 1. Visual Check: Plot Your Raw Series
*(GÃ¶rsel Kontrol: Ham Seriyi Ã‡izdir)*
* **Visual Patterns & Drift:** Does a gentle upward or downward drift hide underneath the spikes?
    *(SÄ±Ã§ramalarÄ±n altÄ±nda hafif bir yukarÄ± veya aÅŸaÄŸÄ± sÃ¼rÃ¼klenme gizleniyor mu?)*
* **Mean Level:** Does it look like the mean level is roughly constant, or does it trend up/down?
    *(Ortalama seviye kabaca sabit mi gÃ¶rÃ¼nÃ¼yor, yoksa yukarÄ±/aÅŸaÄŸÄ± trend mi var?)*
* **Stationarity Assessment:** Based on your visual impression, would you call this series â€œstationaryâ€?

> **Our Analysis (Example):**
> * **Visual:** The series shows strong short-term fluctuations but no clear long-term trend. The mean level appears fairly stable around 500â€“600 units/day.
> * **Conclusion:** This series appears visually stationary, or at least close enough for many forecasting models (d=0).

#### 2. Rolling Mean Check
*(Hareketli Ortalama KontrolÃ¼)*
Letâ€™s smooth out the day-to-day spikes with a **30-day rolling average**. By averaging over a full month, the noisy zero-and-spike pattern flattens out, revealing whether there really is a gradual trend.



> **Analysis:** The mean level drops early in 2013 but stays stable (430â€“480 units) afterwards. It may be treated as **near-stationary**.

#### 3. Statistical Check: ADF Test
*(Ä°statistiksel Kontrol: ADF Testi)*

**What is ADF Test?** (*Augmented Dickey-Fuller Test*)
It is a statistical test used to check for **stationarity**.
* **Null Hypothesis ($H_0$):** The series has a unit root (it is **not stationary**).
* **Alternative Hypothesis ($H_1$):** The series has no unit root (it is **stationary**).

**Decision Rule:**
* If **p-value < 0.05**: Reject $H_0$ â†’ Series is **Stationary**. (Accept `d=0`)
* If **p-value â‰¥ 0.05**: Fail to reject $H_0$ â†’ Series is **Non-Stationary**. (Try `d=1`)

```python
# Code snippet for ADF Test
# Result Example:
# p-value: 0.000467...
```

## ğŸ“‰ Step 2: If Not Stationary, Try One Difference (`d=1`)
*(AdÄ±m 2: Durgun DeÄŸilse, Bir Fark AlmayÄ± Dene)*

If the visual check showed a **trend** or the **ADF p-value** was **â‰¥ 0.05**:
*(EÄŸer gÃ¶rsel kontrol bir trend gÃ¶sterdiyse veya ADF p-deÄŸeri â‰¥ 0.05 ise:)*

### ğŸ› ï¸ Process (SÃ¼reÃ§)

1.  **Compute the First Difference:**
    *(Birinci FarkÄ± Hesapla:)*
    $$y'_t = y_t - y_{t-1}$$

2.  **Visual Check:**
    *(GÃ¶rsel Kontrol:)*
    Does it now **oscillate around zero** with no clear drift?
    *(Åimdi belirgin bir sÃ¼rÃ¼klenme olmadan sÄ±fÄ±r etrafÄ±nda salÄ±nÄ±yor mu?)*

3.  **ADF Test Again:**
    *(Tekrar ADF Testi:)*
    If **p < 0.05**, accept `d=1`.
    *(EÄŸer p < 0.05 ise, d=1 olarak kabul et.)*

> âš ï¸ **Note (Not):** Even if `d=0` passed, we sometimes try `d=1` to see if it improves **model stability**, but be careful not to **over-difference**.
> *(d=0 geÃ§miÅŸ olsa bile, bazen model kararlÄ±lÄ±ÄŸÄ±nÄ± iyileÅŸtirip iyileÅŸtirmediÄŸini gÃ¶rmek iÃ§in d=1 deneriz, ancak aÅŸÄ±rÄ± fark alma [over-differencing] konusunda dikkatli olun.)*


## ğŸ“Œ Summary: Choosing `d`
*(Ã–zet: d SeÃ§imi)*

AÅŸaÄŸÄ±daki tablo, gÃ¶rsel analiz ve istatistiksel test sonuÃ§larÄ±na gÃ¶re `d` parametresini nasÄ±l seÃ§eceÄŸinizi Ã¶zetler.

| Durum (Condition) | Aksiyon (Action) |
| :--- | :--- |
| **Visual:** Flat mean, no trend.<br>*(DÃ¼z ortalama, trend yok.)*<br>**ADF:** p < 0.05 | **Stop.** Accept `d=0`.<br>*(Dur. d=0 kabul et.)* |
| **Visual:** Trend visible.<br>*(Trend gÃ¶rÃ¼nÃ¼r.)*<br>**ADF:** p â‰¥ 0.05 | **Difference once.** Try `d=1`.<br>*(Bir kez fark al. d=1 dene.)* |
| **Visual:** Still trending after `d=1`.<br>*(d=1 sonrasÄ± hala trend var.)*<br>**ADF:** p â‰¥ 0.05 | **Difference again (Rare).** Try `d=2`.<br>*(Tekrar fark al [Nadir]. d=2 dene.)* |

---

> âš ï¸ **Warning (UyarÄ±):** **Over-differencing** (*AÅŸÄ±rÄ± Fark Alma*) can introduce extra **noise** (*gÃ¼rÃ¼ltÃ¼*) and hurt your **forecast** (*tahmin*). Always balance **visual evidence** (*gÃ¶rsel kanÄ±t*) with **statistical tests** (*istatistiksel testler*).


# ğŸ“‰ Reading a PACF Plot: Choosing the AR Order (`p`)
*(PACF GrafiÄŸini Okuma: AR Derecesi `p` SeÃ§imi)*

ARIMA modelinin **AR (AutoRegressive)** bileÅŸeni olan `p` parametresini belirlemek iÃ§in birincil aracÄ±mÄ±z **Partial Autocorrelation Function (PACF)** grafiÄŸidir. 

### ğŸ§ What is PACF?


ACF (Autocorrelation Function), bir gecikmenin (*lag*) hem doÄŸrudan hem de dolaylÄ± etkilerini gÃ¶sterirken; **PACF**, aradaki gecikmelerin etkisini kaldÄ±rdÄ±ktan sonra, sadece o gecikmenin ÅŸimdiki zaman Ã¼zerindeki **saf ve doÄŸrudan etkisini** (*pure/direct effect*) Ã¶lÃ§er.

> ğŸ’¡ **Rule of Thumb:** AR (`p`) derecesini bulmak iÃ§in **PACF** grafiÄŸine, MA (`q`) derecesini bulmak iÃ§in **ACF** grafiÄŸine bakÄ±lÄ±r.

---

### ğŸ“Š How to Interpret the PACF Plot

PACF grafiÄŸindeki her bir Ã§ubuk (*bar*), ilgili gecikmenin korelasyon katsayÄ±sÄ±nÄ± temsil eder. Arka plandaki gÃ¶lgeli alan (genellikle mavi), **95% Confidence Interval** (GÃ¼ven AralÄ±ÄŸÄ±)'dÄ±r.


| Plot Feature (Grafik Ã–zelliÄŸi) | Interpretation (Yorumlama) |
| :--- | :--- |
| **Tall bar outside the grey band**<br>*(Gri bandÄ±n dÄ±ÅŸÄ±ndaki uzun Ã§ubuk)* | **Statistically Significant:** There is a significant, direct correlation at that lag.<br>*(Ä°statistiksel Olarak AnlamlÄ±: O gecikmede anlamlÄ± ve doÄŸrudan bir etki vardÄ±r.)* |
| **Bars drop inside the band and stay there**<br>*(Ã‡ubuklar bandÄ±n iÃ§ine dÃ¼ÅŸÃ¼yor ve orada kalÄ±yor)* | **Cut-off Point:** Useful memory ends here. This sharp drop indicates the order of the AR process.<br>*(Kesilme NoktasÄ±: YararlÄ± hafÄ±za burada biter. Bu keskin dÃ¼ÅŸÃ¼ÅŸ AR sÃ¼recinin derecesini gÃ¶sterir.)* |
| **First bar only**<br>*(Sadece birinci Ã§ubuk)* | **Classic AR(1):** Common in many time series. Only yesterday influences today.<br>*(Klasik AR(1): BirÃ§ok zaman serisinde yaygÄ±ndÄ±r. Sadece dÃ¼n bugÃ¼nÃ¼ etkiler.)* |
| **Several bars then sharp drop**<br>*(BirkaÃ§ Ã§ubuk sonra keskin dÃ¼ÅŸÃ¼ÅŸ)* | **AR(p):** Set `p` equal to the last significant lag before the drop.<br>*(AR(p): `p` deÄŸerini, dÃ¼ÅŸÃ¼ÅŸten Ã¶nceki son anlamlÄ± gecikmeye eÅŸitleyin.)* |

---

### ğŸ› ï¸ Workflow: Choosing the AR Order `p`
*(Ä°ÅŸ AkÄ±ÅŸÄ±: AR Derecesi `p` SeÃ§imi)*

1.  **Stationarity Check:** Ensure the series is stationary. Difference (`d`) if needed.
    *(Durgunluk KontrolÃ¼: Serinin durgun olduÄŸundan emin olun. Gerekirse fark alÄ±n.)*
2.  **Plot PACF:** Use `plot_pacf(series, lags=30)` from `statsmodels`.
    *(PACF Ã‡izimi: `statsmodels` kÃ¼tÃ¼phanesini kullanÄ±n.)*
3.  **Identify Cut-off:** Find the **last bar** that sticks out significantly above the confidence interval.
    *(Kesilme NoktasÄ±nÄ± Belirle: GÃ¼ven aralÄ±ÄŸÄ±nÄ±n dÄ±ÅŸÄ±na Ã§Ä±kan son Ã§ubuÄŸu bulun.)*
    * That lag number = **Candidate `p`** (*Aday p*).
4.  **Validate:** Don't rely solely on the plot. Compare models using **AIC/BIC** scores or **Cross-Validation**.
    *(DoÄŸrulama: Sadece grafiÄŸe gÃ¼venmeyin. AIC/BIC skorlarÄ± veya Ã‡apraz DoÄŸrulama ile modelleri karÅŸÄ±laÅŸtÄ±rÄ±n.)*

> **ğŸ” Technical Detail:** The Confidence Interval is typically calculated as $\pm 1.96 / \sqrt{T}$ where $T$ is the number of observations. Bars within this range are considered **White Noise** (statistical noise).

---

### â“ Common Questions & Troubleshooting

| Question | Quick Answer & Technical Reason  |
| :--- | :--- |
| **Why not pick a huge `p`?**<br>*(Neden Ã§ok bÃ¼yÃ¼k bir `p` seÃ§miyoruz?)* | **Overfitting Risk:** Adding too many lags captures random noise, not the signal. It increases model complexity without improving predictive power (penalized by AIC/BIC).<br>*(AÅŸÄ±rÄ± Ã–ÄŸrenme Riski: Ã‡ok fazla gecikme sinyali deÄŸil gÃ¼rÃ¼ltÃ¼yÃ¼ yakalar. Tahmin gÃ¼cÃ¼nÃ¼ artÄ±rmadan model karmaÅŸÄ±klÄ±ÄŸÄ±nÄ± yÃ¼kseltir.)* |
| **What if I see no bars above the band?**<br>*(Ya bandÄ±n Ã¼zerinde hiÃ§ Ã§ubuk gÃ¶rmezsem?)* | **White Noise or Over-differencing:** The series might differenced too much (`d` is too high), or it holds no predictive pattern. Try `p=0` or reduce `d`.<br>*(Beyaz GÃ¼rÃ¼ltÃ¼ veya AÅŸÄ±rÄ± Fark Alma: Seri gereÄŸinden fazla fark alÄ±nmÄ±ÅŸ olabilir veya tahmin edilebilir bir desen iÃ§ermiyordur.)* |
| **What if bars never drop (slow decay)?**<br>*(Ya Ã§ubuklar hiÃ§ dÃ¼ÅŸmezse / yavaÅŸ azalÄ±rsa?)* | **Non-Stationarity:** The series is likely still non-stationary. Re-examine **differencing** (`d`) or check for **seasonality**.<br>*(Durgun Olmama: Seri muhtemelen hala durgun deÄŸildir. Fark alma iÅŸlemini tekrar inceleyin veya mevsimsellik kontrolÃ¼ yapÄ±n.)* |

# ğŸ“‰ Choosing the MA Order (`q`) with ACF
*(ACF ile MA Derecesi `q` SeÃ§imi)*

ARIMA modelinin **MA (Moving Average - Hareketli Ortalama)** bileÅŸeni olan `q` parametresini belirlemek iÃ§in birincil aracÄ±mÄ±z **Autocorrelation Function (ACF)** grafiÄŸidir.

### ğŸ§ What is `q` in ARIMA?
*(ARIMA'da `q` Nedir?)*

`p` parametresi geÃ§miÅŸ *deÄŸerlere* (satÄ±ÅŸ rakamlarÄ±na) bakarken, **`q` parametresi geÃ§miÅŸ tahmin hatalarÄ±na (forecast errors/residuals)** bakar.
MA modelleri, serideki ÅŸoklarÄ±n veya hatalarÄ±n zaman iÃ§inde nasÄ±l yayÄ±ldÄ±ÄŸÄ±nÄ± modeller.

> **Key Takeaway:** ACF answers "How many past mistakes still impact today?"
> *(Temel Ã‡Ä±karÄ±m: ACF, "GeÃ§miÅŸteki kaÃ§ hata bugÃ¼nÃ¼ hala etkiliyor?" sorusuna cevap verir.)*

---

### ğŸ“Š How to Read an ACF Plot
*(ACF GrafiÄŸi NasÄ±l Okunur)*

MA (`q`) derecesini seÃ§erken, ACF grafiÄŸinde "Cut-off" (Kesilme) noktasÄ±na bakarÄ±z. PACF'in aksine, burada **ACF** grafiÄŸindeki ani dÃ¼ÅŸÃ¼ÅŸler MA derecesini iÅŸaret eder.



#### ğŸ› ï¸ Step-by-Step Workflow (AdÄ±m AdÄ±m Ä°ÅŸ AkÄ±ÅŸÄ±)

1.  **Stationarity First:** Ensure the series is stationary (`d` is fixed).
    *(Ã–nce Durgunluk: Serinin durgun olduÄŸundan emin olun.)*
2.  **Plot ACF:** Use `plot_acf(series)` from `statsmodels`.
    *(ACF Ã‡izimi: `statsmodels` Ã¼zerinden ACF grafiÄŸini Ã§izin.)*
3.  **Identify Cut-off:** Look for the lag where bars drop into the **grey band** (Confidence Interval) and stay there.
    *(Kesilmeyi Belirle: Ã‡ubuklarÄ±n gri banda [GÃ¼ven AralÄ±ÄŸÄ±na] dÃ¼ÅŸtÃ¼ÄŸÃ¼ ve orada kaldÄ±ÄŸÄ± gecikmeyi bulun.)*
4.  **Set `q`:** The last significant lag before the drop is your candidate `q`.
    *(q'yu Ayarla: DÃ¼ÅŸÃ¼ÅŸten Ã¶nceki son anlamlÄ± gecikme, aday q deÄŸerinizdir.)*

---

### ğŸ” Interpreting ACF Signatures
*(ACF Ä°mzalarÄ±nÄ± Yorumlama)*

| Plot Pattern (Grafik Deseni) | Model Implication (Model Ã‡Ä±karÄ±mÄ±) |
| :--- | :--- |
| **Sharp Cut-off after Lag `q`**<br>*(Gecikme `q`'dan sonra keskin kesilme)* | **MA(`q`) Candidate:** Strong evidence for a Moving Average model of order `q`.<br>*(MA(q) AdayÄ±: q derecesinden Hareketli Ortalama modeli iÃ§in gÃ¼Ã§lÃ¼ kanÄ±t.)* |
| **Gradual Decay (Sine Wave / Exponential)**<br>*(Kademeli Azalma / SinÃ¼s DalgasÄ±)* | **AR(`p`) Process:** Typically indicates an Autoregressive process. Look at **PACF** instead to find `p`.<br>*(AR(p) SÃ¼reci: Genellikle Otokorelasyon sÃ¼recini gÃ¶sterir. p'yi bulmak iÃ§in PACF'e bakÄ±n.)* |
| **Spikes at Regular Intervals (s, 2s...)**<br>*(DÃ¼zenli AralÄ±klarda SÄ±Ã§ramalar)* | **Seasonality:** Indicates seasonal patterns (e.g., lag 7, 14, 21). Needs SARIMA.<br>*(Mevsimsellik: Mevsimsel kalÄ±plarÄ± gÃ¶sterir. SARIMA gerektirir.)* |

> ğŸ’¡ **Technical Note:** For a pure MA(q) process:
> * **ACF:** Cuts off after lag `q`. (*Gecikme q'dan sonra kesilir.*)
> * **PACF:** Decays gradually (tails off). (*Kademeli olarak azalÄ±r.*)

---

### âš ï¸ Common Pitfalls & Fixes
*(YaygÄ±n Hatalar ve Ã‡Ã¶zÃ¼mler)*

Model kurarken karÅŸÄ±laÅŸabileceÄŸiniz yaygÄ±n ACF desenleri ve teknik Ã§Ã¶zÃ¼mleri:

| Pitfall (Tuzak) | Symptom on ACF (ACF Belirtisi) | Technical Fix (Teknik Ã‡Ã¶zÃ¼m) |
| :--- | :--- | :--- |
| **Over-differencing**<br>*(AÅŸÄ±rÄ± Fark Alma)* | **No significant bars:** First lag might even be significantly negative (approx -0.5).<br>*(AnlamlÄ± Ã§ubuk yok: Ä°lk gecikme negatif ve anlamsÄ±z olabilir.)* | **Try smaller `d`:** You likely differenced a stationary series unnecessarily. Revert to `d=0` or `d-1`.<br>*(Daha kÃ¼Ã§Ã¼k `d` dene: Muhtemelen durgun bir serinin gereksiz yere farkÄ±nÄ± aldÄ±nÄ±z.)* |
| **Under-differencing**<br>*(Yetersiz Fark Alma)* | **Very slow decay:** Bars stay high and positive for many lags, decreasing linearly.<br>*(Ã‡ok yavaÅŸ azalma: Ã‡ubuklar birÃ§ok gecikme boyunca yÃ¼ksek ve pozitif kalÄ±r, doÄŸrusal azalÄ±r.)* | **Increase `d`:** The series is still non-stationary (Unit Root present). Take an additional difference.<br>*(d'yi ArtÄ±r: Seri hala durgun deÄŸil. Bir fark daha alÄ±n.)* |
| **Seasonality Present**<br>*(Mevsimsellik Var)* | **Periodic Spikes:** Significant correlations appearing at specific lags (e.g., 7, 14 for weekly data).<br>*(Periyodik SÄ±Ã§ramalar: Belirli gecikmelerde [haftalÄ±k veride 7, 14 gibi] anlamlÄ± korelasyonlar.)* | **Seasonal Model:** Consider **SARIMA** (adding Seasonal MA term `Q`) or apply **Seasonal Differencing** (`D=1`, lag=7).<br>*(Mevsimsel Model: SARIMA dÃ¼ÅŸÃ¼nÃ¼n veya Mevsimsel Fark Alma uygulayÄ±n.)* |

---

### âœ… Summary Strategy: Choosing `p` and `q`
*(Ã–zet Strateji: p ve q SeÃ§imi)*

| Parameter | Plot to Watch | Pattern to Look For |
| :--- | :--- | :--- |
| **AR (`p`)** | **PACF** | **Cut-off:** Last significant spike determines `p`. |
| **MA (`q`)** | **ACF** | **Cut-off:** Last significant spike determines `q`. |

> **Final Check:** After choosing `p` and `q`, always validate with **Information Criteria (AIC/BIC)** and check the residuals of your model.
> *(Son Kontrol: SeÃ§imden sonra her zaman Bilgi Kriterleri [AIC/BIC] ile doÄŸrulayÄ±n ve model hatalarÄ±nÄ± [residuals] kontrol edin.)*


# ğŸ“Œ Summary & ARIMA Workflow
*(Ã–zet ve ARIMA Ä°ÅŸ AkÄ±ÅŸÄ±)*

### ğŸš€ Why ARIMA?
*(Neden ARIMA?)*

**ARIMA** (*AutoRegressive Integrated Moving Average*), daha aÄŸÄ±r makine Ã¶ÄŸrenimi yaklaÅŸÄ±mlarÄ±na (*Machine Learning approaches*) dalmadan Ã¶nce baÅŸvurmanÄ±z gereken, **kompakt**, **ÅŸeffaf** ve **istatistiksel olarak titiz** bir tahmin modelidir.

> **âš ï¸ Limitation (KÄ±sÄ±t):**
> ARIMA, by design, handles **trend** and short-term **autocorrelation** but assumes your series has **no built-in seasonality**. There is no term in the definition explicitly modeling repeating cycles.
> *(ARIMA, tasarÄ±mÄ± gereÄŸi trendi ve kÄ±sa vadeli otokorelasyonu yÃ¶netir ancak serinizin yerleÅŸik bir mevsimselliÄŸi olmadÄ±ÄŸÄ±nÄ± varsayar. TanÄ±mÄ±nda tekrarlayan dÃ¶ngÃ¼leri modelleyen aÃ§Ä±k bir terim yoktur.)*
>
> ğŸ’¡ **Solution:** If your data shows strong weekly or annual patterns, upgrade to **SARIMA**, which adds a "Seasonal" component.

---

### âš™ï¸ The Three Pillars: p, d, q
*(ÃœÃ§ Temel Direk: p, d, q)*

ARIMA requires three hyperparameters that define its structure:

| Parameter | Component | Description (AÃ§Ä±klama) |
| :--- | :--- | :--- |
| **p** | **AutoRegression (AR)**<br>*(Oto-Regresyon)* | **Past Values:** It looks back at the last `p` days of sales to detect patterns.<br>*(GeÃ§miÅŸ DeÄŸerler: KalÄ±plarÄ± tespit etmek iÃ§in son p gÃ¼nÃ¼n satÄ±ÅŸlarÄ±na bakar.)* |
| **d** | **Integration (I)**<br>*(Entegrasyon/Fark Alma)* | **Stationarity:** It removes smooth up-or-down trends by **differencing** the data `d` times.<br>*(Durgunluk: Verinin d kez farkÄ±nÄ± alarak yukarÄ± veya aÅŸaÄŸÄ± yÃ¶nlÃ¼ trendleri kaldÄ±rÄ±r.)* |
| **q** | **Moving Average (MA)**<br>*(Hareketli Ortalama)* | **Past Errors:** It learns from the last `q` days of **forecast errors** to correct its predictions.<br>*(GeÃ§miÅŸ Hatalar: Tahminlerini dÃ¼zeltmek iÃ§in son q gÃ¼nÃ¼n tahmin hatalarÄ±ndan Ã¶ÄŸrenir.)* |

---

### ğŸ› ï¸ The Professional Workflow
*(Profesyonel Ä°ÅŸ AkÄ±ÅŸÄ±)*

Follow this step-by-step pipeline to build a robust ARIMA model.

#### 1. Exploratory Analysis
*(KeÅŸifÃ§i Analiz)*
* **Visualize:** Plot the raw series, moving average, and variance.
* **Check Trend:** Is there an obvious upward/downward drift?
* **Test Stationarity:** Run the **ADF Test** (*Augmented Dickey-Fuller*).

#### 2. Differencing (Parameter `d`)
*(Fark Alma)*
* Difference the series (`d` times) until it looks flat (constant mean).
* **Verification:** Ensure ADF p-value < 0.05.
* *Note:* Usually `d=1` is sufficient. `d=2` is rare.

#### 3. Identification (Parameters `p` & `q`)
*(TanÄ±mlama)*
Plot **ACF** and **PACF** charts on the **differenced** series:
* **PACF Cut-off:** Suggests the AR order (**p**).
* **ACF Cut-off:** Suggests the MA order (**q**).

#### 4. Model Fitting
*(Model EÄŸitimi)*
Fit the model using `statsmodels`.

```python
from statsmodels.tsa.arima.model import ARIMA

# Define the model with identified orders
# order = (p, d, q)
model = ARIMA(series, order=(p, d, q))

# Train the model
results = model.fit()

# View statistical summary
print(results.summary())

```

### 5. ğŸ©º Diagnostics: Residual Analysis
*(TanÄ±lama: ArtÄ±k Analizi)*

This is a **critically important step** (*kritik derecede Ã¶nemli bir adÄ±m*). We must check if the **residuals** (*hatalar/artÄ±klar*) resemble **White Noise** (*Beyaz GÃ¼rÃ¼ltÃ¼*).

**Checklist for Residuals:**
*(ArtÄ±klar iÃ§in Kontrol Listesi:)*

* **Mean** (*Ortalama*): Should be close to **0**.
    *(0'a yakÄ±n olmalÄ±dÄ±r.)*
* **Variance** (*Varyans*): Should be **constant**.
    *(Sabit olmalÄ±dÄ±r.)*
* **No Correlation** (*Korelasyon Yok*): The **ACF** of residuals should show no significant **spikes**.
    *(ArtÄ±klarÄ±n ACF grafiÄŸi anlamlÄ± sivrilmeler gÃ¶stermemelidir.)*
* **Test:** Use the **Ljung-Box Test** to statistically confirm residuals are **random**.
    *(Test: ArtÄ±klarÄ±n rastgele olduÄŸunu istatistiksel olarak doÄŸrulamak iÃ§in Ljung-Box Testi kullanÄ±n.)*

---

### 6. ğŸ”® Forecast and Evaluate
*(Tahmin ve DeÄŸerlendirme)*

Once the diagnostics pass, we evaluate the model using specific metrics.

#### ğŸ“‰ Model Selection Metric: AIC
*(Model SeÃ§im MetriÄŸi: AIC)*

* **Definition:** **AIC (Akaike Information Criterion)**.
* **Purpose:** Used for **Model Selection**.
    *(Model SeÃ§imi iÃ§in kullanÄ±lÄ±r.)*
* **Interpretation:** **Lower is better**. It balances **model fit** vs. **complexity**.
    *(Daha dÃ¼ÅŸÃ¼k olmasÄ± daha iyidir. Model uyumu ile karmaÅŸÄ±klÄ±ÄŸÄ± dengeler.)*

#### ğŸ¯ Accuracy Metrics: MAE / RMSE
*(DoÄŸruluk Metrikleri: MAE / RMSE)*

* **Purpose:** Used for **Accuracy**.
    *(DoÄŸruluk iÃ§in kullanÄ±lÄ±r.)*
* **Interpretation:** Evaluate how close the **predictions** are to **actuals** on a test set.
    *(Test setindeki tahminlerin gerÃ§ek deÄŸerlere ne kadar yakÄ±n olduÄŸunu Ã¶lÃ§er.)*

